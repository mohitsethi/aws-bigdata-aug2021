<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.26wmf15</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2600" case="first-letter">Topic</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Analytics</title>
    <ns>0</ns>
    <id>487132</id>
    <revision>
      <id>670788887</id>
      <parentid>667275100</parentid>
      <timestamp>2015-07-10T06:20:48Z</timestamp>
      <contributor>
        <username>Vidyasnap</username>
        <id>22650377</id>
      </contributor>
      <minor/>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14841">{{for|the ice hockey term|Analytics (ice hockey)}}
'''Analytics''' is the discovery and communication of meaningful patterns in data. Especially valuable in areas rich with recorded information, analytics relies on the simultaneous application of [[statistics]], [[computer programming]] and [[operations research]] to quantify performance. Analytics often favors [[data visualization]] to communicate insight.

Firms may commonly apply analytics to business data, to describe, predict, and improve business performance. Specifically, areas within analytics include [[predictive analytics]], [[enterprise decision management]], retail analytics, store assortment and [[stock-keeping unit]] optimization, marketing optimization and [[marketing mix modeling]], [[web analytics]], sales force sizing and optimization, price and promotion modeling, predictive science, credit [[risk analysis (business)|risk analysis]], and [[fraud detection|fraud analytics]]. Since analytics can require extensive computation (see [[big data]]), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics.&lt;ref&gt;{{cite journal|last=Kohavi, Rothleder and Simoudis|title=Emerging Trends in Business Analytics|journal=Communications of the ACM|year=2002|volume=45|issue=8|pages=45–48|doi=10.1145/545151.545177}}&lt;/ref&gt;

==Analytics vs. analysis==
Analytics is a [[Dimension|multidimensional]] discipline. There is extensive use of mathematics and statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data—data analysis. The insights from data are used to recommend action or to guide decision making rooted in business context. Thus, analytics is not so much concerned with individual analyses or analysis steps, but with the entire [[methodology]]. There is a pronounced tendency to use the term ''analytics'' in business settings e.g. [[text analytics]] vs. the more generic [[text mining]] to emphasize this broader perspective. {{Citation needed|date=October 2013}}. There is an increasing use of the term ''advanced analytics'',{{citation needed|date=January 2014}} typically used to describe the technical aspects of analytics, especially in the emerging fields such as the use of [[machine learning]] techniques like [[neural networks]] to do [[predictive modeling]].

== Examples ==

=== Marketing optimization ===
Marketing has evolved from a creative process into a highly data-driven process.  Marketing organizations use analytics to determine the outcomes of campaigns or efforts and to guide decisions for investment and consumer targeting.  Demographic studies, customer segmentation, conjoint analysis and other techniques allow marketers to use large amounts of consumer purchase, survey and panel data to understand and communicate marketing strategy.

[[Web analytics]] allows marketers to collect session-level information about interactions on a website using an operation called [[sessionization]].  [[Google Analytics]] is an example of a popular free analytics tools that marketers use for this purpose. Those interactions provide the web analytics information systems with the information to track the referrer, search keywords, IP address, and activities of the visitor.  With this information, a marketer can improve the marketing campaigns, site creative content, and information architecture.

Analysis techniques frequently used in marketing include marketing mix modeling, pricing and promotion analyses, sales force optimization, customer analytics e.g.: segmentation. Web analytics and optimization of web sites and online campaigns now frequently work hand in hand with the more traditional marketing analysis techniques. A focus on digital media has slightly changed the vocabulary so that marketing mix modeling is commonly referred to as attribution modeling in the digital or [[Marketing mix modeling]] context.

These tools and techniques support both strategic marketing decisions (such as how much overall to spend on marketing and how to allocate budgets across a portfolio of brands and the marketing mix) and more tactical campaign support in terms of targeting the best potential customer with the optimal message in the most cost effective medium at the ideal time.

=== Portfolio analysis ===
A common application of business analytics is [[Portfolio finance|portfolio analysis]]. In this, a [[bank]] or lending agency has a collection of accounts of varying [[Value economics|value]] and [[risk]]. The accounts may differ by the social status (wealthy, middle-class, poor, etc.) of the holder, the geographical location, its net value, and many other factors.  The lender must balance the return on the [[loan]] with the risk of default for each loan. The question is then how to evaluate the portfolio as a whole.

The least risk loan may be to the very wealthy, but there are a very limited number of wealthy people.  On the other hand there are many poor that can be lent to, but at greater risk.  Some balance must be struck that maximizes return and minimizes risk.  The analytics solution may combine [[time series]] analysis with many other issues in order to make decisions on when to lend money to these different borrower segments, or decisions on the interest rate charged to members of a portfolio segment to cover any losses among members in that segment.

=== Risk analytics ===

Predictive models in the banking industry are developed to bring certainty across the risk scores for individual customers. Credit scores are built to predict individual’s delinquency behaviour and widely used to evaluate the credit worthiness of each applicant. Furthermore, risk analyses are carried out in the scientific world and the insurance industry.

=== Digital analytics ===
Digital analytics is a set of business and technical activities that define, create, collect, verify or transform digital data into reporting, research, analyses, recommendations, optimizations, predictions, and automations.&lt;ref&gt;Phillips, Judah &quot;Building a Digital Analytics Organization&quot; Financial Times Press, 2013, pp 7–8.&lt;/ref&gt;

=== Security analytics ===
Security analytics refers to information technology (IT) solutions that gather and analyze security events to bring situational awareness and enable IT staff to understand and analyze events that pose the greatest risk.&lt;ref&gt;{{cite web|title=Security analytics shores up hope for breach detection |url=http://enterpriseinnovation.net/article/security-analytics-shores-hope-breach-detection-192448485|publisher=Enterprise Innovation|accessdate=April 27, 2015}}&lt;/ref&gt; Solutions in this area include [[Security information and event management]] solutions and user behavior analytics solutions. 

=== Software analytics ===
{{main|Software analytics}}
Software analytics is the process of collecting information about the way a piece of [[software]] is used and produced.

==Challenges==
In the industry of commercial analytics software, an emphasis has emerged on solving the challenges of analyzing massive, complex data sets, often when such data is in a constant state of change. Such data sets are commonly referred to as [[big data]]. Whereas once the problems posed by big data were only found in the scientific community, today big data is a problem for many businesses that operate transactional systems online and, as a result, amass large volumes of data quickly.&lt;ref&gt;{{cite web|last=Naone|first=Erica|title=The New Big Data|url=http://www.technologyreview.com/computing/38397/|publisher=Technology Review, MIT|accessdate=August 22, 2011}}&lt;/ref&gt;

The analysis of [[unstructured data]] types is another challenge getting attention in the industry. Unstructured data differs from [[structured data]] in that its format varies widely and cannot be stored in traditional relational databases without significant effort at data transformation.&lt;ref&gt;{{cite book|last1=Inmon|first1=Bill|last2=Nesavich|first2=Anthony|title=Tapping Into Unstructured Data|year=2007|publisher=Prentice-Hall|isbn=978-0-13-236029-6}}&lt;/ref&gt; Sources of unstructured data, such as email, the contents of word processor documents, PDFs, geospatial data, etc., are rapidly becoming a relevant source of [[business intelligence]] for businesses, governments and universities.&lt;ref&gt;{{cite web|last=Wise|first=Lyndsay|title=Data Analysis and Unstructured Data|url=http://www.dashboardinsight.com/articles/business-performance-management/data-analysis-and-unstructured-data.aspx|publisher=Dashboard Insight|accessdate=February 14, 2011}}&lt;/ref&gt; For example, in Britain the discovery that one company was illegally selling fraudulent doctor's notes in order to assist people in defrauding employers and insurance companies,&lt;ref&gt;{{cite news|title=Fake doctors' sick notes for Sale for £25, NHS fraud squad warns|url=http://www.telegraph.co.uk/news/uknews/2626120/Fake-doctors-sick-notes-for-sale-on-web-for-25-NHS-fraud-squad-warns.html|publisher=The Telegraph|accessdate=August 2008|location=London}}&lt;/ref&gt; is an opportunity for insurance firms to increase the vigilance of their unstructured data analysis. The McKinsey Global Institute estimates that big data analysis could save the American health care system $300 billion per year and the European public sector €250 billion.&lt;ref&gt;{{cite news|title=Big Data: The next frontier for innovation, competition and productivity as reported in Building with Big Data|url=http://www.economist.com/node/18741392|accessdate=May 26, 2011 | work=The Economist|date=May 26, 2011| archiveurl= http://web.archive.org/web/20110603031738/http://www.economist.com/node/18741392| archivedate= 3 June 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

These challenges are the current inspiration for much of the innovation in modern analytics information systems, giving birth to relatively new machine analysis concepts such as [[complex event processing]], full text search and analysis, and even new ideas in presentation.&lt;ref&gt;{{cite web|last=Ortega|first=Dan|title=Mobililty: Fueling a Brainier Business Intelligence|url=http://www.itbusinessedge.com/cm/community/features/guestopinions/blog/mobility-fueling-a-brainier-business-intelligence/?cs=47491|publisher=IT Business Edge|accessdate=June 21, 2011}}&lt;/ref&gt; One such innovation is the introduction of grid-like architecture in machine analysis, allowing increases in the speed of massively parallel processing by distributing the workload to many computers all with equal access to the complete data set.&lt;ref&gt;{{cite web|last=Khambadkone|first=Krish|title=Are You Ready for Big Data?|url=http://www.infogain.com/company/perspective-big-data.jsp|publisher=InfoGain|accessdate=February 10, 2011}}&lt;/ref&gt;

Analytics is increasingly used in [[education]], particularly at the district and government office levels. However, the complexity of student performance measures presents challenges when educators try to understand and use analytics to discern patterns in student performance, predict graduation likelihood, improve chances of student success, etc. For example, in a study involving districts known for strong data use, 48% of teachers had difficulty posing questions prompted by data, 36% did not comprehend given data, and 52% incorrectly interpreted data.&lt;ref&gt;U.S. Department of Education Office of Planning, Evaluation and Policy Development (2009). ''Implementing data-informed decision making in schools: Teacher access, supports and use.'' United States Department of Education (ERIC Document Reproduction Service No. ED504191)&lt;/ref&gt; To combat this, some analytics tools for educators adhere to an [[over-the-counter data]] format (embedding labels, supplemental documentation, and a help system, and making key package/display and content decisions) to improve educators’ understanding and use of the analytics being displayed.&lt;ref&gt;Rankin, J. (2013, March 28). [https://sas.elluminate.com/site/external/recording/playback/link/table/dropin?sid=2008350&amp;suid=D.4DF60C7117D5A77FE3AED546909ED2 How data Systems &amp; reports can either fight or propagate the data analysis error epidemic, and how educator leaders can help.] ''Presentation conducted from Technology Information Center for Administrative Leadership (TICAL) School Leadership Summit.''&lt;/ref&gt;

One more emerging challenge is dynamic regulatory needs. For example, in the banking industry, Basel III and future capital adequacy needs are likely to make even smaller banks adopt internal risk models.  In such cases, cloud computing and open source [[R (programming language)]] can help smaller banks to adopt risk analytics and support branch level monitoring by applying predictive analytics.{{citation needed|date=November 2012}}

==Risks==
{{original research|date=March 2015}}
The main risk for the people is discrimination like [[Price discrimination]] or [[Statistical discrimination (economics)|Statistical discrimination]].

There is also the risk that a developer could profit from the ideas or work done by users, like this example:
Users could write new ideas in a note taking app, which could then be sent as a custom event, and the developers could profit from those ideas.  This can happen because the ownership of content is usually unclear in the law.&lt;ref&gt;http://www.techrepublic.com/blog/10-things/10-reasons-why-i-avoid-social-networking-services/&lt;/ref&gt;

If a user's identity is not protected, there are more risks; for example, the risk that private information about users is made public on the internet.

In the extreme, there is the risk that governments could gather too much private information, now that the governments are giving themselves more powers to access citizens' information.  {{Further|Telecommunications data retention}}

== See also ==
{{Col-begin}}
{{Col-1-of-2}}
* [[Analysis]]
* [[Big data]]
* [[Business analytics]]
* [[Business intelligence]]
* [[Complex event processing]]
* [[Data mining]]
* [[Data presentation architecture]]
* [[Learning analytics]]
{{Col-2-of-2}}
* [[List of software engineering topics]]
* [[Machine learning]]
* [[Online analytical processing]]
* [[Online video analytics]]
* [[Operations research]]
* [[Predictive analytics]]
* [[Prescriptive Analytics|Prescriptive analytics]]
* [[Statistics]]
* [[Web analytics]]
* [[Smart grid]]
{{col-end}}

== References ==
{{reflist}}
*

== External links ==
* [http://analyticsmagazine.com/ INFORMS' bi-monthly, digital magazine on the analytics profession]
* [https://thoughts.manthan.com/2015/07/08/43-new-age-analytics-terms-that-are-good-to-know-glossary/ Glossary of popular analytical terms]

{{Wiktionary}}

[[Category:Analytics| ]]
[[Category:Financial data analysis]]
[[Category:Mathematical finance]]
[[Category:Formal sciences]]
[[Category:Business terms]]
[[Category:Business intelligence]]
[[Category:Big data]]</text>
      <sha1>jtr0qggh8x0xpkuqnb14zru89wlhcby</sha1>
    </revision>
  </page>
  <page>
    <title>Prescriptive analytics</title>
    <ns>0</ns>
    <id>35757264</id>
    <revision>
      <id>660597901</id>
      <parentid>657345511</parentid>
      <timestamp>2015-05-03T15:15:44Z</timestamp>
      <contributor>
        <username>Gadget850</username>
        <id>339034</id>
      </contributor>
      <comment>cleanup class &quot;references-small&quot; (class was deleted 21 Dec 2010) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22352">{{Multiple issues|
{{refimprove|date=June 2012}}
{{notability|date=June 2012}}
}}

'''Prescriptive analytics''' is the third and final phase of [[business analytics]] [[Business analytics|(BA)]] which includes descriptive, [[Predictive analytics|predictive]] and prescriptive analytics.&lt;ref&gt;{{cite journal|last=Evans,James R. and Lindner, Carl H.|title=Business Analytics: The Next Frontier for Decision Sciences|journal=Decision Line|date=March 2012|volume=43|issue=2}}&lt;/ref&gt;&lt;ref name=&quot;LustigEtAl&quot;&gt;http://www.analytics-magazine.org/november-december-2010/54-the-analytics-journey{{cite journal|last=Lustig,Irv, Dietrich, Brenda, Johnson, Christer, and Dziekan, Christopher|title=The Analytics Journey|journal=Analytics|date=Nov–Dec 2010}}&lt;/ref&gt;

Referred to as the &quot;final frontier of analytic capabilities,&quot;&lt;ref&gt;https://www.globys.com/2013/06/gartner-terms-prescriptive-analytics-%E2%80%9Cfinal-frontier%E2%80%9D-analytic-capabilities&lt;/ref&gt; Prescriptive analytics automatically synthesizes [[big data]], multiple disciplines of [[mathematical sciences]] and [[computational science]]s, and [[business rules]], to make [[predictions]] and then suggests decision options to take advantage of the predictions.&lt;ref&gt;http://ayata.com/the-evolution-of-big-data-analytics/&lt;/ref&gt;  The first stage of business analytics is descriptive analytics, which still accounts for the majority of all business analytics today.&lt;ref&gt;{{cite journal|last=Davenport,Tom |title=The three '..tives' of business analytics; predictive, prescriptive and descriptive|journal=CIO Enterprise Forum|date=November 2012}}&lt;/ref&gt; Descriptive analytics answers the questions what happened and why did it happen. Descriptive analytics looks at past performance and understands that performance by mining historical data to look for the reasons behind past success or failure. Most management reporting - such as [[sales]], [[marketing]], [[Business operations|operations]], and [[finance]] - uses this type of post-mortem analysis.

[[File:Three Phases of Analytics.png|thumb|left|350px|Prescriptive Analytics extends beyond predictive analytics by specifying both the actions necessary to achieve predicted outcomes, and the interrelated effects of each decision]]The next phase is [[predictive analytics]]. Predictive analytics answers the question what will happen. This is when historical performance data is combined with rules, [[algorithms]], and occasionally external data to determine the probable future outcome of an event or the likelihood of a situation occurring. The final phase is prescriptive analytics,&lt;ref&gt;{{cite journal|last=Haas, Peter J., Maglio, Paul P., Selinger, Patricia G., and Tan, Wang-Chie|title=Data is Dead…Without What-If Models|journal=Proceedings of the VLDB Endowment|year=2011|volume=4|number=12}}&lt;/ref&gt; which goes beyond predicting future outcomes by also suggesting actions to benefit from the predictions and showing the implications of each decision option.&lt;ref&gt;{{cite journal|last=Stewart, Thomas. R., and McMillan, Claude, Jr. |title=Descriptive and Prescriptive Models for Judgment and Decision Making: Implications for Knowledge Engineering|journal=NATO AS1 Senes, Expert Judgment and Expert Systems,|year=1987|volume=F35|pages=314–318}}&lt;/ref&gt;

Prescriptive analytics not only anticipates what will happen and when it will happen, but also why it will happen. Further, prescriptive analytics suggests decision options on how to take advantage of a future opportunity or mitigate a future risk and shows the implication of each decision option. Prescriptive analytics can continually take in new data to re-predict and re-prescribe, thus automatically improving prediction accuracy and prescribing better decision options. Prescriptive analytics ingests hybrid data, a combination of structured (numbers, categories) and unstructured data (videos, images, sounds, texts), and business rules to predict what lies ahead and to prescribe how to take advantage of this predicted future without compromising other priorities.&lt;ref&gt;{{cite journal|last=Riabacke, Mona, Danielson, Mats, and Ekenber, Love |title=State-of-the-Art Prescriptive Criteria Weight Elicitation|journal=Advances in Decision Sciences|year=2012}}&lt;/ref&gt;

All three phases of analytics can be performed through professional services or technology or a combination.  In order to scale, prescriptive analytics technologies need to be adaptive to take into account the growing volume, velocity, and variety of data that most mission critical processes and their environments may produce.

One criticism of prescriptive analytics is that its distinction from [[predictive analytics]] is ill-defined and therefore ill-conceived.&lt;ref&gt;{{cite journal|last=Bill Vorhies|url=http://www.predictiveanalyticsworld.com/patimes/prescriptive-versus-predictive-analytics-distinction-without-difference/ |title=Prescriptive versus Predictive Analytics – A Distinction without a Difference?|journal=Predictive Analytics Times|date=November 2014}}&lt;/ref&gt; [[File:Components of Prescriptive Analytics.png|thumb|600px|The scientific disciplines that comprise Prescriptive Analytics]]

==History==

[[File:Prescriptive Analytics Timeline.jpeg|thumb|300px|Timeline tracing evolution of Prescriptive Analytics capability and software]][[File:Prescriptive Analytics process.png|thumb|left|350px|Prescriptive Analytics incorporates both structured and unstructured data, and uses a combination of advanced analytic techniques and disciplines to predict, prescribe, and adapt.]]Prescriptive Analytics, a term first coined by IBM&lt;ref name=&quot;LustigEtAl&quot;/&gt; and later trademarked by Ayata,&lt;ref&gt;http://trademarks.justia.com/852/06/prescriptive-analytics-85206495.html&lt;/ref&gt; has been around since about 2003. The technology behind prescriptive analytics synergistically combines hybrid [[data]], business rules with [[mathematical model]]s and [[computational model]]s. The data inputs to prescriptive analytics may come from multiple sources: internal, such as inside a corporation; and external, also known as environmental data.  The data may be structured, which includes numbers and categories, as well as [[unstructured data]], such as texts, images, sounds, and videos. Unstructured data differs from [[structured data]] in that its format varies widely and cannot be stored in traditional relational databases without significant effort at data transformation.&lt;ref&gt;{{cite book|last=Inmon|first=Bill|author2=Nesavich, Anthony|title=Tapping Into Unstructured Data|year=2007|publisher=Prentice-Hall|isbn=978-0-13-236029-6}}&lt;/ref&gt; More than 80% of the world's data today is unstructured, according to IBM.

In addition to this variety of data types and growing data volume, incoming data can also evolve with respect to velocity, that is, more data being generated at a faster or a variable pace. Business rules define the [[business process]] and include objectives constraints, preferences, policies, best practices, and boundaries. Mathematical models and computational models are techniques derived from mathematical sciences, computer science and related disciplines such as applied statistics, machine learning, operations research, natural language processing, computer vision, pattern recognition, image processing, speech recognition, and signal processing. The correct application of all these methods and the verification of their results implies the need for resources on a massive scale including human, computational and temporal for every Prescriptive Analytic project. In order to spare the expense of dozens of people, high performance machines and weeks of work one must consider the reduction of resources and therefore a reduction in the accuracy or reliability of the outcome. The preferable route is a reduction that produces a probabilistic result within acceptable limits.

==Applications in Oil and Gas==

[[File:Key Questions Prescriptive Analytics software answers for oil and gas producers.png|thumb|right|450px|Key Questions Prescriptive Analytics software answers for oil and gas producers]]Energy is the largest industry in the world ($6 trillion in size). The processes and decisions related to oil and natural gas exploration, development and production generate large amounts of data. Many types of captured data are used to create models and images of the Earth’s structure and layers 5,000 - 35,000 feet below the surface and to describe activities around the wells themselves, such as depositional characteristics, machinery performance, oil flow rates, reservoir temperatures and pressures.&lt;ref&gt;{{cite journal|last= Basu, Atanu|title= How Prescriptive Analytics Can Reshape Fracking in Oil and Gas Fields|journal= Data-Informed|date=November 2012}}&lt;/ref&gt; Prescriptive analytics software can help with both locating and producing hydrocarbons&lt;ref&gt;{{cite journal|last=  Basu, Atanu |title= How Data Analytics Can Help Frackers Find Oil |journal= Datanami|date=December 2013}}&lt;/ref&gt;
by taking in seismic data, well log data, production data, and other related data sets to prescribe specific recipes for how and where to drill, complete, and produce wells in order to optimize recovery, minimize cost, and reduce environmental footprint.&lt;ref&gt;{{cite journal|last= Mohan, Daniel |title= Machines Prescribing Recipes from 'Things,' Earth, and People |journal =Oil &amp; Gas Investor|date=August 2014}}&lt;/ref&gt;

===Unconventional Resource Development===

[[File:Varied datasets.png|thumb|right|450px|Examples of structured and unstructured data sets generated and by the oil and gas companies and their ecosystem of service providers that can be analyzed together using Prescriptive Analytics software]]With the value of the end product determined by global commodity economics, the basis of competition for operators in upstream E&amp;P is the ability to effectively deploy capital to locate and extract resources more efficiently, effectively, predictably, and safely than their peers.  In unconventional resource plays, operational efficiency and effectiveness is diminished by reservoir inconsistencies, and decision-making impaired by high degrees of uncertainty.   These challenges manifest themselves in the form of low recovery factors and wide performance variations.

Prescriptive Analytics software can accurately predict production and prescribe optimal configurations of controllable drilling, completion, and production variables by modeling numerous internal and external variables simultaneously, regardless of source, structure, size, or format.&lt;ref&gt;{{cite journal |last=Basu, Mohan, Marshall, &amp; McColpin |title=The Journey to Designer Wells |journal=Oil &amp; Gas Investor |date=December 23, 2014}}&lt;/ref&gt; Prescriptive analytics software can also provide decision options and show the impact of each decision option so the operations managers can proactively take appropriate actions, on time, to guarantee future exploration and production performance, and maximize the economic value of assets at every point over the course of their serviceable lifetimes.&lt;ref&gt;{{cite journal |last=Mohan, Daniel |title=Your Data Already Know What You Don't |journal=E&amp;P Magazine |date=September 2014}}&lt;/ref&gt;

===Oilfield Equipment Maintenance===

In the realm of oilfield equipment maintenance, Prescriptive Analytics can optimize configuration, anticipate and prevent unplanned downtime, optimize field scheduling, and improve maintenance planning.&lt;ref&gt;{{cite journal |last=Presley, Jennifer |title=ESP for ESPs |journal=Exploration &amp; Production |date=July 1, 2013}}&lt;/ref&gt; According to General Electric, there are more than 130,000 electric submersible pumps (ESP's) installed globally, accounting for 60% of the world's oil production.&lt;ref&gt;{http://www.ge-energy.com/products_and_services/products/electric_submersible_pumping_systems/}&lt;/ref&gt;  Prescriptive Analytics has been deployed to predict when and why an ESP will fail, and recommend the necessary actions to prevent the failure.&lt;ref&gt;{{cite journal |last=Wheatley, Malcolm |title=Underground Analytics |journal=DataInformed |date=May 29, 2013}}&lt;/ref&gt;

In the area of [[Health, Safety and Environment|Health, Safety, and Environment]], prescriptive analytics can predict and preempt incidents that can lead to reputational and financial loss for oil and gas companies.

===Pricing===

Pricing is another area of focus. [[Natural gas prices]] fluctuate dramatically depending upon supply, demand, [[econometrics]], [[geopolitics]], and weather conditions. Gas producers, pipeline transmission companies and [[Utility companies|utility firms]] have a keen interest in more accurately predicting gas prices so that they can lock in favorable terms while hedging downside risk. Prescriptive analytics software can accurately predict prices by modeling internal and external variables simultaneously and also provide decision options and show the impact of each decision option.&lt;ref&gt;{{cite journal|last=Dr. Watson, Michael|title=Advanced Analytics in Supply Chain - What is it, and is it Better than Non-Advanced Analytics?|journal=Supply Chain Digest|date=November 2012}}&lt;/ref&gt;

==Applications in healthcare==

Multiple factors are driving [[healthcare]] providers to dramatically improve business processes and operations as the United States healthcare industry embarks on the necessary migration from a largely fee-for service, volume-based system to a fee-for-performance, value-based system. Prescriptive analytics is playing a key role to help improve the performance in a number of areas involving various stakeholders: payers, providers and pharmaceutical companies.

Prescriptive analytics can help providers improve effectiveness of their clinical care delivery to the population they manage and in the process achieve better patient satisfaction and retention.  Providers can do better population health management by identifying appropriate intervention models for risk stratified population combining data from the in-facility care episodes and home based telehealth.

Prescriptive analytics can also benefit healthcare providers in their capacity planning by using analytics to leverage operational and usage data combined with data of external factors such as economic data, population demographic trends and population health trends, to more accurately plan for future capital investments such as new facilities and equipment utilization as well as understand the trade-offs between adding additional beds and expanding an existing facility versus building a new one.&lt;ref&gt;{{cite journal|last=Foster, Roger|title=Big data and public health, part 2: Reducing Unwarranted Services|journal=Government Health IT|date=May 2012}}&lt;/ref&gt;

Prescriptive analytics can help pharmaceutical companies to expedite their drug development by identifying patient cohorts that are most suitable for the clinical trials worldwide - patients who are expected to be compliant and will not drop out of the trial due to complications.  Analytics can tell companies how much time and money they can save if they choose one patient cohort in a specific country vs. another.

In provider-payer negotiations, [[Health care provider|providers]] can improve their negotiating position with health insurers by developing a robust understanding of future service utilization. By accurately predicting utilization, providers can also better allocate personnel.

==See also==
{{Col-begin}}
{{Col-1-of-2}}
* [[Analytics]]
* [[Applied statistics|Applied Statistics]]
* [[Big data|Big Data]]
* [[Business analytics]]
* [[Business intelligence|Business Intelligence]]
* [[Data mining]]
{{Col-2-of-2}}
* [[Forecasting]]
* [[Hadoop]]
* [[Map reduce|MapReduce]]
* [[OLTP]]
* [[Operations research|Operations Research]]
* [[Statistics]]
{{col-end}}

==References==
{{reflist}}
{{refbegin}}

==Further reading==
{{refbegin|30em}}
* [[Thomas H. Davenport|Davenport, Thomas H]]., Kalakota, Ravi, Taylor, James, Lampa, Mike, Franks, Bill, Jeremy, Shapiro, Cokins, Gary, Way, Robin, King, Joy, Schafer, Lori, Renfrow, Cyndy and Sittig, Dean,  [http://iianalytics.com/wp-content/uploads/2011/12/2012-IIA-Predictions-Brief-Final.pdf ''Predictions for Analytics in 2012''] International Institute for Analytics (December 15, 2011)
* Bertolucci, Jeff, [http://www.informationweek.com/big-data/news/big-data-analytics/prescriptive-analytics-and-big-data-nex/240152863 ''Prescriptive Analytics and Data: Next Big Thing?''] InformationWeek. (April 15, 2013).
* Basu, Atanu, [http://www.analytics-magazine.org/march-april-2013/755-executive-edge-five-pillars-of-prescriptive-analytics-success ''Five Pillars of Prescriptive Analytics Success''] Analytics. (March / April 2013).
* Laney, Douglas and Kart, Lisa, (March 20, 2012). [http://www.parabal.com/uploads/docs/Greenplum/Emerging%20Role%20of%20the%20Data%20Scientist%20and%20the%20Art%20of%20Data%20Science.pdf ''Emerging Role of the Data Scientist and the Art of Data Science''] [[Gartner]].
* [[Robert R. McCormick School of Engineering and Applied Science|McCormick Northwestern Engineering]] [http://www.analytics.northwestern.edu/analytics-examples/prescriptive-analytics.html ''Prescriptive analytics is about enabling smart decisions based on data''].
* [http://business.gwu.edu/decisionsciences/i2sds/pdf/Program%20in%20BA%20presentation.pdf ''Business Analytics Information Event''], I2SDS and Department of Decision Sciences, School of Business, [[The George Washington University]] (February 10, 2011).
* [http://www.or-exchange.com/questions/1344/difference-between-operations-research-and-business-analysis &quot;The Difference Between Operations Research and Business Analysis&quot;] [[Informs|OR Exchange / Informs]] (April 2011).
* Farris, Adam, [http://www.analytics-magazine.org/november-december-2011/695-how-big-data-is-changing-the-oil-a-gas-industry &quot;How Big Data is Changing the Oil &amp; Gas Industry&quot;] Analytics. (November / December 2012).
* Venter, Fritz and Stein, Andrew [http://www.analytics-magazine.org/november-december-2011/694-images-a-videos-really-big-data &quot;Images &amp; Videos: Reall Big Data&quot;] Analytics. (November / December 2012).
* Venter, Fritz and Stein, Andrew [http://www.analytics-magazine.org/november-december-2011/694-images-a-videos-really-big-data &quot;The Technology Behind Image Analytics&quot;] Analytics. (November / December 2012).
* Horner, Peter and Basu, Atanu, [http://www.analytics-magazine.org/januaryfebruary-2012/503-analytics-a-the-future-of-healthcare ''Analytics and the Future of Healthcare''] Analytics. (January / February 2012).
* Ghosh, Rajib, Basu, Atanu and Bhaduri, Abhijit, [http://www.analytics-magazine.org/septemberoctober-2011/402-health-care-a-analytics.html ''From ‘Sick’ Care to ‘Health’ Care''] Analytics. (July / August 2011).
* Fischer, Eric, Basu, Atanu, Hubele, Joachim and Levine, Eric, [http://www.analytics-magazine.org/march-april-2011/278-predictive-analytics-tv-ads-wanamakers-dilemma-a-analytics.html ''TV ads, Wanamaker’s Dilemma &amp; Analytics''] Analytics. (March / April 2011)
* Basu, Atanu and Worth, Tim, [http://www.analytics-magazine.org/july-august-2010/128-predictive-analytics-game-changer.html ''Predictive Analytics Practical ways to Drive Customer Service, Looking Forward''] Analytics. (July / August 2010).
* Jain, Rajeev, Basu, Atanu, and Levine, Eric, [https://www.ayata.com/pdfs/Predictive-Analytics-In-Clean-Energy.pdf ''Putting Major Energy Decisions through their Paces, A Framework for a Better Environment through Analytics''] OR/MS Today (December 2010).
* Bhaduri, Abhijit and Basu, Atanu, [http://www.ayata.com/company/resources?start=6 Predictive Human Resources, Can Math Help Improve HR Mandates in an Organization?] OR/MS Today (October 2010).
* Brown, Scott, Basu, Atanu and Worth, Tim, [http://www.analytics-magazine.org/november-december-2010/98-predictive-analytics-in-field-service ''Predictive Analytics in Field Service, Practical Ways to Drive Field Service, Looking Forward] Analytics''. (November / December 2010).
* Pease, Andrew [http://support.sas.com/resources/papers/proceedings12/165-2012.pdf ''Bringing Optimization to the Business''], SAS Global Forum 2012, Paper 165-2012 (2012).
* Wheatley, Malcolm [http://data-informed.com/underground-analytics-the-value-in-predicting-when-an-oil-pump-fails/ &quot;Underground Analytics- The Value of Predicting When an Oil Pump Fails&quot;] DataInformed, May 29, 2013.
* Presley, Jennifer [http://www.epmag.com/item/ESP-ESPs_118057 &quot;ESP for ESPs] Exploration &amp; Production Magazine, July 1, 2013
* Basu, Atanu [http://data-informed.com/prescriptive-analytics-can-reshape-fracking-oil-gas-fields/ &quot;How Prescriptive Analytics Can Reshape Fracking in Oil &amp; Gas&quot;] DataInformed, December 10, 2013.
* Basu, Atanu [http://www.wired.com/insights/2014/01/big-data-analytics-can-deliver-u-s-energy-independence/ &quot;What The Frack: U.S. Energy Prowess with Shale, Big Data Analytics&quot;] WIRED Blog. (January 2014).
* Logan, Amy [http://www.ugcenter.com/Technology/Science-Fiction-A-Fact-The-EP-World_134336/ &quot;Science Fiction Now a Fact in the E&amp;P World&quot;] Unconventional Oil &amp; Gas Center, June 2, 2014.
* Mohan, Daniel [http://www.epmag.com/item/Your-data-know-you-dont_137311/ &quot;Your Data Already Know What You Don't&quot;] Exploration &amp; Production Magazine, September, 2014.
* van Rijmenam, Mark [https://datafloq.com/read/future-big-data-use-cases-prescriptive-analytics/668&quot;The Future of Big Data? Three Use Cases of Prescriptive Analytics&quot;] Datafloq, December 29, 2014.
{{refend}}

==External links==
&lt;!-- Do not add forum, reseller, spam links to this article --&gt;
* [http://analyticsmagazine.com/ INFORMS' bi-monthly, digital magazine on the analytics profession]
* [http://www.analytics.northwestern.edu/analytics-examples/prescriptive-analytics.html Northwestern University Master of Science in Analytics]
* [http://business.gwu.edu/decisionsciences/i2sds/businessanalytics.cfm The George Washington University]
* [http://www.ayata.com AYATA]
* [http://www.youtube.com/watch?v=3l__n5zlVRQ&amp;feature=plcp Prescriptive Analytics Overview]
* Menon, Jai [http://www.youtube.com/watch?v=VtETirgVn9c &quot;Why Data Matters: Moving Beyond Prediction&quot;] IBM
* [http://www.mma.ugent.be University of Ghent Analytics program]

[[Category:Analytics]]
[[Category:Big data|analytics]]
[[Category:Business intelligence]]
[[Category:Business terms]]
[[Category:Formal sciences]]
[[Category:Health care]]</text>
      <sha1>jqxca8kcz7fjobvlyoyh0sbqjuxbkm4</sha1>
    </revision>
  </page>
  <page>
    <title>ECL (data-centric programming language)</title>
    <ns>0</ns>
    <id>31108124</id>
    <revision>
      <id>672360511</id>
      <parentid>646600042</parentid>
      <timestamp>2015-07-21T01:42:10Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>/* Hello world */ &lt;source lang=&quot;ecl&quot;&gt;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5947">{{Infobox programming language
| name                   = ECL
| developer              = [[HPCC|HPCC Systems]], LexisNexis Risk Solutions
| logo                   = 
| paradigm               = [[declarative programming|declarative]], [[structured]], [[Data-centric programming language|data-centric]]
| typing                 = [[type system#Static typing|static]], [[type system#Strong and weak typing|strong]], [[type system#Safely and unsafely typed systems|safe]]
| major implementations  = [[Windows Cluster]], [[Linux Cluster]]
| year                   = 2000
| designer               = 
| latest release version = 
| latest release date    = 
| influenced_by          = [[Prolog]], [[Pascal (programming language)|Pascal]], [[SQL]], [[Snobol4]], [[C++]], [[Clarion (programming language)|Clarion]]
| influenced             = 
| operating_system       = [[Linux]]
| license                = 
| website                = http://hpccsystems.com/
}}

'''ECL''' is a declarative, data centric programming language designed in 2000 to allow a team of programmers to process [[big data]] across a high performance computing cluster without the programmer being involved in many of the lower level, imperative decisions.&lt;ref&gt;[http://www.lexisnexis.com/risk/about/guides/program-guide.html A Guide to ECL, [[Lexis-Nexis]].]&lt;/ref&gt;&lt;ref&gt;&quot;Evaluating use of data flow systems for large graph analysis,&quot; by A. Yoo, and I. Kaplan. Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers, MTAGS, 2009&lt;/ref&gt;

== History ==
ECL was initially designed and developed in 2000 by David Bayliss as an in-house productivity tool within [[Lexis-Nexis|Seisint Inc]] and was considered to be a ‘secret weapon’ that allowed Seisint to gain market share in its data business. Equifax had an SQL-based process for predicting who would go bankrupt in the next 30 days, but it took 26 days to run the data. The first ECL implementation solved the same problem in 6 minutes. The technology was cited as a driving force behind the acquisition of Seisint by [[LexisNexis]] and then again as a major source of synergies when LexisNexis acquired ChoicePoint Inc.&lt;ref&gt;[http://www.reed-elsevier.com/mediacentre/pressreleases/2004/Pages/AcquisitionofSeisint.aspx Acquisition of Seisint]&lt;/ref&gt;

== Language constructs ==
ECL, at least in its purest form, is a declarative, data centric language. Programs, in the strictest sense, do not exist. Rather an ECL application will specify a number of core datasets (or data values) and then the operations which are to be performed on those values.

=== Hello world ===
ECL is to have succinct solutions to problems and sensible defaults. The &quot;Hello World&quot; program is characteristically short:
 &quot;Hello World&quot;.
Perhaps a more flavorful example would take a list of strings, sort them into order, and then return that as a result instead.

&lt;source lang=&quot;ecl&quot;&gt;
// First declare a dataset with one column containing a list of strings
// Datasets can also be binary, CSV, XML or externally defined structures

D := DATASET([{'ECL'},{'Declarative'},{'Data'},{'Centric'},{'Programming'},{'Language'}],{STRING Value;});
SD := SORT(D,Value);
output(SD)
&lt;/source&gt;

The statements containing a &lt;code&gt;:=&lt;/code&gt; are defined in ECL as attribute definitions. They do not denote an action; rather a definition of a term. Thus, logically, an ECL program can be read: &quot;bottom to top&quot;

 OUTPUT(SD)

What is an SD?
&lt;source lang=&quot;ecl&quot;&gt;
 SD := SORT(D,Value); 
&lt;/source&gt;
SD is a D that has been sorted by ‘Value’

What is a D?
&lt;source lang=&quot;ecl&quot;&gt;
 D := DATASET([{'ECL'},{'Declarative'},{'Data'},{'Centric'},{'Programming'},{'Language'}],{STRING Value;});
&lt;/source&gt;
D is a dataset with one column labeled ‘Value’ and containing the following list of data.

=== ECL primitives ===
ECL primitives that act upon datasets include: SORT, ROLLUP, DEDUP, ITERATE, PROJECT, JOIN, NORMALIZE, DENORMALIZE, PARSE, CHOSEN, ENTH, TOPN, DISTRIBUTE

=== ECL encapsulation ===
Whilst ECL is terse and LexisNexis claims that 1 line of ECL is roughly equivalent to 120 lines of C++ it still has significant support for large scale programming including data encapsulation and code re-use. The constructs available include: MODULE, FUNCTION, FUNCTIONMACRO, INTERFACE, MACRO, EXPORT, SHARED

=== Support for Parallelism in ECL ===
In the [[HPCC]] implementation, by default, most ECL constructs will execute in parallel across the hardware being used. Many of the primitives also have a LOCAL option to specify that the operation is to occur locally on each node.

=== Comparison to Map-Reduce ===
The Hadoop Map-Reduce paradigm actually consists of three phases which correlate to ECL primitives as follows.
{| class=&quot;wikitable&quot;
|-
! Hadoop Name/Term
! ECL equivalent
! Comments
|-
| MAPing within the MAPper	
| PROJECT/TRANSFORM
| Takes a record and converts to a different format; in the [[Hadoop]] case the conversion is into a key-value pair
|-
| SHUFFLE (Phase 1)
| DISTRIBUTE(,HASH(KeyValue))
| The records from the mapper are distributed dependent upon the KEY value
|-
| SHUFFLE (Phase 2)
| SORT(,LOCAL)
| The records arriving at a particular reducer are sorted into KEY order
|-
| REDUCE
| ROLLUP(,Key,LOCAL)
| The records for a particular KEY value are now combined together
|}

== References ==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://www.nytimes.com/2008/02/21/technology/21iht-reed.4.10279549.html Reed Elsevier to acquire ChoicePoint for $3.6 billion]
* [http://www.bloomberg.com/apps/news?pid=newsarchive&amp;sid=aBuqYZDOSPL4&amp;refer=uk  Reed Elsevier's LexisNexis Buys Seisint for $775 Mln]
* [http://www.reuters.com/finance/stocks/keyDevelopments?symbol=ENL&amp;pn=15  Reed Elsevier]

[[Category:Declarative programming languages]]
[[Category:Data-centric programming languages]]
[[Category:Big data]]</text>
      <sha1>o9qy1ynsu26wqdj35p3w9enswp30a6c</sha1>
    </revision>
  </page>
  <page>
    <title>Data literacy</title>
    <ns>0</ns>
    <id>39675445</id>
    <revision>
      <id>660699906</id>
      <parentid>631222888</parentid>
      <timestamp>2015-05-04T03:21:10Z</timestamp>
      <contributor>
        <username>Daniel Mietchen</username>
        <id>322970</id>
      </contributor>
      <comment>{{Literacy}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4120">'''Data literacy''' is the ability to read, create and communicate data as information and has been formally described in varying ways. Discussion of the skills inherent to data literacy and possible instructional methods have emerged as [[data collection]] becomes routinized and talk of [[data analysis]] and [[big data]] has become commonplace in the news, business,&lt;ref&gt;{{cite book|last=Hey, A. J., Tony Hey, Tansley, S. and Tolle, K., Eds.|title=The fourth paradigm: data-intensive scientific discovery.|year=2009|publisher=Microsoft}}&lt;/ref&gt; government&lt;ref&gt;{{cite web|title=Open Data Philly|url=http://opendataphilly.org/|accessdate=14 June 2013}}&lt;/ref&gt;  and society in countries across the world.&lt;ref&gt;{{cite journal|last=Na, L., &amp; Yan, Z.|title=Promote Data-intensive Scientific Discovery, Enhance Scientific and Technological Innovation Capability: New Model, New Method, and New Challenges Comments on&quot; The Fourth Paradigm: Data-intensive Scientific Discovery|journal=Bulletin of Chinese Academy of Sciences|year=2013|volume=1|issue=16}}&lt;/ref&gt;

==Related terms==

Data literacy focuses on the ability to build knowledge from data, and to communicate that meaning to others. It is related to other fields, including:
* [[Media literacy]]
* [[Information literacy]]
* [[New literacies]]
* [[Numeracy]]
* [[Transliteracy]]
* [[21st-century skills]]

== Definitions ==
* A traditional view emphasizes the numeric, statistical nature of data as information, including &quot;... understanding what data mean, including how to read graphs and charts appropriately, draw correct conclusions from data, and recognize when data are being used in misleading or inappropriate ways&quot;.&lt;ref&gt;{{cite journal|last=Carlson, J. R., Fosmire, M., Miller, C., Sapp Nelson, M.|title=Determining Data Information Literacy Needs: A Study of Students and Research Faculty|journal=Libraries Faculty and Staff Scholarship and Research|year=2011|volume=23|url=http://docs.lib.purdue.edu/lib_fsdocs/23}}&lt;/ref&gt;
* A more progressive view describes data literacy as &quot;... the ability to: formulate and answer questions using data as part of evidence-based thinking; use appropriate data, tools, and representations to support this thinking;interpret information from data; develop and evaluate data-based inferences and explanations;and use data to solve real problems and communicate their solutions.&quot;&lt;ref&gt;{{cite journal|last=Vahey, P., Yarnall, L., Patton, C., Zalles, D., &amp; Swan, K.|title=Mathematizing middle school: Results from a cross-disciplinary study of data literacy.|journal=American Educators Research Association Annual Conference|date=April 2006|volume=5}}&lt;/ref&gt;
* A workforce-focused example includes varying technical and digital formats by describing data literacy as &quot;... competence in finding, manipulating, managing, and interpreting data, including not just numbers but also text and images.&quot;&lt;ref&gt;{{cite web|last=Harris|first=Jeanne|title=Data Is Useless Without the Skills to Analyze It|url=http://blogs.hbr.org/cs/2012/09/data_is_useless_without_the_skills.html|work=Harvard Business Review|accessdate=14 June 2013}}&lt;/ref&gt;
==Applications==
* Journalism&lt;ref&gt;{{cite web|title=Become Data Literate in 3 Simple Steps|url=http://datajournalismhandbook.org/1.0/en/understanding_data_0.html}}&lt;/ref&gt;
* Education&lt;ref&gt;{{cite web|title=Data Literacy|url=http://ites.ncdpi.wikispaces.net/Data+Literacy}}&lt;/ref&gt;&lt;ref&gt;[http://dataqualitycampaign.org/find-resources/teacher-data-literacy-its-about-time &quot;Teacher Data Literacy: It's About Time&quot;] [[Data Quality Campaign]]&lt;/ref&gt;
==List of libraries provided Data literacy==
The Massachusetts Institute of Technology’s (MIT) Data Management and Publishing tutorial, 
The EDINA Research Data Management Training (MANTRA), 
The University of Edinburgh’s Data Library and 
The University of Minnesota libraries’ Data Management Course for Structural Engineers.
== References ==
{{reflist}}
{{Literacy}}

{{DEFAULTSORT:Literacy}}
[[Category:Computing and society]]
[[Category:Technology in society]]
[[Category:Literacy]]
[[Category:Mathematics education]]
[[Category:Big data]]
{{Education-stub}}</text>
      <sha1>9vuw28lrsjib97djzrogtaufwklc6ca</sha1>
    </revision>
  </page>
  <page>
    <title>Industrial Internet</title>
    <ns>0</ns>
    <id>38232204</id>
    <revision>
      <id>670825118</id>
      <parentid>670373283</parentid>
      <timestamp>2015-07-10T13:17:55Z</timestamp>
      <contributor>
        <ip>2601:2C4:C000:BA61:58F6:2329:B9A:94EF</ip>
      </contributor>
      <comment>/* Cyber */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9500">The '''industrial internet''' is a term coined by [[Frost &amp; Sullivan]]&lt;ref&gt;{{cite web|url=http://www.frost.com/sublib/display-report.do?id=7275-01-00-00-00&amp;bdata=bnVsbEB%2BQEJhY2tAfkAxNDIwMTM3MjUwMTc0 |title=The Industrial Internet|date=2000-06-08 |accessdate=2000-06-08}}&lt;/ref&gt; and refers to the integration of complex physical machinery with networked sensors and software. The industrial Internet draws together fields such as [[machine learning]], [[big data]], the [[Internet of things]], [[Machine to machine|machine-to-machine communication]] and [[Cyber-physical system]] to ingest data from machines, analyze it (often in real-time), and use it to adjust operations.

As of 27 March 2014, the [[Industrial Internet Consortium| Industrial Internet Consortium (IIC)]] was founded by [[AT&amp;T]], [[Cisco Systems|Cisco]], [[General Electric]], [[IBM]], and [[Intel]] to bring together industry players—from multinational corporations to academia and governments—to accelerate the development, adoption and wide-spread use of Industrial Internet technologies.&lt;ref&gt;Hardy, Quentin. [http://bits.blogs.nytimes.com/2014/03/27/consortium-wants-standards-for-internet-of-things/?_php=true&amp;_type=blogs&amp;_php=true&amp;_type=blogs&amp;_r=2 &quot;Consortium Wants Standards for Internet of Things&quot;]. ''New York Times''. 27 March 2014.&lt;/ref&gt;

==Design Guidelines==
[[File:CPS for Manufacturing.png|400px|right]]
Since the meaning of Industrial Internet is similar to that of Cyber-physical Systems (CPS), the design of an Industrial Internet platform can also follow the “5C Architecture”.&lt;ref name=cPS&gt;{{cite journal|last1=Lee|first1=Jay|last2=Bagheri|first2=Behrad|last3=Kao|first3=Hung-An|title=A Cyber-Physical Systems architecture for Industry 4.0-based manufacturing systems|journal=Manufacturing Letters|date=January 2015|volume=3|pages=18–23|doi=10.1016/j.mfglet.2014.12.001}}&lt;/ref&gt; “5C” refers to the five levels of designing a CPS, which is much more clear and concrete than the commonly referenced two functional components: advanced connectivity and intelligent data analytics. These five levels are (1) Smart connection; (2) Data-to-information conversion; (3) Cyber; (4) Cognition; and (5) Configuration.

===Smart Connection===
A necessity of building connection between the cyber space and the physical space is the acquisition of data from industrial equipment. Data might be collected from different sources: add-on sensors, controllers, human inspection, maintenance log, alarm / event systems, etc. One of the challenges at this level is the diversity of equipment types and communication protocols. Machine-to-Machine techniques such as MTConnect&lt;ref&gt;{{cite journal|last1=Vijayaraghavan|first1=Athulan|last2=Sobel|first2=Will|last3=Fox|first3=Armando|last4=Dornfeld|first4=David|last5=Warndorf|first5=Paul|title=Improving machine tool interoperability using standardized interface protocols: MT connect|journal=Proceedings of 2008 ISFA|date=2008-06-23|url=http://escholarship.org/uc/item/4zs976kx}}&lt;/ref&gt; are of vital importance to serve as one of the solutions.

===Data-to-Information Conversion===
The era of Big Data does not promise self-evident insightful information. Instead, it usually means that significant effort has to be taken to “mine” knowledge among a large number of less useful information. Through the development of machine learning, statistical analysis, and data mining techniques, this level of the platform will perform intelligent analysis on the data and bring self-awareness to assets.

===Cyber===
Cyber level is what fundamentally differentiates CPS or Industrial Internet from conventional data-driven modeling frameworks. The cyber level serves as a central information hub, where information from networked machines or assets is pumped into it, and customized analytics will be performed to extract knowledge of machine conditions over time. These analytics will equip machines with the ability of self-comparison – the foundation of learning from the past. On the other hand, similarities between machine performances and other units can be measured to predict the future behavior – the ability of peer comparison, which brings more accurate prediction.

===Cognition===
The task of Cognition level is to generate a thorough understanding of the monitored machines or assets, so that the acquired insights can be visualized by users to better support decision-making.

===Configuration===
Eventually, Configuration level will realize the feedback from Cyber space to the Physical space. Resilience control system (RCS) methodology will be applied to make corrective and preventive decisions to the monitored system.

==Software platform==
'''[[Predix(software)|Predix]]''' is [[General Electric]]s's software platform for the Industrial Internet.&lt;ref&gt;{{cite web|author=GE |url=https://www.gesoftware.com/predix |title=Predix Powers Industrial-strength Apps |publisher=gesoftware.com |accessdate=2015-06-29}}&lt;/ref&gt;

==Examples==
The [[Google driverless car]] takes in environmental data from roof-mounted [[LIDAR]], uses [[Machine vision|machine-vision]] techniques to identify road geometry and obstacles, and controls the car’s throttle, brakes and steering mechanism in real-time.&lt;ref&gt;{{cite news|url=http://www.nytimes.com/2011/12/18/sunday-review/the-internet-gets-physical.html|author=Steve Lohr|title=The Internet Gets Physical|newspaper=[[The New York Times]]|accessdate=2013-08-18}}&lt;/ref&gt;

The [[Union Pacific Railroad]] mounts infrared thermometers, microphones and ultrasound scanners alongside its tracks. These sensors scan every train as it passes and send readings to the railroad’s data centers, where [[Pattern matching|pattern-matching]] software identifies equipment at risk of failure.&lt;ref&gt;{{cite web|author=Chris Murphy |url=http://www.informationweek.com/global-cio/interviews/union-pacific-delivers-internet-of-thing/240004930 |title=Union Pacific Delivers Internet Of Things Reality Check - Global Cio |publisher=Informationweek.com |date=2012-08-08 |accessdate=2013-08-18}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Chris Murphy |url=http://www.informationweek.com/global-cio/interviews/silicon-valley-needs-to-get-out-more/240143972 |title=Silicon Valley Needs To Get Out More - Global Cio - Executive |publisher=Informationweek.com |date=2012-12-07 |accessdate=2013-08-18}}&lt;/ref&gt; Falling prices for computing power and networked sensors mean that similar techniques can be applied to small, common devices like [[machine tool]]s.&lt;ref&gt;{{cite web|author=Jon Bruner |url=http://radar.oreilly.com/2012/10/listening-for-tired-machinery.html |title=Listening for tired machinery - O'Reilly Radar |publisher=Radar.oreilly.com |date=2012-10-29 |accessdate=2013-08-18}}&lt;/ref&gt; In that case scenario, following a 5C architecture defined for [[Cyber-Physical Systems]] will help to standardize the use of Industrial Internet in manufacturing and related disciplines&lt;ref name=&quot;cPS&quot;/&gt;&lt;ref&gt;{{cite web|url=http://www.imscenter.net/cyber-physical-platform|website=IMSCenter|title=IMSCenter}}&lt;/ref&gt;

==See also==
* [[Cloud-Based Design and Manufacturing|Cloud-based design and manufacturing]]
* [[Big data]]
* [[SCADA]]
* [[Industrial Ethernet]]
* [[Internet of Things]]
* [[Machine to machine]]
* [[Industrial control system]]
* [[Industry 4.0]]
* [[Intelligent Maintenance Systems]]
* [[Cyber-physical system]]

==References==
{{Reflist}}

==External links==
* Mark Fell. [http://carre-strauss.com/documents/IoT_Roadmap.pdf &quot;Roadmap for the Internet of Things - Its Impact, Architecture and Future Governance&quot;] Carré &amp; Strauss, 2014.
* Lohr, Steve. [http://www.nytimes.com/2011/12/18/sunday-review/the-internet-gets-physical.html &quot;The Internet Gets Physical&quot;] New York Times, December 17, 2011.
* Bruner, Jon. [http://radar.oreilly.com/2013/01/defining-the-industrial-internet.html &quot;Defining the industrial Internet&quot;] O'Reilly Radar, January 11, 2013.
* Murphy, Chris. [http://www.informationweek.com/global-cio/interviews/silicon-valley-needs-to-get-out-more/240143972 &quot;Silicon Valley Needs To Get Out More&quot;] InformationWeek, December 7, 2012.
* Loukides, Mike. [http://radar.oreilly.com/2012/11/to-eat-or-be-eaten.html &quot;To eat or be eaten?&quot;] O’Reilly Radar, November 30, 2012.	
* N.P., Ullekh. [http://articles.economictimes.indiatimes.com/2012-12-16/news/35837159_1_ge-internet-data &quot;How GE’s over $100 billion investment in ‘industrial internet’ will add $15 trillion to world GDP&quot;] Economic Times, December 16, 2012.
* Smarr, Larry. [http://www.nytimes.com/2011/12/06/science/larry-smarr-an-evolution-toward-a-programmable-world.html &quot;An Evolution Toward a Programmable Universe&quot;] New York Times, December 5, 2011.
* Evans, Peter C. and Marco Annunziata. [http://www.ge.com/docs/chapters/Industrial_Internet.pdf &quot;Industrial Internet: Pushing the Boundaries of Minds and Machines&quot;] GE white paper, November 26, 2012.
* Bacidore, Mike. [http://www.plantservices.com/voices/from_the_editor.html &quot;Are your prepared to work in an autonomous plant?&quot;], PlantService, March 2013.
* 
*{{cite web
  |url=http://www.industrialinternet.us
  |title= Industrial Internet 101 - A Beginner's Guide to the Next Industrial Revolution
}}
* [http://www.imscenter.net &quot;NSF Industry/University Cooperative Research Center on Intelligent Maintenance Systems&quot;]

{{DEFAULTSORT:Industrial Internet}}
[[Category:Industrial automation]]
[[Category:Industrial computing]]
[[Category:Internet of Things]]
[[Category:Technology forecasting]]
[[Category:Big data]]</text>
      <sha1>ll74cvcoevl7q9pjl1a9clp15ufg905</sha1>
    </revision>
  </page>
  <page>
    <title>Medio</title>
    <ns>0</ns>
    <id>39903888</id>
    <revision>
      <id>639645027</id>
      <parentid>633733641</parentid>
      <timestamp>2014-12-26T02:30:33Z</timestamp>
      <contributor>
        <username>Ceyockey</username>
        <id>150564</id>
      </contributor>
      <comment>removed [[Category:Companies established in 2004]]; added [[Category:Software companies established in 2004]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6205">{{Infobox company
| name     = Medio
| logo     = MedioLogo 2013.png
| type     = Corporation
| foundation       = [[Seattle]] (2004)  
| location         = 701 Pike Street Suite 1500, Seattle, WA 98101
| key_people       = Brian Lent (Founder)  Rob Lilleness (CEO)&lt;ref&gt;[http://investing.businessweek.com/research/stocks/private/person.asp?personId=1109472&amp;privcapId=24552563 &quot;Robert P. Lilleness - Businessweek&quot;]&lt;/ref&gt; 
| industry         = [[Mobile web analytics|Mobile analytics]] and [[big data]]
| homepage         = [http://www.medio.com Medio.com]
}}

'''Medio '''is a [[Business-to-business|B2B]] [[mobile web analytics|mobile analytics]] provider based in [[Seattle]], WA. The company processes pre-existing data&lt;ref&gt;{{cite news| url=http://www.unr.edu/nevada-today/news/2012/engineering-grad-high-tech-frontier | title=Engineering grad Lent shares story| author=John Trent | publisher=UNR| date=October 5, 2012| accessdate=2013-08-02}}&lt;/ref&gt; to provide historic and [[predictive analytics]]. Medio is built on a cloud-based&lt;ref&gt;{{cite news| url=http://www.bizjournals.com/seattle/blog/techflash/2011/10/ibm-takes-medio-systems-to-the-cloud.html | title=IBM powers Seattle's Medio Systems in the cloud| author=Greg Lamm | publisher=Puget Sound Business Journal | date=October 26, 2011}}&lt;/ref&gt; [[Hadoop]] platform and is designed to interpret [[big data]] for mobile enterprise. Medio has had various partners including: [[IBM]], [[Rovio Entertainment|Rovio]],&lt;ref&gt;{{cite web |author=Nicole Perlroth  |url=http://www.forbes.com/sites/nicoleperlroth/2011/08/19/angry-birds-developer-partners-with-medio-as-it-heads-into-billion-dollar-valuation-territory/ |title=Angry Birds Developer Partners With Medio As It Heads Into Billion Dollar Valuation Territory |publisher=Forbes |date=August 19, 2011 |accessdate=2013-08-30}}&lt;/ref&gt; [[Verizon]], [[T-Mobile]],&lt;ref&gt;{{cite web|last=Sharma |first=Amol |url=http://online.wsj.com/article/SB119482551027089534.html |title=T-Mobile Wagers Deal With Google Is Worth the Risk - WSJ.com |publisher=Online.wsj.com |date=November 12, 2007 |accessdate=2013-08-11}}&lt;/ref&gt; [[American Broadcasting Company|ABC]], and [[Disney]]&lt;ref name=&quot;beat&quot; /&gt;

Medio was founded in 2004 by Brian Lent, Bill Bryant, David Bluhm, and Michael Libes and employed 40 people.&lt;ref&gt;{{cite web| author=John Cook| date=October 27, 2005| url=http://www.seattlepi.com/news/article/Venture-Capital-Aiming-to-establish-mobile-search-1186148.php |title=Venture Capital: Aiming to establish mobile search |publisher=seattlepi.com  |accessdate=2013-08-14}}&lt;/ref&gt; Founded to be the '[[Google]]' of mobile search engines,&lt;ref name=&quot;times&quot; /&gt; Medio was backed by $30 Million in initial venture funding from various tech companies including: [[Accel Partners]], [[Mohr Davidow Ventures]], and [[Frazier Technology Ventures]].

Medio received $11 Million more in 2006&lt;ref name=&quot;million&quot;&gt;{{cite web|author=John Cook |url=http://www.seattlepi.com/news/article/Medio-attracts-30-million-1219933.phpm |title=Medio attracts $30 million: Seattle mobile search startup will expand |publisher=seattlepi.com |date=2006-11-15 |accessdate=2013-08-30}}&lt;/ref&gt; to create a mobile analytics search engine capable of searching for ringtones, graphics, and internet-delivered information.&lt;ref name=&quot;times&quot;&gt;{{cite web|last=Duryee |first=Tricia |url=http://seattletimes.com/html/businesstechnology/2003625160_btmedio19.html |title=Business &amp; Technology &amp;#124; Medio launches cellphone ad network &amp;#124; Seattle Times Newspaper |publisher=Seattletimes.com |date=2007-03-19 |accessdate=2013-08-30}}&lt;/ref&gt; This sparked employment to over 100 employees for some time, but in 2009 Google released their new mobile search engine. Rob Lilleness, who joined the company as President and COO in 2007 and was subsequently named CEO in 2009, took that as an opportunity to refocus as a predictive analytics and data science provider, using their recommendations engine as a key component of their newly focused company.&lt;ref&gt;[http://blog.seattlepi.com/venture/2007/07/02/medio-taps-top-executive/ &quot;Medio taps top executive&quot;], ''seattlepi.com'' July 2, 2007.&lt;/ref&gt; The shift resulted in lay-offs of much of the staff, scaling back to nearly 60 employees.&lt;ref name=&quot;allthingsd&quot;&gt;{{cite web| author=Ina Fried| date=March 17, 2011| url=http://allthingsd.com/20110317/onetime-mobile-search-player-medio-aims-for-rebirth-as-analytics-company/ |title=Onetime Mobile Search Player Medio Aims for Rebirth as Analytics Company| publisher=AllThingsD |accessdate=2013-07-26}}&lt;/ref&gt;

By the end of 2010 the company became profitable, nearly tripling its sales from previous years.&lt;ref&gt;{{cite web| date=August 19, 2008| author=Brier Dudley| url=http://seattletimes.com/html/technologybrierdudleysblog/2015957105_angry_birds_maker_rovio_taps_s.html |title=Angry Birds maker Rovio taps Seattle's Medio &amp;#124; Seattle Times Newspaper |publisher=Seattletimes.com |accessdate=2013-08-15}}&lt;/ref&gt; With the latest version of the Medio Platform and the release of products like K-Invite, Medio has grown to 70 employees with a total of $44 Million in venture funding.&lt;ref name=&quot;beat&quot;&gt;{{cite web| date= May 14, 2013| author=Dean Takahashi| url=http://venturebeat.com/2013/05/14/medio-launches-way-for-game-and-app-developers-to-get-more-users/ |title=Medio launches way for game and app developers to get more users |publisher=VentureBeat |accessdate=2013-07-06}}&lt;/ref&gt;

On July 1, 2014, Medio was acquired by Nokia and the company plans to grow their presence in Seattle.&lt;ref&gt;[http://www.forbes.com/sites/greatspeculations/2014/06/17/nokia-looks-to-bolster-its-here-maps-business-with-recent-buys/ &quot;Nokia Looks To Bolster Its HERE Maps Business With Recent Buys&quot;], ''Forbes'', June 17, 2014.&lt;/ref&gt;&lt;ref&gt;[http://www.geekwire.com/2014/nokia-acquires-mobile-analytics-startup-medio-location-platform/ &quot;Nokia acquires mobile analytics company Medio for ‘Here’ location platform&quot;], ''Geekwire'', June 12, 2014.&lt;/ref&gt;

== References ==

{{Reflist}}

== External links ==
* [http://medio.com/ Medio.com Website]

{{DEFAULTSORT:Medio}}
[[Category:Analytics]]
[[Category:Big data]]
[[Category:Mobile technology]]
[[Category:Software companies established in 2004]]</text>
      <sha1>kcpdjvbnk56xnk2ydi8nyd8y2jqnzcl</sha1>
    </revision>
  </page>
  <page>
    <title>Predictive analytics</title>
    <ns>0</ns>
    <id>4141563</id>
    <revision>
      <id>673706754</id>
      <parentid>673624528</parentid>
      <timestamp>2015-07-29T23:23:18Z</timestamp>
      <contributor>
        <username>Kuru</username>
        <id>764407</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/Flaticida|Flaticida]] ([[User talk:Flaticida|talk]]) to last version by Qwertyus</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="52199">{{Refimprove|date=June 2011}}
'''Predictive analytics''' encompasses a variety of statistical techniques from [[Predictive modelling|modeling]], [[machine learning]], and [[data mining]] that analyze current and historical facts to make [[prediction]]s about future, or otherwise unknown, events.&lt;ref name=&quot;Nyce&quot;&gt;{{citation|last=Nyce|first=Charles|title=Predictive Analytics White Paper|url=http://www.aicpcu.org/doc/predictivemodelingwhitepaper.pdf|publisher=American Institute for Chartered Property Casualty Underwriters/Insurance Institute of America|year=2007|page=1}}&lt;/ref&gt;&lt;ref name=&quot;Eckerson&quot;&gt;{{citation|last=Eckerson|first=Wayne|author-link=Wayne Eckerson|title=Extending the Value of Your Data Warehousing Investment|url=http://tdwi.org/articles/2007/05/10/predictive-analytics.aspx?sc_lang=en|publisher=The Data Warehouse Institute|date=May 10, 2007}}&lt;/ref&gt;

In business, predictive models exploit [[pattern detection|patterns]] found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding [[decision making]] for candidate transactions.&lt;ref&gt;{{cite book|last1=Coker|first1=Frank|title=Pulse: Understanding the Vital Signs of Your Business |date=2014|publisher=Ambient Light Publishing|location=Bellevue, WA|isbn=978-0-9893086-0-1|pages=30, 39, 42,more |edition=1st}}&lt;/ref&gt;

The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.

Predictive analytics is used in [[actuarial science]],&lt;ref name=&quot;Conz&quot;&gt;{{citation|last=Conz|first=Nathan|title=Insurers Shift to Customer-focused Predictive Analytics Technologies|url=http://www.insurancetech.com/business-intelligence/210600271|magazine=Insurance &amp; Technology|date=September 2, 2008}}&lt;/ref&gt; [[marketing]],&lt;ref&gt;{{citation|last=Fletcher|first=Heather|title=The 7 Best Uses for Predictive Analytics in Multichannel Marketing|url=http://www.targetmarketingmag.com/article/7-best-uses-predictive-analytics-modeling-multichannel-marketing/1#|magazine=Target Marketing|date=March 2, 2011}}&lt;/ref&gt; [[financial services]],&lt;ref&gt;{{citation|last=Korn|first=Sue|title=The Opportunity for Predictive Analytics in Finance|url= http://www.hpcwire.com/hpcwire/2011-04-21/the_opportunity_for_predictive_analytics_in_finance.html|magazine=HPC Wire|date=April 21, 2011}}&lt;/ref&gt; [[insurance]], [[telecommunications]],&lt;ref name=&quot;Barkin&quot;&gt;{{citation|last=Barkin|first=Eric|title=CRM + Predictive Analytics: Why It All Adds Up|url=http://www.destinationcrm.com/Articles/Editorial/Magazine-Features/CRM---Predictive-Analytics-Why-It-All-Adds-Up-74700.aspx|magazine=Destination CRM|date=May 2011}}&lt;/ref&gt; [[retail]],&lt;ref&gt;{{citation|last1=Das|first1=Krantik|last2=Vidyashankar|first2=G.S.|title=Competitive Advantage in Retail Through Analytics: Developing Insights, Creating Value|url=http://www.information-management.com/infodirect/20060707/1057744-1.html|magazine=Information Management|date=July 1, 2006}}&lt;/ref&gt; [[travel]],&lt;ref&gt;{{citation|last=McDonald|first=Michèle|title=New Technology Taps 'Predictive Analytics' to Target Travel Recommendations|url=http://www.travelmarketreport.com/technology?articleID=4259&amp;LP=1,|magazine=Travel Market Report|date=September 2, 2010}}&lt;/ref&gt; [[healthcare]],&lt;ref&gt;{{citation|last=Stevenson|first=Erin|title=Tech Beat: Can you pronounce health care predictive analytics?|url=http://www.times-standard.com/business/ci_19561141|newspaper=Times-Standard|date=December 16, 2011}}&lt;/ref&gt; [[Pharmaceutical company|pharmaceuticals]]&lt;ref&gt;{{citation|last=McKay|first=Lauren|title=The New Prescription for Pharma|url=http://www.destinationcrm.com/articles/Web-Exclusives/Web-Only-Bonus-Articles/The-New-Prescription-for-Pharma-55774.aspx|magazine=Destination CRM|date=August 2009}}&lt;/ref&gt; and other fields.

One of the most well known applications is [[credit scoring]],&lt;ref name=&quot;Nyce&quot; /&gt; which is used throughout [[financial services]]. Scoring models process a customer's [[credit history]], [[loan application]], customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.

==Definition==
Predictive analytics is an area of data mining that deals with [[information extraction|extracting information]] from data and using it to predict [[trend analysis|trend]]s and behavior patterns. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs.&lt;ref&gt;{{cite book|last1=Finlay|first1=Steven|title=Predictive Analytics, Data Mining and Big Data. Myths, Misconceptions and Methods|date=2014|publisher=Palgrave Macmillan|location=Basingstoke|isbn=1137379278|pages=237|edition=1st}}&lt;/ref&gt; The core of predictive analytics relies on capturing relationships between [[explanatory variable]]s and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.

Predictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, &quot;Predictive analytics—Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.&quot;&lt;ref&gt;{{cite book|last1=Siegel|first1=Eric|title=Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die|date=2013|publisher=Wiley|isbn=978-1-1183-5685-2|edition=1st}}&lt;/ref&gt;

==Types==
Generally, the term predictive analytics is used to mean [[predictive modeling]], &quot;scoring&quot; data with predictive models, and [[forecasting]]. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.

===Predictive models===
Predictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, or fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.

The available sample units with known attributes and known performances is referred to as the “training sample.” The units in other samples, with known attributes but unknown performances, are referred to as “out of [training] sample” units. The out of sample bear no chronological relation to the training sample units. For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.

===Descriptive models===
Descriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.

===Decision models===
[[Decision model]]s describe the relationship between all the elements of a decision — the known data (including results of predictive models), the decision, and the forecast results of the decision — in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.

==Applications==
Although predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.

===Analytical customer relationship management (CRM)===
Analytical [[Customer Relationship Management]] is a frequent commercial application of Predictive Analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base.  They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones.  Analytical Customer Relationship Management can be applied throughout the [[Customer lifecycle management|customers lifecycle]] ([[Customer acquisition management|acquisition]], [[Cross-selling|relationship growth]], [[Customer retention|retention]], and win-back).  Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements.

===Clinical decision support systems===
Experts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated [[clinical decision support system]]s incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: &quot;Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care.&quot;{{Citation needed|date=June 2012}}

===Collection analytics===
Many portfolios have a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.

===Cross-sell===
Often corporate organizations collect and maintain abundant data (e.g. [[customer record]]s, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient [[cross-selling|cross sales]], or selling additional products to current customers.&lt;ref name=&quot;Eckerson&quot; /&gt; This directly leads to higher profitability per customer and stronger customer relationships.

===Customer retention===
With the number of competing services available, businesses need to focus efforts on maintaining continuous [[consumer satisfaction]], rewarding [[consumer loyalty]] and minimizing [[customer attrition]]. In addition, small increases in customer retention have been shown to increase profits disproportionately. One study concluded that a 5% increase in customer retention rates will increase profits by 25% to 95%.&lt;ref&gt;{{cite web|last1=Reichheld|first1=Frederick|last2=Schefter|first2=Phil|title=The Economics of E-Loyalty|url=http://hbswk.hbs.edu/archive/1590.html|website=http://hbswk.hbs.edu/|publisher=Havard Business School|accessdate=10 November 2014}}&lt;/ref&gt; Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer's decision is almost impossible. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer’s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon.&lt;ref  name=&quot;Barkin&quot; /&gt; An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.

===Direct marketing===
When [[marketing]] consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The goal of predictive analytics is typically to lower the [[cost per order]] or [[cost per action]].

===Fraud detection===
[[Fraud]] is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent [[financial transaction|transactions]] (both offline and online), [[identity theft]]s and false [[insurance claim]]s. These problems plague firms of all sizes in many industries. Some examples of likely victims are [[Credit card fraud|credit card issuers]], insurance companies,&lt;ref name = &quot;Schiff&quot;&gt;{{citation|last=Schiff|first=Mike|title=BI Experts: Why Predictive Analytics Will Continue to Grow|url=http://tdwi.org/Articles/2012/03/06/Predictive-Analytics-Growth.aspx?Page=1|publisher=The Data Warehouse Institute|date=March 6, 2012}}&lt;/ref&gt; retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the &quot;bads&quot; and reduce a business's exposure to fraud.

Predictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. [[Mark Nigrini]] developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.&lt;ref&gt;{{cite web
| last = Nigrini
| first = Mark
| title = Forensic Analytics: Methods and Techniques for Forensic Accounting Investigations
| publisher = John Wiley &amp; Sons Inc.
| location = Hoboken, NJ
| ISBN = 978-0-470-89046-2
| date = June 2011
| url = http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470890460.html
}}&lt;/ref&gt;

The [[IRS|Internal Revenue Service (IRS) of the United States]] also uses predictive analytics to mine tax returns and identify [[tax fraud]].&lt;ref name=&quot;Schiff&quot; /&gt;

Recent{{When|date=October 2011}} advancements in technology have also introduced predictive behavior analysis for [[web fraud]] detection. This type of solution utilizes [[heuristics]] in order to study normal web user behavior and detect anomalies indicating fraud attempts.

===Portfolio, product or economy-level prediction===
Often the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.&lt;ref&gt;{{cite journal |last=Dhar |first=Vasant |title=Prediction in Financial Markets: The Case for Small Disjuncts |journal=ACM Transactions on Intelligent Systems and Technologies|date=April 2011|volume=2|issue=3|url=http://dl.acm.org/citation.cfm?id=1961191}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Dhar|first=Vasant |author2=Chou, Dashin |author3=Provost Foster|title=Discovering Interesting Patterns in Investment Decision Making with GLOWER – A Genetic Learning Algorithm Overlaid With Entropy Reduction|journal=Data Mining and Knowledge Discovery|date=October 2000|volume=4|issue=4|url=http://dl.acm.org/citation.cfm?id=593502}}&lt;/ref&gt;

===Risk management===

When employing risk management techniques, the results are always to predict and benefit from a future scenario. The [[Capital asset pricing model]] (CAP-M) &quot;predicts&quot; the best portfolio to maximize return, [[Probabilistic Risk Assessment]] (PRA)--when combined with mini-[[Delphi method|Delphi Techniques]] and statistical approaches yields accurate forecasts and [[RiskAoA]] is a stand-alone predictive tool.&lt;ref&gt;https://acc.dau.mil/CommunityBrowser.aspx?id=126070&lt;/ref&gt; These are three examples of approaches that can extend from project to market, and from near to long term. [[Underwriting]] (see below) and other business approaches identify risk management as a predictive method.

===Underwriting===

Many businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help [[underwrite]] these quantities by predicting the chances of illness, [[Default (finance)|default]], [[bankruptcy]], etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.&lt;ref name=&quot;Conz&quot; /&gt; Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.

==Technology and big data influences==
[[Big data]] is a collection of data sets that are so large and complex that they become awkward to work with using traditional [[database management]] tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include [[web log]]s, [[RFID]], [[sensor network|sensor]] data, [[social network]]s, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Big Data is the core of most predictive analytic services offered by IT organizations.&lt;ref&gt;http://www.hcltech.com/sites/default/files/key_to_monetizing_big_data_via_predictive_analytics.pdf&lt;/ref&gt; 
Thanks to technological advances in computer hardware — faster CPUs, cheaper memory, and [[Massive parallel processing|MPP]] architectures —  and new technologies such as [[Hadoop]], [[MapReduce]], and [[In-database processing|in-database]] and [[text analytics]] for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and [[unstructured data]] for new insights.&lt;ref name=&quot;Schiff&quot; /&gt; Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed &lt;ref&gt;{{cite paper
|url= http://www.eng.tau.ac.il/~bengal/DID.pdf|title=Efficient Construction of Decision Trees by the Dual Information Distance Method|author= Ben-Gal I. Dana A., Shkolnik N. and Singer|publisher= Quality Technology &amp; Quantitative Management (QTQM), 11( 1), 133-147 |year=2014}}&lt;/ref&gt;[http://www.eng.tau.ac.il/~bengal/DID.pdf]
&lt;ref&gt;{{cite paper
|url= http://www.eng.tau.ac.il/~bengal/genre_statistics.pdf|title=Peer-to-peer information retrieval using shared-content clustering|author=Ben-Gal I., Shavitt Y., Weinsberg E., Weinsberg U. |publisher= Knowl Inf Syst
DOI 10.1007/s10115-013-0619-9 |year=2014}}&lt;/ref&gt;[http://www.eng.tau.ac.il/~bengal/genre_statistics.pdf]

==Analytical Techniques==

The approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.

===Regression techniques===

[[Regression analysis|Regression]] models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there are a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below.

====Linear regression model====

The [[linear regression model]] analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.

The goal of regression is to select the parameters of the model so as to minimize the sum of the squared residuals.  This is referred to as '''[[ordinary least squares]]''' (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the [[Gauss–Markov theorem|Gauss-Markov]] assumptions are satisfied.

Once the model has been estimated we would be interested to know if the predictor variables belong in the model – i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model’s coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R² statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is &quot;explained&quot; (accounted for) by variation in the independent variables.

====Discrete choice models====

Multivariate regression (above) is generally used when the response variable is continuous and has an unbounded range. Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are [[logistic regression]], [[multinomial logit]] and [[probit]] models. Logistic regression and probit models are used when the dependent variable is [[binary numeral system|binary]].

====Logistic regression====
{{details|logistic regression}}
In a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (See Allison's Logistic Regression for more information on the theory of Logistic Regression).

The [[Wald test|Wald]] and [[likelihood-ratio test]] are used to test the statistical significance of each coefficient ''b'' in the model (analogous to the t tests used in OLS regression; see above). A test assessing the goodness-of-fit of a classification model is the &quot;percentage correctly predicted&quot;.

====Multinomial logistic regression====

An extension of the [[binary logit model]] to cases where the dependent variable has more than 2 categories is the [[multinomial logit model]]. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as [[Random multinomial logit]].

====Probit regression====

[[Probit model]]s offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.

A good way to understand the key difference between probit and logit models is to assume that there is a latent variable z.

We do not observe z but instead observe y which takes the value 0 or 1. In the logit model we assume that y follows a [[logistic distribution]]. In the probit model we assume that y follows a standard normal distribution. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.

====Logit versus probit====

The [[Probit model]] has been around longer than the [[logit model]]. They behave similarly, except that the [[logistic distribution]] tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the [[odds ratio]] is easier to interpret in the logit model.

Practical reasons for choosing the probit model over the logistic model would be:
* There is a strong belief that the underlying distribution is normal
* The actual event is not a binary outcome (''e.g.'', bankruptcy status) but a proportion (''e.g.'', proportion of population at different debt levels).

====Time series models====

[[Time series]] models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.

Time series models estimate difference equations containing stochastic components. Two commonly used forms of these models are [[autoregressive model]]s (AR) and [[Moving average model|moving average]] (MA) models. The [[Box-Jenkins]] methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the [[Autoregressive moving average model|ARMA]] (autoregressive moving average) model which is the cornerstone of stationary time series analysis. [[Autoregressive integrated moving average|ARIMA]] (autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series. Box and Jenkins suggest differencing a non stationary time series to obtain a stationary series to which an ARMA model can be applied. Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.

Box and Jenkins proposed a three stage methodology which includes: model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions.  In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.

In recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH ([[autoregressive conditional heteroskedasticity]]) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.

====Survival or duration analysis====

[[Survival analysis]] is another name for time to event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).

Censoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple [[linear regression]]. The [[normal distribution]], being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.

The assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.

An important concept in survival analysis is the [[hazard rate]], defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.

Most models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function.  A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. All these distributions are for a non-negative random variable.

Duration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are [[Kaplan-Meier]] and Cox proportional hazard model (non parametric).

====Classification and regression trees====
{{main|decision tree learning}}
Globally-optimal classification tree analysis (GO-CTA) (also called hierarchical optimal discriminant analysis) is a generalization of [[optimal discriminant analysis]] that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.

Classification and regression trees (CART) are a [[non-parametric statistics|non-parametric]] [[decision tree learning]] technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.

[[Decision trees]] are formed by a collection of rules based on variables in the modeling data set:
* Rules based on variables' values are selected to get the best split to differentiate observations based on the dependent variable
* Once a rule is selected and splits a node into two, the same process is applied to each &quot;child&quot; node (i.e. it is a recursive procedure)
* Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later [[Pruning (decision trees)|pruned]].)

Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.

A very popular method for predictive analytics is Leo Breiman's [[Random forests]].

====Multivariate adaptive regression splines====

[[Multivariate adaptive regression splines]] (MARS) is a [[Non-parametric statistics|non-parametric]] technique that builds flexible models by fitting [[piecewise]] [[linear regression]]s.

An important concept associated with regression splines is that of a knot.  Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.

In multivariate and adaptive regression splines, [[basis function]]s are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables.
Multivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.

Multivariate and adaptive regression spline approach deliberately [[overfit]]s the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.

===Machine learning techniques===

[[Machine learning]], a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including [[medical diagnostics]], [[credit card fraud detection]], [[Face recognition|face]] and [[speech recognition]] and analysis of the [[stock market]].  In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human [[cognition]] and learn from training examples to predict future events.

A brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell (1997).

====Neural networks====

[[Neural networks]] are [[Nonlinearity|nonlinear]] sophisticated modeling techniques that are able to [[Model (abstract)|model]] complex functions. They can be applied to problems of [[Time series|prediction]], [[Statistical classification|classification]] or [[Control theory|control]] in a wide spectrum of fields such as [[finance]], [[cognitive psychology]]/[[cognitive neuroscience|neuroscience]], [[medicine]], [[engineering]], and [[physics]].

Neural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training in neural networks used by different networks, [[Supervised learning|supervised]] and [[Unsupervised learning|unsupervised]] training, reinforcement learning, with supervised being the most common one.

Some examples of neural network training techniques are [[backpropagation]], quick propagation, [[Conjugate gradient method|conjugate gradient descent]], [[Radial basis function|projection operator]], Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer [[perceptron]]s, [[Self-organizing map|Kohonen network]]s, [[Hopfield network]]s, etc.

====Multilayer Perceptron (MLP)====
The Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of nonlinearly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The backpropagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule).

====Radial basis functions====
A [[radial basis function]] (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of [[neural network]]s where they are used as a replacement for the [[Sigmoid function|sigmoidal]] transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the [[Feed forward (control)|feed-forward]] networks such as the multilayer [[perceptron]].

====Support vector machines====

[[Support Vector Machine]]s (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. They are learning machines that are used to perform binary classifications and regression estimations.  They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc.

====Naïve Bayes====

[[Naive Bayes classifier|Naïve Bayes]] based on Bayes conditional probability rule is used for performing classification tasks. Naïve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the problem of ‘[[curse of dimensionality]]’ i.e. when the number of predictors is very high.

====''k''-nearest neighbours====

The [[K-nearest neighbor algorithm|nearest neighbour algorithm]] (KNN) belongs to the class of pattern recognition statistical methods. The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values.  A new sample is classified by calculating the distance to the nearest neighbouring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours; (2) the decision rule used to derive a classification from the k-nearest neighbours; and (3) the number of neighbours used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e.: as the size of the training set increases, if the observations are [[iid|independent and identically distributed (i.i.d.)]], regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error. See Devroy et al.

====Geospatial predictive modeling====

Conceptually, [[Geospatial Predictive Modeling|geospatial predictive modeling]] is rooted in the principle that the occurrences of
events being modeled are limited in distribution. Occurrences of events are neither uniform
nor random in distribution – there are spatial environment factors (infrastructure, sociocultural,
topographic, etc.) that constrain and influence where the locations of events occur.
Geospatial predictive modeling attempts to describe those constraints and influences by
spatially correlating occurrences of historical geospatial locations with environmental factors
that represent those constraints and influences. Geospatial predictive modeling is a process
for analyzing events through a geographic filter in order to make statements of likelihood for
event occurrence or emergence.

==Tools==
Historically, using predictive analytics tools—as well as understanding the results they delivered—required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists{{citation needed|date=March 2014}}. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model.&lt;ref name=&quot;Halper&quot;&gt;{{citation|last=Halper|first=Fern|title=The Top 5 Trends in Predictive Analytics|url=http://www.information-management.com/issues/21_6/the-top-5-trends-in-redictive-an-alytics-10021460-1.html|magazine=Information Management|date=November 1, 2011}}&lt;/ref&gt; Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems{{citation needed|date=March 2014}}, so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results.&lt;ref name=&quot;Eckerson&quot; /&gt; For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.&lt;ref&gt;{{citation|last=MacLennan|first=Jamie|title=5 Myths about Predictive Analytics|url=http://tdwi.org/articles/2012/05/01/5-predictive-analytics-myths.aspx|publisher=The Data Warehouse Institute|date=May 1, 2012}}&lt;/ref&gt;

There are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.

Notable open source predictive analytic tools include:
{{columns-list|2|
* [[scikit-learn]]
* [[KNIME]]
* [[OpenNN]]
* [[Orange (software)|Orange]]
* [[R (programming language)|R]]
* [[Weka (machine learning)|Weka]]
* [[GNU Octave]]
* [[Apache Mahout]]
}}

Notable commercial predictive analytic tools include:
{{columns-list|2|
* [[Alpine Data Labs]]
* [[Actuate Corporation|BIRT Analytics]]
* [[Angoss|Angoss KnowledgeSTUDIO]]
* [[SPSS|IBM SPSS Statistics]] and [[SPSS Modeler|IBM SPSS Modeler]]
* [[KXEN Inc.|KXEN Modeler]]
* [[Mathematica]]
* [[MATLAB]]
* [[Minitab]]
* [[Neural Designer]]
* [[Oracle Corporation|Oracle Data Mining (ODM)]]
* [[Pervasive Software|Pervasive]]
* [[Predixion Software]]
* [[RapidMiner]] &lt;!-- latest version is NOT open-source --&gt;
* [[RCASE]]
* [[Revolution Analytics]]
* [[SAP AG|SAP]]
* [[SAS (software)|SAS]] and [[SAS (software)#Components|SAS Enterprise Miner]]
* [[STATA]]
* [[STATISTICA]]
* [[Tibco Software|TIBCO]]

}}

The most popular commercial predictive analytics software packages according to the Rexer Analytics Survey for 2013 are IBM SPSS Modeler, SAS Enterprise Miner, and Dell Statistica &lt;http://www.rexeranalytics.com/Data-Miner-Survey-2013-Intro.html&gt;

===PMML===
In an attempt to provide a standard language for expressing predictive models, the [[Predictive Model Markup Language]] (PMML) has been proposed. Such an XML-based language provides a way for the different tools to define predictive models and to share these between PMML compliant applications. PMML 4.0 was released in June, 2009.

==Criticism==
There are plenty of skeptics when it comes to computers and algorithms abilities to predict the future, including [[Gary King (political scientist)|Gary King]], a professor from Harvard University and the director of the Institute for Quantitative Social Science.
&lt;ref&gt;{{citation|last=Temple-Raston|first=Dina|title=Predicting The Future: Fantasy Or A Good Algorithm?|url=http://www.npr.org/2012/10/08/162397787/predicting-the-future-fantasy-or-a-good-algorithm|publisher=NPR|date=Oct 8, 2012}}&lt;/ref&gt;
People are influenced by their environment in innumerable ways. Trying to understand what people will do next assumes that all the influential variables can be known and measured accurately. &quot;People's environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn't as useful as it seemed before.&quot;
&lt;ref&gt;{{citation|last=Alverson|first=Cameron|title=Polling and Statistical Models Can't Predict the Future |url=http://www.cameronalverson.com/2012/09/polling-and-statistical-models-cant.html|publisher=Cameron Alverson|date=Sep 2012}}&lt;/ref&gt;

==See also==
*[[Criminal Reduction Utilising Statistical History]]
*[[Data mining]]
*[[Learning analytics]]
*[[Odds algorithm]]
*[[Pattern recognition]]
*[[Prescriptive Analytics|Prescriptive analytics]]
*[[Predictive modeling]]
*[[RiskAoA]] a predictive tool for discriminating future decisions.

==References==
{{more footnotes|date=October 2011}}
{{Reflist|30em}}

==Further reading==
* {{cite book | author=Agresti, Alan| title=Categorical Data Analysis| location= Hoboken | publisher=John Wiley and Sons| year=2002 | isbn=0-471-36093-7}}
* Coggeshall, Stephen, Davies, John, [[Roger Jones (physicist and entrepreneur)|Jones, Roger.]], and Schutzer, Daniel, &quot;Intelligent Security Systems,&quot; in {{cite book | author=Freedman, Roy S., Flein, Robert A., and Lederman, Jess, Editors  | title=Artificial Intelligence in the Capital Markets | location= Chicago | publisher=Irwin| year=1995 | isbn=1-55738-811-3}}
* {{cite book | author=L. Devroye, L. Györfi, G. Lugosi| title=A Probabilistic Theory of Pattern Recognition| location= New York | publisher=Springer-Verlag| year=1996 | id=}}
* {{cite book | author=Enders, Walter| title=Applied Time Series Econometrics| location= Hoboken | publisher=John Wiley and Sons| year=2004 | isbn=0-521-83919-X }}
* {{cite book | author=Greene, William| title=Econometric Analysis, 7th Ed| location=London  | publisher=Prentice Hall| year=2012 | isbn=978-0-13-139538-1}}
* {{cite book | author=Guidère, Mathieu; Howard N, Sh. Argamon| title=Rich Language Analysis for Counterterrrorism| location=Berlin, London, New York  | publisher=Springer-Verlag| year=2009 | isbn=978-3-642-01140-5}}
* {{cite book | author=Mitchell, Tom| title=Machine Learning| location=New York  | publisher=McGraw-Hill| year=1997 | isbn=0-07-042807-7}}
* {{cite book | author=Siegel, Eric|title=Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die|publisher=John Wiley|year=2013|isbn=978-1-1183-5685-2}}
* {{cite book | author=Tukey, John| title=Exploratory Data Analysis| location=New York  | publisher=Addison-Wesley| year=1977 | isbn=0-201-07616-0}}
* {{cite book | author=Finlay, Steven| title=Predictive Analytics, Data Mining and Big Data. Myths, Misconceptions and Methods| location=Basingstoke  | publisher=Palgrave Macmillan| year=2014 | isbn=978-1-137-37927-6}}
* {{cite book | author=Coker, Frank| title=Pulse: Understanding the Vital Signs of Your Business| location=Bellevue, WA| publisher=Ambient Light Publishing| year=2014 | isbn=978-0-9893086-0-1}}

[[Category:Financial crime prevention]]
[[Category:Statistical models]]
[[Category:Business intelligence]]
[[Category:Insurance]]
[[Category:Big data|analytics]]
[[Category:Analytics]]</text>
      <sha1>5zwjdthnit13ebd0agbx905nxqsyzzs</sha1>
    </revision>
  </page>
  <page>
    <title>CloverETL</title>
    <ns>0</ns>
    <id>39740150</id>
    <revision>
      <id>670480355</id>
      <parentid>651032401</parentid>
      <timestamp>2015-07-08T06:30:55Z</timestamp>
      <contributor>
        <ip>86.153.148.68</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11966">{{Infobox software
| name                   = CloverETL
| logo                   = [[File:The CloverETL logo.png|180px]]
| developer              = [http://www.javlininc.com/ Javlin Inc.]
| released               = 2002
| status                 = Active
| latest_release_version = 4.0.6
| latest_release_date    = June 23, 2015
| operating_system       = [[Cross-platform]]
| genre                  = [[:Category:Extract, transform, load tools|ETL tools]]
| license                = dual [[GNU Lesser General Public License|LGPL]], commercial
| website                = http://www.cloveretl.com/
}}
{{Portal|Free software}}

'''CloverETL''' is a [[Java (programming language)|Java]]-based [[data integration]] framework designed to transform, cleanse, and distribute data into applications, databases, and [[Data Warehouse|data warehouses]]. A family of products that starts with an open source runtime engine, CloverETL's commercial offerings include a fully featured Designer and Server platform. The Server adds automation and workflow orchestration, allowing customers to deploy full production environments, with the possibility to scale to a cluster for added performance and robustness. Its goal is to be flexible and light-footed, so that it can be customized and embedded into third party applications. The [[Open Source|open source]] and commercial products are developed and supported by Javlin, a data integration software and solutions provider.

Javlin's offices are located in the Washington DC area; London, UK; and Prague, Czech Republic and serve customers in North America, Europe, Asia, and Australia. With approximately 60 employees, Javlin serves more than 3,000 customers, including five [[OEM]] partners.&lt;ref name=Topsy&gt;Topsy. N.p., n.d. Web. 20 June 2013. &lt;http://topsy.com/s/cloveretl&gt;.&lt;/ref&gt; Parts of the CloverETL platform – the Engine, Designer, and Server – can be embedded on an OEM basis.

Customers include [[Oracle Corporation|Oracle]], [[International Business Machines Corporation|Initiate Systems/IBM]], [[Comcast]], [[SUNY]], and other Fortune 500 companies.

== History ==

In 2002, the CloverETL project – named jETeL – was launched as the first Java-based open source ETL tool.{{citation needed|date=September 2013}}&lt;!-- need source for &quot;first&quot; --&gt; In 2006, it was renamed to clover.ETL, followed by CloverETL, now a registered trademark, in 2009. Starting out as a proof of concept, its purpose was to bring the performance and functionality of big enterprise [[Extract, transform, load|ETL tools]] to regular users who, at the time, did not have access to enterprise-level systems. Over time, it evolved into a data integration toolset ranging from the original core library (CloverETL Engine) to a full-fledged enterprise platform.

The CloverETL Engine is offered for free under [[GNU Lesser General Public License|LGPL]] with vendor support for the open-source ETL community.&lt;ref name=Elucidates&gt;Roy, Krishna. &quot;Javlin Elucidates CloverETL Strategy as It Continues to Take Aim at Data Integration.&quot; MIS Impact Report (2013): 1–4.&lt;/ref&gt; In 2010, a visual data transformation designer was also made public for free use.

Javlin, the official developer and support of CloverETL, was founded in 2005 under the name “Javlin Consulting”. The company’s founder and president, David Pavlis, is also the creator of CloverETL.

== Architecture ==

CloverETL is a [[Java (programming language)|Java]]-based ETL tool with [[Open Source|open source]] components. It is either used in standalone mode – as a command-line or server application – or embedded in other applications – as a Java library.  CloverETL is accompanied by the CloverETL Designer [[graphical user interface]] available as either an [[Eclipse (software)|Eclipse]] plug-in or standalone application.

A [[data transformation]] in CloverETL is represented by a transformation dataflow, or graph, containing a set of interconnected components joined by edges. A component can either be a source (reader), a transformation (reformat, sort, filter, joiner, etc.) or a target (writer). The edges act as pipes, transferring data from one component to another. Each edge has a certain metadata assigned to it that describes the format of the data it transfers. The transformation graphs are represented in  [[XML]] files and can be dynamically generated.

Each component runs in a separate [[Thread (computer science)|thread]] and acts either as a consumer or a producer. This is used to drive data through the transformation for both simple and complex graphs and makes the platform extendable by building custom components, connections etc. Transformation graphs can then be combined into a jobflow, which defines the sequence in which the individual graphs are executed.

===Fundamental aspects===
* ''Java based'' – supported platforms include [[Windows]], [[Unix]], [[Linux]], [[OS X]] and others
* ''Visual design'' – data transformations are designed visually in the CloverETL Designer (based on [[Eclipse java]])
* ''XML-based resources'' – resources such as graphs, connections, [[metadata]], etc. are stored in XML format
* ''Engine based'' – deploy a data transformation engine that executes transformation prescriptions
* ''CloverETL Transformation Language (CTL) ''–  A data-oriented programming language used to define business logic for data transformations. Offers direct access to data and functions. Syntax highlighting,    code assist, and automatic code generation included.
* ''Performance'' – utilizes multiple [[Central processing unit|CPUs]]/cores and can run on a cluster of computers to increase performance – see [[Massively parallel (computing)]]
* ''Transaction-oriented setups'' – Web-services, [[Service Oriented Architecture|SOA]], [[Enterprise service bus|ESB]]

The Server version of CloverETL supports parallel execution of transformations and runs inside a JavaEE [[Web container|application container]].

== Suite of Products ==

* ''[http://sourceforge.net/projects/cloveretl/ CloverETL Engine]'' – the core for running data transformation graphs- available under LGPLv2 or commercial license (consulting)
* ''CloverETL Designer'' – a commercial visual data integration tool for standalone or enterprise, used to design and execute transformation graphs
* ''CloverETL Server'' – an enterprise automation and monitoring data integration platform. Offers features such as workflows, scheduling, monitoring, user management, or real-time ETL abilities.
* ''CloverETL Cluster'' – an offering for big data, parallel data processing, and robustness – uses a pipeline for parallel data processing

=== Commercial Extras  ===

These come packaged with all commercial licenses.

* ''CloverETL Data Quality''– a data profiling and validation extension for data quality tasks and assessing the current condition of data quality

Open Source solutions typically appeal to [[independent software vendor]]s (ISVs) and [[systems integrator]]s (SIs) who see these solutions as attractive alternatives to writing code.&lt;ref name=Vendors&gt;&quot;Data Integration Vendors.&quot; Adeptia. N.p., n.d. Web. 20 June 2013. &lt;http://www.adeptia.com/products/etl_vendor_comparison.html&gt;.&lt;/ref&gt; Products can be embedded into solutions for [[Enterprise Service Bus]] (ESB), [[Business Intelligence]] (BI), etc.&lt;ref name=GoodData&gt;&quot;GoodData Selects CloverETL to Enrich Data Integration – GoodData.&quot; GoodData. N.p., 6 December 2012. Web. 20 June 2013. &lt;http://www.gooddata.com/in-the-news/gooddata-selects-cloveretl-to-enrich-data-integration/&gt;&lt;/ref&gt;
&lt;ref name= &quot;Research&quot;&gt;Wang, Qian. &quot;Research of ETL on University Data Exchange Platform.&quot; IEEE Xplore. N.p., n.d. Web. 20 June 2013. &lt;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&gt;.&lt;/ref&gt;

CloverETL is embedded in the [[Oracle Corporation|Oracle Endeca Information Discovery Integrator]] as well as [[GoodData]] CloudConnect&lt;ref name=GoodData /&gt;&lt;ref name=OIBEE&gt;&quot;Oracle Endeca Information Discovery- CloverETL.&quot; OBIEE, Endeca and ODI. N.p., 25 Oct. 2012. Web. 20 June 2013. &lt;http://www.varanasisaichand.com/2012/10/oracle-endeca-information-discovery.html&gt;.&lt;/ref&gt;&lt;ref name=Oracle&gt;&quot;Introduction – Oracle Identity Analytics Business Administrator's Guide.&quot; Oracle. N.p., n.d. Web. 20 June 2013. &lt;http://docs.oracle.com/cd/E27119_01/doc.11113/e23124/businessadministratorsguideprintable23.html&gt;.&lt;/ref&gt;&lt;ref name=Endeca&gt;&quot;Endeca – Information Discovery Integrator (CloverETL).&quot; GerardNicocom Weblog RSS. N.p., n.d. Web. 20 June 2013. &lt;http://gerardnico.com/wiki/cloveretl/cloveretl&gt;.&lt;/ref&gt;

===CloverETL Community Edition===
The CloverETL Community Edition is based on the Open Source transformation engine and also includes a limited CloverETL Designer. It is for users with modest data transformations and ETL requirements. The CloverETL Community Edition is free. The current version of CloverETL Community comes with a Graphic User Interface (GUI). In the past, the Community Edition used a command line style prompt to create and design data management projects.

CloverETL Community is Java-based and has been deployed on the following Operating System platforms: Linux both 32 &amp; 64 bit), Windows (both 32 &amp; 64 bit), HP-UX, AIX, AS/400 (IBM System I), Solaris, and Mac OS X. The Community edition contains connectors for the following data sources: text file delimited, fix-length and combined, XML, XLS, RDBMS through JDBC, WebServices through REST/SOAP protocols, JMS, LDAP, dBase/FoxBase/FoxPro, bulk-loaders for Oracle, DB2, MS SQL, Informix, MySQL and PostgreSQL, and QuickBase.&lt;ref&gt;Gutierrez, Jeremiah, Kent Lawson, Eddie Molina, Nestor Rodriguez. “Data Warehousing Tool Evaluation – ETL Focused.&quot; Southwest Decision Sciences Institute. 2012. 8-9. &lt;http://www.swdsi.org/swdsi2012/proceedings_2012/papers/Papers/PA151.pdf&gt;&lt;/ref&gt;

With the Community Edition, users have access to the transformation components that allow them to accomplish common data transformations tasks such as reformatting, filtering, and sorting data. Users also can use available components for aggregating, merging, or deduplicating data. The CloverETL Community Edition provides the Hash Join component and allows use of the DBExecute, System Execute, and HTTPConector components as well.

== Partners ==

* [[GoodData]]
* [[IBM]]
* [[Oracle Corporation|Oracle]]
* [[MuleSoft]]
* AddressDoctor
* DataMotion
* ProcessGold

== Technical specifications ==

* [[Java (programming language)|Java]]/[[JavaEE]]/[[Eclipse (software)|Eclipse]] (Java 6+)
* Supported platforms
* Windows 32/64
* Linux 32/64
* Mac OS X (64)
* Amazon AWS
* HP-UX
* AIX
* AS/400
* Solaris
* Embeddable as a library or service
* Parallel data processing / bulk &amp; transaction processing

=== Connectors ===

* CSV and text files delimited, fix-length &amp; combined
* [[XML]], large XML files support
* XLS/XLSX ([[MS Excel]])
* Most [[RDBMS]] through [[JDBC]]
* [[Amazon Redshift]], [[Amazon S3]]
* WebServices through [[XML]]/[[JSON]] protocols
* [[Hadoop]] MapReduce, [[HDFS]]
* [[Vertica|HP Vertica]]
* [[JMS]]
* [[LDAP]], [[Lotus Notes]]
* dBase/FoxBase/[[FoxPro]]
* bulk-loaders for Oracle, DB2, MS SQL, Informix, MySQL and PostgreSQL
* [[QuickBase]] ''(by Intuit)'', [[Infobright]]
* Supports remote reading/writing through FTP/SFTP/HTTP/HTTPS protocols and also from ZIP/GZIP/TAR archives

== Competitors ==

Other ETL frameworks include:&lt;ref name=Vendors /&gt;

* [[Ab Initio (company)|Ab Initio]]
* [[SQL Server Integration Services|Microsoft SSIS]]
* [[Talend Open Studio]]
* [[Pentaho Data Integration]]
* [[Informatica]]
* [[Apatar]]
* [[Astera Software]]

== References ==

{{Reflist|35em}}

== External links ==
* [http://www.cloveretl.com/ CloverETL website]
* [http://www.endeca123.com/category/data-integratorclover-etl/ Oracle Endeca123 blog] embedded CloverETL

{{DEFAULTSORT:Cloveretl}}
[[Category:Extract, transform, load tools]]
[[Category:Big data]]
[[Category:Data warehousing products]]</text>
      <sha1>omwp4dw0qbaxvzcaetqbd273umdmbsk</sha1>
    </revision>
  </page>
  <page>
    <title>Compuverde</title>
    <ns>0</ns>
    <id>35236254</id>
    <revision>
      <id>628813460</id>
      <parentid>601855918</parentid>
      <timestamp>2014-10-08T17:23:16Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor/>
      <comment>Typo fixing, replaced: it’s headquarters → its headquarters using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2697">{{multiple issues|
{{Orphan|date=June 2012}}
{{advert|date=October 2013}}
}}

'''Compuverde''' is an IT company with a focus on [[big data]] storage. The company was founded by Stefan Bernbo in 1994 and still has, even though they act on the global arena, its headquarters are the southeastern part of [[Sweden]].&lt;ref&gt;http://www.trademarkia.com/compuverde-85518126.html&lt;/ref&gt;&lt;ref&gt;http://compuverde.com/&lt;/ref&gt; 
As a provider of big data storage, Compuverde is a member of the worldwide nonprofit association [[Storage Networking Industry Association|SNIA]] (Storage Networking Industry Association) which aims to work towards the goal of promoting acceptance, deployment, and confidence in storage-related architectures, systems, services, and technologies across IT and business communities.&lt;ref&gt;http://snia.org/&lt;/ref&gt;

Compuverde’s executive chairman Mikael Blomqvist is a Swedish entrepreneur and apart from his mission at Compuverde Mikael is a board member of [[Blekinge Institute of Technology]], Krigskassan and Sparbanken Kronan.&lt;ref&gt;http://www.bth.se/info/pressreleaser.nsf/sidor/2f454db5425ce3c6c125770e0035f350?OpenDocument&lt;/ref&gt;&lt;ref&gt;http://www.sparbanksstiftelsenkronan.se/website1/1.0.1.0/8/1/&lt;/ref&gt; In 1989 Mikael Blomqvist founded the cable insulation producer [[Roxtec]] where he also acted as CEO.&lt;ref&gt;http://www.roxtec.com/fileadmin/InfoBase/global/documents/Press_information_Roxtec.pdf&lt;/ref&gt;

January 2012 Compuverde, [[Blekinge Institute of Technology]] and [[Ericsson]] received recognition from the Development of Knowledge and Competence (KK-stiftelsen) in Sweden for a joint venture project on big data storage and cloud computing.&lt;ref&gt;http://compuverde.com/news-room/press-and-media/millions-to-the-cloud/&lt;/ref&gt;&lt;ref&gt;http://www.bltsydostran.se/nyheter/karlskrona/telecom-city-och-bth-forbereder-sig(3161504).gm&lt;/ref&gt;

Compuverde’s software platform enables redundant object storage using cluster of standardized servers to store 100+ [[petabytes]] of accessible data.

* Runs on commodity hardware.
* Linear scaling for extreme storage needs.
* Designed for both cloud services and corporate use.
* Hardware independent symmetric architecture &lt;ref&gt;http://compuverde.com/&lt;/ref&gt;

{| class=&quot;wikitable&quot;
|-
! Compuverde !!
|-
| Type || Privately held 
|-
| Industry || IT
|-
| Type of mark   || Service Mark 
|-
| Founded  || 1994
|-
| Founder || Stefan Bernbo
|-
| Headquarters  || Karlskrona, Blekinge, Sweden 
|-
| Area served   || Worldwide 
|-
| Key people || Stefan Bernbo (Founder,CEO,Chairman) 
Mikael Blomqvist (Executive Chairman) 
|}

==References==
&lt;references /&gt;

[[Category:Companies established in 1994]]
[[Category:Companies of Sweden]]
[[Category:Big data]]</text>
      <sha1>ari22vm0e2pwugon8052kvh196vxfz0</sha1>
    </revision>
  </page>
  <page>
    <title>Cask (company)</title>
    <ns>0</ns>
    <id>37988515</id>
    <revision>
      <id>634291041</id>
      <parentid>633584281</parentid>
      <timestamp>2014-11-17T22:56:12Z</timestamp>
      <contributor>
        <ip>50.240.223.121</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4446">
{{Infobox company
| name = Cask
| type = [[Privately held company]]
| logo = Cask logo.jpeg
| industry = [[Big Data]], [[Cloud Computing]], [[Hadoop]]
| foundation = 2011
| founders = Jonathan Gray, Nitin Motgi
| location = [[Palo Alto, California]], [[United States|USA]]
| key_people = {{nowrap begin}}Jonathan Gray &lt;small&gt;(Co-founder and CEO)&lt;/small&gt;{{wrap}}Nitin Motgi &lt;small&gt;(Co-founder)&lt;/small&gt;{{wrap}}Tom Leonard &lt;small&gt;(Sales)&lt;/small&gt;{{wrap}}Vikram Bhan &lt;small&gt;(Operations)&lt;/small&gt;{{wrap}}Boyd Davis &lt;small&gt;(COO)&lt;/small&gt;{{wrap}}Andreas Neumann &lt;small&gt;(Software)&lt;/small&gt;{{wrap}}
| products = CDAP&lt;sup&gt;TM&lt;/sup&gt;{{wrap}}COOPR&lt;sup&gt;TM&lt;/sup&gt;{{wrap}}Tigon&lt;sup&gt;TM&lt;/sup&gt;
| homepage = [http://cask.co// www.cask.co]
| footnotes =
}}

'''Cask''' is a cloud-based [[Big Data]] application platform for developers. Rather than simply providing another [[cloud service]] for writing and running [[Hadoop]] jobs, Cask allows developers to more easily build, deploy and manage Big Data applications on top of the components within the Hadoop ecosystem.&lt;ref&gt;http://gigaom.com/data/ex-yahoo-facebook-big-data-vets-launch-paas-for-hadoop/&lt;/ref&gt;

==Naming==
The word “cask” signals that Cask’s software provides containers, a way for different types of applications to run in concert and share data, for instance. “Cask is short, sweet and ‘developer-y,’” he (Jonathan Gray, CEO) said. &lt;ref&gt;http://blogs.wsj.com/venturecapital/2014/09/25/hadoop-developer-cask-formerly-continuuity-goes-open-source/&lt;/ref&gt;

==Product==
The Cask Data Application Platform (CDAP) is an open source project for distributed data and applications. CDAP is a layer of software running on top of Hadoop platforms. 
Cask Coopr is open source cluster management software that provisions, manages and scales clusters on public clouds and private clouds.
Tigon is a distributed framework built on [[Apache Hadoop]]™ and [[Apache HBase]]™ for real-time, high-throughput, low-latency data processing and analytics applications. &lt;ref&gt;http://cask.co/products/&lt;/ref&gt;

The company’s primary offering CDAP, its data application platform, helps users build apps that use Hadoop in smarter ways beyond to build a low-cost, large data warehouse.
Cask also offers Coopr, a tool that helps users quickly and easily provision clusters in the cloud, such as Hadoop. In part due to its distributed nature, Hadoop can be difficult to deploy. In fact, Cask built Coopr as an internal tool but customers started asking about it. Cooper works in public, private and OpenStack clouds, so developers can use it to provision CDAP/Hadoop in any of those locations. &lt;ref&gt;http://thenewstack.io/continuuity-becomes-cask-and-keeps-focus-on-bringing-hadoop-to-the-enterprise/&lt;/ref&gt;

Cask (formerly Continuuity) has teamed with [[AT&amp;T]] Labs on an open source project called Tigon (formerly jetStream) that pairs a high-throughput [[SQL]] database with a real-time data-processing engine. The goal is to underpin applications that can handle multiple levels of latency, consistency and analysis on streaming data. &lt;ref&gt;https://gigaom.com/2014/06/03/att-labs-continuuity-will-open-source-a-hadoop-streaming-engine-called-jetstream/&lt;/ref&gt;

==Leadership==
Papaioannou, along with cofounders Nitin Motgi and Jonathan Gray, were reportedly early adopters of the Hbase big data technology while working at companies such as Facebook and Yahoo.&lt;ref&gt;http://adtmag.com/articles/2012/10/24/continuuity-targets-big-data-developers.aspx&lt;/ref&gt;

==Funding==
Cask’s approach to Hadoop virtualization has attracted attention from renowned venture capital firm [[Andreessen Horowitz]], which participated in Series A and A2 rounds totaling $12.5 million. Also backing the data and app virtualization approach is Mike Olson, chief strategy officer at [[Cloudera]], who says that it will “expand the market by enabling new use cases and accelerating application development.&lt;ref&gt;http://www.datanami.com/2014/09/25/hadoop-data-virtualization-cask-now-open-source/&lt;/ref&gt;

==See also==
* [[Platform as a Service]]
* [[Big Data]]
* [[Cloudera]]
* [[Hadoop]]
* [[AT&amp;T]]



==References==
{{Reflist}}

==External links==
* [http://www.cask.co// Cask's Corporate Site ]
* [http://blog.cask.co/ Cask's Blog ]

[[Category:Big data]]
[[Category:Hadoop]]
[[Category:Information technology companies of the United States]]
[[Category:Privately held companies based in California]]
[[Category:Companies established in 2011]]</text>
      <sha1>arzytm64vq1gz0cwleycaspym6xvmsd</sha1>
    </revision>
  </page>
  <page>
    <title>Flytxt</title>
    <ns>0</ns>
    <id>36854280</id>
    <revision>
      <id>661809337</id>
      <parentid>661805184</parentid>
      <timestamp>2015-05-11T07:47:20Z</timestamp>
      <contributor>
        <username>Jishith</username>
        <id>1529149</id>
      </contributor>
      <comment>Updated area of operation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5418">{{Infobox company
|  name     = Flytxt 
|  type     = Private Ltd
|  logo     = [[File:Flytxt Logo.png]]
|  company_slogan   = DATA&gt;&gt;&gt;&gt;ECONOMIC VALUE
|  foundation       = 2008&lt;ref&gt;http://www.bloomberg.com/research/stocks/private/snapshot.asp?privcapId=139378144&lt;/ref&gt;
|  location         = [[Netherlands]], [[Dubai]], [[India]], [[Kenya]]
|  key_people       = Vinod Vasudevan, Group CEO &lt;br/&gt;  Prateek Kapadia, CTO&lt;ref&gt;{{cite web|title=Flytxt platform ‘live’ in Africa telecom markets|url=http://www.thehindubusinessline.com/features/smartbuy/tech-news/flytxt-platform-live-in-africa-telecom-markets/article6268665.ece|website=The Hindu Business Line|accessdate=17 October 2014}}&lt;/ref&gt;
|  industry         = [[Telecommunications]], [[Big Data]] &lt;ref&gt;http://www.technopark.org/companies/a-z-listing?controller=companies&amp;view=company&amp;id=62&lt;/ref&gt; [[Mobile marketing]], [[Mobile advertising]]
|  products         = NEON&lt;br /&gt;
QREDA&lt;br /&gt; mADmart
|  num_employees    = ~350
|  homepage         = [http://www.flytxt.com/ www.flytxt.com], 
}}

'''Flytxt ''' is a [[software]] product company that specializes in [[Big data|Big Data]] Analytics enabled solutions for telecom industry. Flytxt provides Big Data analytics powered revenue and customer experience management solutions for Communication Service Providers (CSPs) that helps the latter generate measurable economic value from their existing data. With a vision to generate over 10% Economic Value for its CSP customers, Flytxt built its platform and applications around its patent pending technologies that interpret, infer, discover and predict KPIs, insights, recommendations and actions from large volumes of subscriber data residing with CSPs using advanced analytics. &lt;ref&gt;http://communities.nasscom.in/post/deriving-measurable-economic-value-with-big-data-analytics-flytxt&lt;/ref&gt;

Flytxt has operations in [[South Asia]] and [[Africa]].&lt;ref&gt;http://www.thehindubusinessline.com/industry-and-economy/info-tech/article3832145.ece&lt;/ref&gt; [[Vinod Vasudevan]] is the Group [[Chief Executive Officer|CEO]] of Flytxt.&lt;ref&gt;http://www.siliconindia.com/shownews/Flytxt_appoints_Vinod_V_Vasudevan_as_the_group_CEO-nid-51045-cid-2.html&lt;/ref&gt;

==Products==
Flytxt has two products in the [[Big Data]] Analytics, NEON&lt;ref&gt;http://www.moneycontrol.com/news/business/flytxt-launches-neon-mobile-platformindia_326599.html&lt;/ref&gt; and QREDA.&lt;ref&gt;http://www.indiainfoline.com/Markets/News/Flytxt-launches-QREDA/5054116760&lt;/ref&gt; Flytxt also has a mobile advertising market place called mADmart.&lt;ref&gt;http://www.thehindubusinessline.com/industry-and-economy/info-tech/flytxt-wins-graham-bell-award/article5335600.ece&lt;/ref&gt;

==Achievements==

*Finalist in CommsMEA awards - ‘African Vendor of the Year’ category &lt;ref&gt;http://www.computerworld.co.nz/mediareleases/21172/flytxt-amongst-the-four-companies-shortlisted-for/&lt;/ref&gt;
*Stevie Award - Mobile Marketing Campaign of the Year &lt;ref&gt;http://www.bloomberg.com/article/2014-10-13/aE34bgwmpzzM.html&lt;/ref&gt;
*ICMG Architecture Excellence Awards &lt;ref&gt;http://live.icmgworld.com/architectureawards/2015/index.php/press-release&lt;/ref&gt;
*Leader in Marketing Analytics category at the 2014 Frost &amp; Sullivan India ICT Awards&lt;ref&gt;http://www.frost.com/prod/servlet/cpo/291232401&lt;/ref&gt;
*Flytxt NEON's entry to Exemplar quadrant of NASSCOM's Product Excellent Matrix for marketing analytics applications, 2013.&lt;ref&gt;{{cite news|title=Flytxt platform gets into NASSCOM Product Excellence Matrix|url=http://keralaitnews.com/1327/flytxt-platform-gets-into-nasscom-product-excellence-matrix|newspaper=Kerala IT News}}&lt;/ref&gt; 
*Aegis Graham Bell award, 2013&lt;ref&gt;{{cite web|title=Flytxt wins Graham Bell award|url=http://www.thehindubusinessline.com/industry-and-economy/info-tech/flytxt-wins-graham-bell-award/article5335600.ece|publisher=The Hindu Business Line}}&lt;/ref&gt; 
* Cool Vendor in Emerging Markets, 2013 by [[Gartner]]&lt;ref&gt;http://articles.timesofindia.indiatimes.com/2013-05-03/telecom/39008570_1_big-data-analytics-gartner-flytxt&lt;/ref&gt;
* Winner, Red Herring Asia Top 100 companies, 2012&lt;ref&gt;http://www.redherring.com/red-herring-asia/2012-asia-top-100/&lt;/ref&gt;
* Winner, [[Institute of Electrical and Electronics Engineers]] Cloud Computing Challenge (C3) at [[Athens]], [[Greece]], 2011&lt;ref&gt;http://www.thehindubusinessline.com/industry-and-economy/info-tech/article2698511.ece&lt;/ref&gt;
* Finalist, Top 50 Innovative Companies, [[NASSCOM]] Emerge List, 2011&lt;ref&gt;http://www.thehindu.com/news/cities/Thiruvananthapuram/article2530648.ece&lt;/ref&gt;
* Finalist, Asia Communication Awards, 2012

==Corporate social responsibility==
Flytxt has an in-house staff club named ''Konnekt'' that organises charity activities and staff welfare programs.&lt;ref&gt;{{Cite web|url = http://www.thehindu.com/features/metroplus/joyful-times/article5478384.ece|title = Joyful times}}&lt;/ref&gt;

==See also==
*[[Intent marketing]]
* [[Big data|Big Data]] 
* [[Mobile advertising|Mobile Advertising]]

==References==
{{reflist}}

==External links==
*[http://www.flytxt.com Official website]
*[http://www.financialexpress.com/news/india-is-now-a-hot-market-for-mobiles/707959/ Interview with Flytxt CEO]

{{DEFAULTSORT:Flytxt Mobile Solutions}}
[[Category:Software companies of India]]
[[Category:CRM software companies]]
[[Category:Indian companies established in 2007]]
[[Category:Big data]]
[[Category:Companies based in Thiruvananthapuram]]
[[Category:Companies of the Netherlands]]</text>
      <sha1>2z1n2z06igfagqlb8ho4ay13c26053q</sha1>
    </revision>
  </page>
  <page>
    <title>Greenplum</title>
    <ns>0</ns>
    <id>20294190</id>
    <revision>
      <id>670510942</id>
      <parentid>670160420</parentid>
      <timestamp>2015-07-08T12:42:51Z</timestamp>
      <contributor>
        <username>Slmingol</username>
        <id>18024157</id>
      </contributor>
      <minor/>
      <comment>Fixed typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3168">{{Distinguish2|[[Greenplum database]], the software developed by [[Pivotal Software]]}}

{{Infobox company|
  name   = Greenplum  |
  logo   = [[Image:greenplumlogotype.jpg|200px]] |
  type   = Division of [[Pivotal Software]] |
  foundation     = 2003 |
  location       = [[San Mateo, California|San Mateo]], [[California]], [[United States]] |
  industry       = [[Big Data]] technologies|
  products       = Unfied Analytics Platform (UAP), Database Software, Chorus Software, Enterprise-Ready Hadoop, Data Computing Appliance (DCA), Analytics Labs |
  homepage       =  &lt;!-- {{URL |www.greenplum.com}} redirect now to GoPivotal --&gt;
}}

'''Greenplum''' was a [[big data]] analytics company headquartered in [[San Mateo, California|San Mateo]], [[California]].  Greenplum was acquired by [[EMC Corporation]] in July 2010,.&lt;ref name=&quot;acquired&quot;&gt;
{{cite news 
| url = http://www.emc.com/about/news/press/2010/20100706-01.htm
| title = EMC to Acquire Greenplum |work= Press release
| date = July 6, 2010
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;

== Company ==
Greenplum, the company, was founded in September 2003 by Scott Yara and Luke Lonergan.

It was a merger of two smaller companies Metapa in [[Los Angeles]] and Didera in [[Fairfax, Virginia]].&lt;ref&gt;{{Cite news |title= Metapa Buys Didera |author= Maureen O'Gara |date= September 26, 2003 |work= Linux Business News |url= http://www0.cloudcomputingexpo.com/node/35438  }}&lt;/ref&gt;
Investors included SoundView Ventures, Hudson Ventures and Royal Wulff Ventures. A total of $20 million in funding was announced at the merger.&lt;ref&gt;{{Cite news |title= Metapa Acquires Didera and Closes Additional Funding; Industry Pioneers in High-Performance Computing Combine to Create Breakthrough Linux Database Clustering Solution for Decision Support |work= Press release |date= September 23, 2003 |url= http://www.businesswire.com/news/home/20030923005198/en/Metapa-Acquires-Didera-Closes-Additional-Funding-Industry  }}&lt;/ref&gt;
Greenplum, based in  in [[San Mateo, California]], released its [[database management system]] software in April 2005 calling it Bizgres.

In July 2006 a partnership with [[Sun Microsystems]] was announced.  Sun Microsystems was a reference architecture and used by the majority of Greenplum's customers to run its database until a transition was made to Linux in the 2009 timeframe.  Greenplum was acquired by [[EMC Corporation]] in July 2010,&lt;ref name=&quot;acquired&quot;/&gt; becoming the foundation of EMC's Big Data Division.  Greenplum's products at the time of acquisition were the Greenplum Database,  Chorus, and Data Science Labs.  Greenplum had customers in [[vertical market]]s from financial services, telecommunications, Internet, retail, transportation and pharmaceuticals industries.&lt;ref&gt;{{cite web  | url = http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/| title = ebay's two enormous data warehouses }}&lt;/ref&gt;  They included [[Silver Spring Networks]], [[Zions Bancorporation]], [[Reliance Communications]], [[NYSE Euronext]], [[Orbitz]], [[Havas Digital]], [[China Unicom]], and [[Tagged]].

==References==
{{reflist}}

[[Category:Data warehousing products]]
[[Category:Big data]]</text>
      <sha1>ku45fd0qi3dzedni17y74km3cddjodj</sha1>
    </revision>
  </page>
  <page>
    <title>MarkLogic</title>
    <ns>0</ns>
    <id>2741545</id>
    <revision>
      <id>670266742</id>
      <parentid>670261670</parentid>
      <timestamp>2015-07-06T22:41:24Z</timestamp>
      <contributor>
        <username>Stesmo</username>
        <id>98915</id>
      </contributor>
      <comment>Removed external links in the body of the article [[WP:EL]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21998">{{Infobox company |
  name   = MarkLogic Corporation|
  logo   = [[File:Marklogic-logo.PNG|221px|MarkLogic]]
|
&lt;/gallery&gt;
|
  type   = Private |
  founded     = 2001 |
  location       = [[San Carlos, California]] |
  key_people     = Gary Bloom, CEO
  Christopher Lindblad, co-founder |
  industry       = [[Software]]|
  products       = MarkLogic Essential Enterprise; MarkLogic Global Enterprise  |
  homepage       = [http://www.marklogic.com/ www.marklogic.com]
}}
MarkLogic Corporation (formerly Cerisent), or '''MarkLogic''' is an [[United States|American]] software business that develops and provides an enterprise [[NoSQL]] database, MarkLogic.&lt;ref&gt;{{cite web|url=http://www.bizjournals.com/sanjose/news/2013/04/15/top-5-biggest-fundings-in-silicon.html?page=all|title=Top 5: MarkLogic topped last week's Silicon Valley fundings|publisher=Silicon Valley Business Journal|date=4 October 2013|accessdate=27 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://gigaom.com/2013/04/10/marklogic-nets-25m-to-keep-up-enterprise-nosql-pitch/|title=MarkLogic nets $25M to keep up enterprise NoSQL pitch|publisher=Gigaom|date=4 October 2013|accessdate=27 January 2015}}&lt;/ref&gt; MarkLogic is a document-oriented database platform that has a schema-agnostic data model for storing and managing XML, JSON, RDF, and Geospatial data. It uses a distributed, scale-out architecture, provides [[ACID]] transactions, and maintains government-certified security.

The company has its headquarters in [[Silicon Valley]] with field offices in [[Chicago]], [[Frankfurt]], [[London]], [[Munich]], [[New York]], [[Paris]], [[Singapore]], [[Stockholm]], [[Tokyo]], [[Utrecht]] and [[Washington D.C.]]

==History==
The company was founded in 2001 by Christopher Lindblad,&lt;ref&gt;{{cite web|url=http://www.crunchbase.com/person/christopher-lindblad |title=Christopher Lindblad &amp;#124; Crunchbase Profile |publisher=CrunchBase|accessdate=27 January 2015}}&lt;/ref&gt; who was the Chief Architect of the Ultraseek search engine at Infoseek, and Paul Pedersen, a professor of computer science at [[Cornell University]]  and [[UCLA]], to address shortcomings with existing search and data products. At the time, it used the then-dominant [[XML]] document markup standard and [[XQuery]] as the query standard for accessing collections of documents up to hundreds of terabytes in size. Newer versions handle [[JSON]] data and use [[XSLT]], and MarkLogic 8 processes data using [[JavaScript]].&lt;ref&gt;{{cite web|url=http://newsbreaks.infotoday.com/NewsBreaks/MarkLogic--Introduces-Stable-of-New-Features-for-the-XML-Server-51064.asp|title=MarkLogic 4.0 Introduces Stable of New Features for the XML Server|publisher=Information Today|date=9 October 2008|accessdate=27 January 2015}}&lt;/ref&gt;

In May 2012, Gary Bloom joined MarkLogic as Chief Executive Officer.&lt;ref&gt;{{cite news|last=Hoge|first=Patrick|title=MarkLogic appoints Gary Bloom CEO|url=http://www.bizjournals.com/sanfrancisco/blog/2012/05/marklogic-appoints-gary-bloom-ceo.html|newspaper=San Francisco Business Times|date=17 May 2012|accessdate=27 January 2015}}&lt;/ref&gt; Prior to joining Veritas Software as CEO in 2000, Bloom held several senior positions at Oracle and was widely considered the successor to Larry Ellison.&lt;ref&gt;{{cite web|last=Foremski|first=Tom|title=Former senior Oracle exec Gary Bloom named CEO of Mark Logic|url=http://www.zdnet.com/blog/foremski/former-senior-oracle-exec-gary-bloom-named-ceo-of-mark-logic/2264|publisher=ZDnet|date=17 May 2012|accessdate=27 January 2015}}&lt;/ref&gt;

MarkLogic is privately held with investments from [[Sequoia Capital]], [[Tenaya Capital]] and Northgate Capital.&lt;ref&gt;{{cite web|last=Joyce|first=Wells|title=MarkLogic Secures New $25 Million Investment and Targets Four Primary Product Areas|url=http://www.dbta.com/Articles/Editorial/News-Flashes/MarkLogic-Secures-New-$25-Million-Investment-and-Targets-Four-Primary-Product-Areas-88961.aspx|publisher=DBTA.com|date=11 April 2013|accessdate=27 January 2015}}&lt;/ref&gt;

In 2012, MarkLogic was the vendor with the largest revenue for [[Apache Hadoop|Hadoop]]/NoSQL Software or Services, with 13 percent of total marketshare.&lt;ref&gt;{{cite web|last=Kelly|first=Jeff|title=Hadoop-NoSQL Software and Services Market Forecast 2012–2017|url=http://wikibon.org/wiki/v/Hadoop-NoSQL_Software_and_Services_Market_Forecast_2012-2017#Market_Share|publisher=Wikibon|date=16 September 2013|accessdate=27 January 2015}}&lt;/ref&gt;

For the 2012 London Olympics, the [[BBC]] used MarkLogic to power its Olympic Data Services, an application that had to be built in 12 months. &quot;Given the timescales, this project would not have been achievable using a SQL database, which would have pushed the design towards more complete modeling of the data.&quot;&lt;ref&gt;{{cite web|last=Rogers|first=David|title=Building the Olympic Data Services|url=http://www.bbc.co.uk/blogs/internet/posts/olympic_data_xml_latency|publisher=BBC|accessdate=1 August 2012}}&lt;/ref&gt; BBC broke all traffic records during the 2-week games, 2.8 Petabytes on peak day, including more than 100m video requests.&lt;ref&gt;{{cite web|last=ORiordan|first=Cait|title=The story of the digital Olympics: streams, browsers, most watched, four screens|url=http://www.bbc.co.uk/blogs/internet/posts/digital_olympics_reach_stream_stats|publisher=BBC|date=13 August 2012|accessdate=27 January 2015}}&lt;/ref&gt;

Since 1 October 2013, MarkLogic has been used to help power the U.S. government healthcare.gov site.&lt;ref&gt;{{cite web|title=Tension and Flaws Before Health Website Crash|url=http://www.nytimes.com/2013/11/23/us/politics/tension-and-woes-before-health-website-crash.html|date=26 November 2013|accessdate=27 January 2015}}&lt;/ref&gt;

Product releases:
* 2003 - Product Release: Cerisent XQE 1.0
* 2004 - Product Release: Cerisent XQE 2.0
* 2005 - Product Release: MarkLogic Server 3.0
* 2006 - Product Release: MarkLogic Server 3.1
* 2007 - Product Release: MarkLogic Server 3.2
* 2008 - Product Release: MarkLogic Server 4.0
* 2009 - Product Release: MarkLogic Server 4.1
* 2010 - Product Release: MarkLogic Server 4.2
* 2011 - Product Release: MarkLogic Server 5.0
* 2012 - Product Release: MarkLogic Server 6.0
* 2013 - Product Release: MarkLogic Server 7.0
* 2015 - Product Release: MarkLogic Server 8.0

==Licensing and Support==
MarkLogic Server is available under various licensing and delivery models. These were announced in October 2013:&lt;ref name=&quot;macfadden&quot;&gt;{{cite web|last=MacFadden|first=Gary|title=MarkLogic 7 Leads the NoSQL Class, Adding Semantics and Other Enhancements|url=http://wikibon.org/wiki/v/MarkLogic_7_Leads_the_NoSQL_Class,_Adding_Semantics_and_Other_Enhancements|publisher=Wikibon|date=30 October 2013|accessdate=27 January 2015}}&lt;/ref&gt;

* '''MarkLogic Developer:''' Free, full-featured version.&lt;ref&gt;{{cite web|last=Miller|first=Ron|title=Spotlight: MarkLogic makes developer tools available for free|url=http://www.fiercecontentmanagement.com/story/spotlight-marklogic-makes-developer-tools-available-free/2013-02-18|publisher=FierceContentManagement|date=18 February 2013|accessdate=28 January 2015}}&lt;/ref&gt; Included API's extend to all versions of MarkLogic. Not for production use

* '''MarkLogic Essential Enterprise:''' Full-featured Enterprise NoSQL database that includes search engine, replication, backup, high availability, recovery, fine-grained security, location services, and alerting. Semantics and advanced language packs are options. Available as perpetual license, term/yearly license or hourly.

* '''MarkLogic Global Enterprise:''' Version designed for use for large, globally distributed applications. Semantics, tiered storage, geospatial alerting and advanced language packs are options.

==Technology==
MarkLogic is a [[Document-oriented database|NoSQL document database]] that has evolved from its [[XML database]] roots to embrace the &quot;enterprise NoSQL&quot; label. In addition to the distributed, scale-out architecture expected from a [[NoSQL]] database, it has role-based security features, JSON storage, direct use of Apache Hadoop Distributed File System (HDFS), multiple indexing strategies and ACID consistency.&lt;ref&gt;{{cite book|title=Who's Who in NoSQL DBMSs|date=23 August 2013|publisher=Gartner|author=Nick Heudecker|edition=G00252015|author2=Merv Adrian}}&lt;/ref&gt; It is the most popular native [[XML databases|XML DBMS]] as of April 2013.&lt;ref&gt;{{cite web|url=http://db-engines.com/en/ranking/native+xml+dbms |title=DB-Engines Ranking of Native XML DBMS |publisher=DB-Engines}}&lt;/ref&gt; The product combines a database, search engine and application services together in one platform.

MarkLogic features include replication, rollback, automated failover, point-in-time recovery, backup/restore, backup to Amazon S3, JSON, can run directly on Hadoop Distributed File System, parallelized ingest, role-based security, full text search, location services, geospatial alerting, RDF triple store and SPARQL query support.&lt;ref&gt;{{cite web|last=Feinberg|first=Donald|title=Magic Quadrant for Operational Database Management Systems|url=http://www.gartner.com/technology/reprints.do?id=1-1M1BXOP&amp;ct=131021&amp;st=sb|publisher=Gartner|author2=Merv Adrian |author3=Nick Heudecker |date=21 October 2013}}&lt;/ref&gt;

MarkMail is a free public mailing list archive service that emphasizes interactivity and search analytics.&lt;ref&gt;{{cite web|last=O'Brien|first=Tim|title=Interview with Jason Hunter of MarkMail.org|url=http://broadcast.oreilly.com/2008/11/interview-with-jason-hunter-of.html|publisher=O'Reilly Community|date=24 November 2008|accessdate=27 January 2015}}&lt;/ref&gt; Every search result shows a histogram traffic chart of the messages matching the query, and also the top matching lists and senders. MarkMail started in November 2007 with approximately four million email messages. As of 24 November 2013, the service claims inclusion of 66,058,071 messages across 8,761 lists, of which 2,975 were active lists.&lt;ref&gt;{{cite web|url=http://markmail.org/browse |title=MarkMail list of lists}}&lt;/ref&gt; The archive includes complete list histories for [[Apache HTTP Server|Apache]], [[FreeBSD]], [[GNOME]], [[Extensible Messaging and Presence Protocol|Jabber]], [[Java.net]], [[KDE]], [[Mozilla]], [[MySQL]], [[OpenOffice.org]], [[Perl|Perl.org]], [[PostgreSQL]], [[Python (programming language)|Python]], [[Red Hat]], [[Ruby (programming language)|Ruby]], [[W3C]], and [[Xen]], among others.

==Sample Applications==
MarkLogic's Enterprise NoSQL database platform is widely used in publishing, government, finance and other sectors, with some hundreds of large-scale systems in production. Below are some of the organizations using MarkLogic.

* [[American Lawyer Media]]—content management, processing, publication and repurposing{{Citation needed|date=January 2015}}
* [[American Psychological Association]]—semantic search, improved search speed, and accelerated delivery of new content&lt;ref&gt;{{cite web|title=APA PsycNET® Is Now Powered by MarkLogic|url=http://www.apa.org/pubs/databases/news/2012/03/marklogic.aspx|date=March 2012|accessdate=27 January 2015}}&lt;/ref&gt;
* [[Bank of America]] [[Merrill Lynch]]&lt;ref name=&quot;bridgwater&quot;&gt;{{cite web|last=Bridgwater|first=Adrian|title=Data Is Good, 'Bidirectionalized Bitemporal' Data Is Better|url=http://www.forbes.com/sites/adrianbridgwater/2014/11/24/data-is-good-bidirectionalized-bitemporal-data-is-better/|publisher=Forbes|date=24 November 2014|accessdate=28 January 2015}}&lt;/ref&gt;
* [[BBC]]—supports the site that reported the 2012 London Olympics&lt;ref&gt;{{cite web|last=Shah|first=Sooraj|title=BBC and Press Association select MarkLogic to handle Olympics data|url=http://www.computing.co.uk/ctg/news/2183832/bbc-press-association-select-marklogic-handle-olympics/page/1|publisher=Computing|date=13 June 2012|accessdate=27 January 2015}}&lt;/ref&gt;
* [[Boeing]]—develops and maintains a mission-critical government security application&lt;ref&gt;{{cite press release |author=&lt;!--Staff writer(s); no by-line.--&gt;|title=Avalon Named MarkLogic’s Partner of the Year|url=http://www.biia.com/avalon-named-marklogics-partner-of-the-year|location=San Francisco, Calif|publisher=BIIA|date=28 April 2011|access-date=2015-01-27}}&lt;/ref&gt;
* [[Centers for Medicare and Medicaid Services]]&lt;ref&gt;{{cite web|last=Lipton, et al.|first=Eric|title=Tension and Flaws Before Health Website Crash|url=http://www.nytimes.com/2013/11/23/us/politics/tension-and-woes-before-health-website-crash.html|date=22 November 2013|accessdate=27 January 2015}}&lt;/ref&gt;
* [[The Church of Jesus Christ of Latter-day Saints]]&lt;ref name=&quot;macfadden&quot;/&gt;
* CQ Roll Call—applications that help users search and find US legislation&lt;ref&gt;{{cite press release |author=&lt;!--Staff writer(s); no by-line.--&gt;|title=BC-CA-MARKLOGIC|url=http://news.yahoo.com/bc-ca-marklogic-20110215-030050-683.html|location=San Carlos, Calif.|publisher=Yahoo! News|date=15 February 2011|access-date=2015-01-27}}&lt;/ref&gt;
* [[Dow Jones &amp; Company|Dow Jones]]—Three services planned including [[Factiva]], [[WSJ.com]] and Dow Jones Financial Services&lt;ref&gt;{{cite web|last=Quint|first=Barbara|title=Dow Jones Moving to MarkLogic Platform; Factiva First |url=http://newsbreaks.infotoday.com/NewsBreaks/Dow-Jones-Moving-to-MarkLogic-Platform-Factiva-First-86705.asp|publisher=Information Today, Inc.|date=13 December 2012|accessdate=27 January 2015}}&lt;/ref&gt;
* [[Fairfax County, Virginia]]—property records management application&lt;ref&gt;{{cite web|title=Fairfax County land—use solution puts big data on the map|url=http://gcn.com/articles/2012/11/06/fairfax-county-land-use-big-data.aspx|publisher=GCN|date=6 November 2012|accessdate=27 January 2015}}&lt;/ref&gt;
* [[Federal Aviation Administration]]—Emergency Operations Network&lt;ref&gt;{{cite web|last=Marks|first=Oliver|title=MarkLogic - Unstructured Data Supertanker|url=http://www.zdnet.com/article/marklogic-unstructured-data-supertanker/|publisher=ZDNet|date=15 March 2011|accessdate=27 January 2015}}&lt;/ref&gt;
* [[Informatics Corporation of America]]—help healthcare providers find and analyze patient information&lt;ref&gt;{{cite press release|author=&lt;!--Staff writer(s); no by-line.--&gt;|title=Informatics Corporation of America Partners with Mark Logic|url=http://www.reuters.com/article/2009/08/20/idUS203979+20-Aug-2009+BW20090820|location=Nashville, Tenn.|publisher=Reuters|date=20 August 2009|access-date=27 January 2015}}&lt;/ref&gt;
* [[J.P. Morgan]]&lt;ref name=&quot;bridgwater&quot;/&gt;
* [[Lagardère Active]]—&quot;[E]enables managers to control content, and supplement the semantic contents through automated processing, and create joins between trades in response to their needs.&quot;&lt;ref&gt;{{cite web|title= The example of Lagardère Active: An automated transformation chain|url=http://www.4dconcept.fr/en/experiences/lagardere-active-xml-content-management|publisher=4Dconcept|accessdate=28 January 2015}}&lt;/ref&gt;
* [[LexisNexis]]—legal research products&lt;ref&gt;{{cite web|last=Darrow|first=Barb|title=LexisNexis puts MarkLogic to work in big data makeover|url=https://gigaom.com/2011/12/08/lexisnexis-puts-marklogic-to-work-in-big-data-makeover/|publisher=Gigaom|date=8 December 2011|accessdate=28 January 2015}}&lt;/ref&gt;
* Lex Paradigm—Xquire component content management system&lt;ref&gt;{{cite press release|author=&lt;!--Staff writer(s); no by-line.--&gt;|title=Typefi and LexParadigm announce technology partnership|url=http://www.typefi.com/news-and-events/recent-news/131-typefi-and-lexparadigm-announce-technology-partnership|location=Exton, Penn.|publisher=Typefi|date=12 March 2013|accessdate=28 January 2015}}&lt;/ref&gt;
* [[Library of Congress]]—The largest library in the world is responsible for making its contents available for Congress and the American public. It uses MarkLogic to search, retrieve and display video, data and digitized documents from the Library’s collections.&lt;ref&gt;{{cite press release|author=&lt;!--Staff writer(s); no by-line.--&gt;|title=Government Agencies Discuss the Importance of Managing Unstructured Data at MarkLogic Government Summit|url=http://www.reuters.com/article/2010/11/11/idUS127503+11-Nov-2010+BW20101111|location=Tysons Corner, Virginia|publisher=Reuters|date=11 November 2010|accessdate=28 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Unstructured Data contained by MarkLogic|url=http://www.librarianwithdesign.com/blog/2010/11/unstructured-data-contained-by-marklogic/|publisher=Librarian with Design|date=19 November 2010|accessdate=28 January 2015}}&lt;/ref&gt;
* MBS Digital Direct—digital publishing platform&lt;ref&gt;{{cite press release |author=&lt;!--Staff writer(s); no by-line.--&gt;|title=MarkLogic and MBS Partner to Deliver New Digital Content for Publishing and eTraining|url=http://www.cnbc.com/id/100631966|location=Columbia, MO|publisher=CNBC|date=10 April 2013|access-date=2015-01-27}}&lt;/ref&gt;
* [[McGraw Hill Financial]]—prototyping program that creates new products and develop mobile applications&lt;ref&gt;{{cite press release |author=&lt;!--Staff writer(s); no by-line.--&gt;|title=World Class Organizations to Share Stories of Innovation and Success at MarkLogic 2011 User Conference|url=http://www.reuters.com/article/2011/03/22/idUS94912+22-Mar-2011+BW20110322|location=San Francisco, Calif.|publisher=Reuters|date=22 March 2011|access-date=28 January 2015}}&lt;/ref&gt;
* [[Mitchell 1]]—auto information application&lt;ref&gt;{{cite web|title=Automotive software gets boost from MarkLogic|url=http://www.kmworld.com/Articles/News/KM-In-Practice/Automotive-software-gets-boost-from-MarkLogic-75032.aspx/|publisher=KMWorld Magazine|date=20 April 2011|accessdate=28 January 2015}}&lt;/ref&gt;
* New England Journal of Medicine&lt;ref name=vb&gt;{{cite web|last=Marshall|first=Matt|title=Mark Logic secures $15M for XML content server|url=http://venturebeat.com/2007/07/17/mark-logic-secures-15-million-in-third-round-financing/|date=17 July 2007|accessdate=28 January 2015}}&lt;/ref&gt;
* [[Press Association]]—content management and publishing platform&lt;ref&gt;{{cite web|last=Heath|first=Nick|title=How PA cleared the big data hurdle at the London Olympics|url=http://www.techrepublic.com/blog/european-technology/how-pa-cleared-the-big-data-hurdle-at-the-london-olympics/|publisher=European Technology|date=30 January 2013|accessdate=28 January 2014}}&lt;/ref&gt;
* Really Strategies—RSuite content management system&lt;ref&gt;{{cite web|title=RSuite CMS 4 Now Powered By MarkLogic 7|url=http://www.econtentmag.com/Articles/News/News-Item/RSuite-CMS-4-Now-Powered-By-MarkLogic-7-93735.htm|publisher=eContent|date=10 December 2014|accessdate=28 January 2014}}&lt;/ref&gt;
* [[Royal Society of Chemistry]]—manage and publish content for its RSC Publishing site, Learn Chemistry site, and the [[Merck Index]]&lt;ref&gt;{{cite web|last=Leonard|first=John|title=A positive reaction: big data technology at the Royal Society of Chemistry|url=http://www.computing.co.uk/ctg/analysis/2305957/a-positive-reaction-big-data-technology-at-the-royal-society-of-chemistry|publisher=Computing|date=8 November 2013|accessdate=28 January 2015}}&lt;/ref&gt;
* [[Springer Science+Business Media|Springer]]—content platform&lt;ref name=kamuff&gt;{{cite web|last=Kamauff|first=Ryan|title=CTOvision Podcast, an interview with Alicia Saia of MarkLogic|url=https://ctovision.com/2013/06/ctovision-podcast-an-interview-with-alicia-saia-of-marklogic/|date=12 June 2013|accessdate=28 January 2015}}&lt;/ref&gt;
* [[Sony]]-Ebook operations.&lt;ref name=&quot;kamuff&quot;/&gt;
* [[Thomson Corporation]]&lt;ref name=&quot;vb&quot;/&gt;
* [[UBS]]&lt;ref name=&quot;bridgwater&quot;/&gt;
* [[United States Army]]&lt;ref name=&quot;vb&quot;/&gt;
* [[US Patent Office]]—speed patent application process&lt;ref&gt;{{cite web|title=US patent office embraces big data|url=https://civsourceonline.com/2013/04/25/us-patent-office-embraces-big-data|publisher=CivSource:date=25 April 2013|accessdate=28 January 2015}}&lt;/ref&gt;
* [[Warner Bros.]]—Technical Operations Platform Solution (TOPS) automates digital servicing processes&lt;ref&gt;{{cite web|last=Tribbey|first=Chris|title=Experts: Metadata More Important Than Ever|url=http://www.hollywooditsociety.com/blog/experts-metadata-more-important-than-ever/|date=1 August 2014|accessdate=28 January 2015}}&lt;/ref&gt;
* [[Wiley Publishing|Wiley]]—Strategic publishing application&lt;ref&gt;{{cite web|title=Wiley Uses Mark Logic for Custom Publishing Application|url=http://www.econtentmag.com/Articles/News/News-Item/Wiley-Uses-Mark-Logic-for-Custom-Publishing-Application-53526.htm|date=21 April 2009|accessdate=28 January 2015}}&lt;/ref&gt;
* [[Zynx Health]]—evidence-based, clinical decision support solutions for healthcare{{Citation needed|date=January 2015}}

==US Affordable Care Act==

The Centers for Medicaid and Medicare, responsible for implementation of the [[Patient Protection and Affordable Care Act]] (ACA) uses MarkLogic to power its database, including the [[Federal Data Services Hub]] and parts of Federally Facilitated Marketplace.

According to the ''New York Times,'' the main contractor for ACA originally objected to using MarkLogic.&lt;ref&gt;{{cite web|last=Lipton, et al.|first=Eric|title=Tension and Flaws Before Health Website Crash|url=http://www.nytimes.com/2013/11/23/us/politics/tension-and-woes-before-health-website-crash.html?_r=0|date=23 November 2013|accessdate=28 January 2015}}&lt;/ref&gt;
==See also==
*[[Unstructured data|Unstructured Data]]
*[[Big data]]
*[[Bitemporal]]
*[[Hadoop]]
*[[Node.js]]
*[[RDF]]
*[[SPARQL]]
*[[Semantic Web]]

==References==
{{Reflist|2}}

==Further reading ==
* Fowler, Adam. eBook, &quot;Enterprise NoSQL for Dummies&quot;. ISBN 9781118905746.
* Hunter, Jason. &quot;Inside MarkLogic Server&quot;
* McCreary, Dan, and Ann Kelly. ''Making Sense of NoSQL''. Manning Publications Co. August 2012. ISBN 9781617291074. 
* Zhang, Andy. ''Beginning Mark Logic with XQuery and MarkLogic Server''. Champion Writers, Inc. 24 June 2009. ISBN 1608300153.

==External links==
* [http://www.marklogic.com/ MarkLogic home page]

[[Category:NoSQL]]
[[Category:Software companies based in California]]
[[Category:Computer companies of the United States]]
[[Category:Companies established in 2001]]
[[Category:Companies based in San Carlos, California]]
[[Category:Big data]]
[[Category:Semantic Web]]
[[Category:XML databases]]</text>
      <sha1>ov7jroh1l90swuaclfd22ckf5xer7uh</sha1>
    </revision>
  </page>
  <page>
    <title>Hopper (company)</title>
    <ns>0</ns>
    <id>39014446</id>
    <revision>
      <id>621396088</id>
      <parentid>621395920</parentid>
      <timestamp>2014-08-15T19:47:33Z</timestamp>
      <contributor>
        <username>Hopper bunny</username>
        <id>22217666</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5354">{{Infobox company
| name             = Hopper
| logo             = [[File:Logo for Hopper.png]]
| caption          =
| trading_name     = &lt;!-- d/b/a/, doing business as - if different from legal name above --&gt;
| native_name      = &lt;!-- Company's name in home country language --&gt;
| native_name_lang = &lt;!-- Use ISO 639-2 code, e.g. &quot;fr&quot; for French. If there is more than one native name, in different languages, enter those names using {{tl|lang}}, instead. --&gt;
| romanized        =
| former type      = 
| type             = 
| traded_as        = 
| industry         = [[Big Data]], [[Travel]]
| genre            = &lt;!-- Only used with media and publishing companies --&gt;
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = 2007
| founder          = Frédéric Lalonde ([[CEO]]), Joost Ouwerkerk ([[Chief technology officer|CTO]]), Sébastien Rainville (Development)
| defunct          = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| location_city    = [[Cambridge, Massachusetts]]
| location_country = 
| locations        = &lt;!-- Number of locations, stores, offices &amp;c. --&gt;
| area_served      = North America
| key_people       =  
| products         = 
| production       = 
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| aum              = &lt;!-- Only used with financial services companies --&gt;
| assets           = 
| equity           = 
| owner            = 
| num_employees    = 
| parent           = 
| divisions        = 
| subsid           = 
| homepage         = [http://www.hopper.com/ Hopper.com]
| footnotes        = 
| intl             =
| bodystyle        =
}}
'''Hopper''' provides insightful, data-driven research to help travelers make better decisions about where to go, and when to fly and buy.&lt;ref&gt;{{cite web|title=Hopper's About page|url=http://hopper.com/corp/about.html|accessdate=22 March 2013}}&lt;/ref&gt;

==History==
Hopper is a big-data start-up&lt;ref&gt;{{cite web|last=Alspach|first=Kyle|title=Hopper raises $12M to pioneer data-powered travel planning|url=http://www.bizjournals.com/boston/blog/startups/2012/08/hopper-travel-site-raises-12-million.html|publisher=Boston Business Journal|accessdate=4 April 2013}}&lt;/ref&gt;  that was founded in [[Montreal, Canada]], in 2007 by CEO Frédéric Lalonde, CTO Joost Ouwerkerk, and developer Sébastien Rainville with the goal of becoming &quot;the [[Google]] of travel.&quot;&lt;ref&gt;{{cite web|last=Kirsner|first=Scott|title=Website hopes to add retro to travel planning|url=http://www.boston.com/business/technology/articles/2011/08/08/site_takes_retro_approach_to_travel_planning/|publisher=Boston.com|accessdate=26 March 2013}}&lt;/ref&gt;

The company is currently headquartered in [[Cambridge, Massachusetts]], in the [[Kendall Boiler and Tank Company]] building, and also maintains an office in Montreal.&lt;ref&gt;{{cite web|title=Hopper's &quot;About&quot; page|url=http://www.hopper.com/corp/about.html|accessdate=29 March 2013}}&lt;/ref&gt;

===Funding===
In 2008, the company secured $2 million in funding. In 2011, a second round of funding secured $8 million from [[Atlas Venture]] and [[Brightspark Ventures]].&lt;ref&gt;{{cite web|title=Hopper Raises $8M to Reinvent Travel Search|url=http://finance.yahoo.com/news/Hopper-Raises-8M-Reinvent-iw-718821752.html|publisher=Yahoo Finance|accessdate=22 March 2013}}&lt;/ref&gt; The next year, Hopper secured $12 million from [[OMERS Ventures]].&lt;ref&gt;{{cite web|last=Ogg|first=Erica|title=Hopper gets another $12M to organize the web’s travel data|url=http://gigaom.com/2012/08/15/hopper-gets-another-12m-to-organize-the-webs-travel-data/|publisher=GigaOM|accessdate=22 March 2013}}&lt;/ref&gt;

==Hack/reduce==
In May 2012, Hopper CEO Frédéric Lalonde and entrepreneur [[Christopher P. Lynch]] founded [[Hack/reduce]], a non-profit hacker space designed to attract and foster big-data talent in the Boston area.&lt;ref&gt;{{cite web|last=Latamore|first=Bert|title=Hopper Brings Big-Data-as-a-Service to Online Travel Planning|url=http://servicesangle.com/blog/2012/06/28/hopper-brings-big-data-as-a-service-to-online-travel-planning/|publisher=Services Angle|accessdate=22 March 2013}}&lt;/ref&gt; Hack/reduce, which is located in the same building as Hopper's offices, has partnered with [[MIT]], [[CSAIL]], and [[Harvard]],&lt;ref&gt;{{cite web|last=Rousmaniere|first=Dana|title=Harvard Business Review interview with Lelonde|url=http://blogs.hbr.org/cs/2012/10/what_could_you_accomplish_with.html|accessdate=22 March 2013}}&lt;/ref&gt; and has received a grant from Governor [[Deval Patrick]] as part of his Massachusetts Big Data Initiative.&lt;ref&gt;{{cite web|title=Governor Patrick Announces New Initiative To Strengthen Massachusetts’ Position As A World Leader In Big Data|url=http://www.mass.gov/governor/pressoffice/pressreleases/2012/2012530-governor-announces-big-data-initiative.html|work=The Official Website of the Governor of Massachusetts|accessdate=27 March 2013}}&lt;/ref&gt;

==External links==
* [http://www.hopper.com/ Official website]
* [https://www.facebook.com/pages/Hopper/155688707787821 Hopper on Facebook]
* [https://twitter.com/hoppertravel Hopper on Twitter]

==References==
{{Reflist}}

[[Category:Travel websites]]
[[Category:Travel technology]]
[[Category:Travel ticket search engines]]
[[Category:Consumer guides]]
[[Category:Big data]]
[[Category:Online companies]]
[[Category:Companies based in Boston, Massachusetts]]</text>
      <sha1>mizfcb268c5uupfd78v2zr3xty90jhx</sha1>
    </revision>
  </page>
  <page>
    <title>Hack/reduce</title>
    <ns>0</ns>
    <id>37579425</id>
    <revision>
      <id>648170797</id>
      <parentid>594901636</parentid>
      <timestamp>2015-02-21T10:42:24Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>clean up / infobox standardisation using [[Project:AWB|AWB]] (10833)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3609">{{Infobox organization
| name   = hack/reduce
| logo   = [[Image:Logo of hack-reduce.png|200px]]
| type   = [[501(c)#501(c)(3)|501(c)(3)]]
| founded_date      = 2012
| location          = [[Cambridge, Massachusetts]], USA 
| origins           = 
| key_people        = 
| area_served       = 
| product           =
| focus             = [[Big Data]]
| method            = 
| revenue           = 
| endowment         = 
| num_volunteers    = 
| num_employees     = 
| num_members       = 
| subsib            = 
| owner             = 
| Non-profit_slogan = Code Big or Go Home!
| homepage          = {{URL|www.hackreduce.org}}
| dissolved         = 
| footnotes         = 
}}

[[File:Kendall Boiler and Tank Company building.jpg|thumb|right|The [[Kendall Boiler and Tank Company]] building where hack/reduce is housed]]
'''hack/reduce''' is a 501(c)(3) non-profit created to cultivate a community of [[Big Data]] experts in [[Boston, Massachusetts|Boston]].&lt;ref name=&quot;About&quot;&gt;[http://www.hackreduce.org/about/ About]. hack/reduce {{accessdate|2012-11-08}}&lt;/ref&gt;  It is located in the historic [[Kendall Boiler and Tank Company]] building at 275 Third Street in [[Kendall Square]] in [[Cambridge, Massachusetts]].

It was founded by serial entrepreneurs [[Christopher P. Lynch]] and Frederic Lalonde in May 2012.&lt;ref name=&quot;boston1&quot;&gt;[http://www.boston.com/business/innovation/2012/11/07/big-data-center-opening-cambridge/Suutqva9ORpZ1j2n1URkMP/story.html Big data center opening in Cambridge - Business]. Boston.com {{accessdate|2012-11-08}}&lt;/ref&gt; At its founding, hack/reduce raised more than $500,000 from [[Atlas Venture|local venture capital firms]] and [[Samuel Madden (computer scientist)|industry technology leaders]].&lt;ref name=&quot;boston1&quot;/&gt; hack/reduce is a “community hacker space” for Big Data that provides high performance compute resources, large data sets and subject matter expertise to help  identify and implement Big Data projects.&lt;ref name=&quot;About&quot;/&gt; It has partnerships with [[MIT]], [[CSAIL]], [[Bentley University]]. and [[Harvard]].&lt;ref&gt;[http://www.xconomy.com/boston/2012/11/07/hackreduce-to-open-thursday-as-lynch-fires-back-at-big-data-knuckleheads/ Hack/Reduce to Open Thursday as Lynch Fires Back at Big Data &quot;Knuckleheads&quot;]. Xconomy. Retrieved on 2012-11-08.&lt;/ref&gt; Sponsors include [[Microsoft]], [[IBM]], [[GoGrid]], Massachusetts Technology Collaborative, [[Dell]], [[Atlas Venture]], [[Bessemer Venture Partners]], [[Hopper (company)|Hopper]], Bright Spark Ventures, [[Google]], and others.&lt;ref&gt;[http://www.bizjournals.com/boston/blog/startups/2012/11/big-data-center-hackreduce-cambridge.html 'Big data' center hack/reduce getting big buzz - Boston Business Journal]. Bizjournals.com {{accessdate|2012-11-08}}&lt;/ref&gt;
 
In June 2012, Massachusetts Governor Deval Patrick announced the [[Massachusetts Big Data Initiative]] which comprised corporate, academic, and government programs supporting Big Data as well as a $50,000 grant and the State's support for hack/reduce.&lt;ref&gt;[http://bostinno.com/2012/11/07/no-hacks-allowed-hackreduce-launches-thursday-to-help-boston-step-up-its-big-data-game/ hack/reduce: Boston Steps Up Its Big Data Game]. BostInno {{accessdate|2012-11-08}}&lt;/ref&gt;

==References==
{{Reflist}}

{{Coord|42|21|58|N|71|5|2|W|display=title}}

{{DEFAULTSORT:Hack reduce}}
[[Category:501(c)(3) nonprofit organizations]]
[[Category:Big data]]
[[Category:Organizations established in 2012]]
[[Category:2012 establishments in Massachusetts]]
[[Category:Economy of Boston, Massachusetts]]
[[Category:Organizations based in Cambridge, Massachusetts]]


{{nongov-org-stub}}</text>
      <sha1>lg1qqvf5vpny7ip2ama5bpg9hvkx1ng</sha1>
    </revision>
  </page>
  <page>
    <title>Oracle Big Data Appliance</title>
    <ns>0</ns>
    <id>34229379</id>
    <revision>
      <id>639585372</id>
      <parentid>624408788</parentid>
      <timestamp>2014-12-25T14:21:04Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor/>
      <comment>/* References */Remove redundant |year= parameter from CS1 citations; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7220">{{Advert|date=March 2012}}

The '''Oracle Big Data Appliance''' consists of hardware and software from [[Oracle Corporation]] designed to integrate enterprise data, both structured and unstructured.  It includes the [[Oracle Exadata]] Database Machine and the Oracle Exalytics Business Intelligence Machine, used for obtaining, consolidating and loading unstructured data into Oracle Database 11g.&lt;ref name=&quot;OBA&quot; /&gt; The product also includes an open source distribution of [[Apache Hadoop]], [[Oracle NoSQL Database]], [[Oracle Data Integrator]] with Application Adapter for Hadoop, Oracle Loader for Hadoop, an open source distribution of [[R (programming language)|R]], [[Oracle Linux]], and [[HotSpot|Oracle Java Hotspot]] Virtual Machine.&lt;ref name=&quot;BDE&quot; /&gt;

== History ==
Oracle announced the Oracle Big Data Appliance Mon, October 3 at [[Oracle OpenWorld]] 2011.&lt;br /&gt;
Oracle is notorious for acquiring other companies' software / hardware  and making its own software compatible.{{Citation needed|date=March 2012}} They have maximized their position in the field of Big Data by maximizing their platform capabilities by acquiring the web tier, the middleware, the database software, the database tier and the storage tier. This allows them to offer what they consider the total package for Big Data.

== The Challenge ==
The purpose of Oracle’s Big Data appliance is to integrate all enterprise data, structured and unstructured using a combination of hardware and software. This integration includes capturing the mountains of data from department silos, from weblogs, social media feeds, smart meters, sensors and other devices that generate massive volumes of data that are found in most enterprises. This maneuvering of data will change how business users perceive data and use it.

== Hardware ==
The major hardware components of the Big Data appliance consist of: a full rack configuration with 864GB of main memory and 432 TB of storage. A full rack consists of 18 servers nodes that include a Sun server which is made up of: 2 CPUs (6-core Intel processors), 48 GB memory per node (upgradable to 96 GB or 144 GB), 12 x 2TB disks per node, InfiniBand Networking and 10 GbE connectivity.
&lt;ref name=&quot;BBOR&quot; /&gt;

== Software ==
* '''Oracle NoSQL Database''' is a distributed, scalable, key-value database based on Oracle’s Berkeley DB Java Edition High Availability storage engine. It is reputed to have predictable levels of throughput and latency and requires minimal administrative interaction. NoSQL database will be available in both open-source and commercial versions.&lt;ref name=&quot;OND&quot; /&gt;
* '''[[Apache Hadoop]]''' is a framework that allows for the dispersed processing of large data sets across groups of computers using a simple programming model.
* '''[[Oracle Data Integrator]] with Application Adapter for Hadoop'''
* '''Oracle Loader for Hadoop (OLH)''' enables users to use Hadoop MapReduce processing to create enhanced data sets for efficient loading and analysis in Oracle Database 11g. The difference between this loader and others is that it generates Oracle internal formats to load data faster and use less database system resources.
* '''Oracle R Enterprise''' tool is the combining of the open source distribution of R, a programming language and software environment for statistical computing and publication-quality graphics (Winter, 2011) with Oracle Database 11g. Oracle R Enterprise uses the approach that the models will run in-database and process large data sets, using the Oracle Database 11g and Exadata.
* '''Oracle Linux''' is an enterprise-class Linux distribution supported by Oracle.
* '''Oracle Java Hotspot Virtual Machine''' is a core component of the Java SE platform. It implements the Java Virtual Machine Specification, and is delivered as a shared library in the Java Runtime Environment.&lt;ref name=&quot;JavaSEHotSpot&quot; /&gt;

The software available will also be sold separately, to allow customers to define their own configurations with their existing Big Data infrastructure as well as a component in the data appliance.&lt;ref name=&quot;ORR&quot; /&gt;

== How it Works ==
A simplistic view is an organization would use the Oracle Big Data Appliance (Hadoop and NoSQL) to capture the data, then use Big Data Connectors to a data warehouse where they can use Oracle Enterprise R or any other  data mining techniques to analyze the data further...

== Support ==
In partnership with Cloudera, the Hadoop software and services provider, Oracle will provide first-line support, Tier 1,  for the appliance and all software (including the Hadoop distribution and Cloudera Manager) through its issue-tracking support infrastructure ([[Metalink|My Oracle Support]]). Cloudera will handle Tier 2 and 3 support as well as training and consulting engagements.&lt;ref name=CLO /&gt;

== Additional Information ==
[http://www.idevnews.com/stories/4858/Oracle-Digs-in-On-NoSQL-Hadoop-End-to-End-Big-Data?print=1 Oracle Digs in On NoSQL, Hadoop, End-to-End Big Data ]
&lt;br /&gt;
[http://www.oracle.com/us/technologies/big-data/index.html Oracle Big Data Appliance Overview]

== References ==
&lt;references&gt;
&lt;ref name=&quot;OBA&quot;&gt;{{cite web|last=Darrow|first=Barb|title=Oracle BigData Appliance stakes big claim|url=http://gigaom.com/2011/10/03/oracle-big-data-appliance-stakes-big-claim/|accessdate=30 December 2011|date=2011-10-03}}&lt;/ref&gt;
&lt;ref name=&quot;CLO&quot;&gt;{{cite web|last=Henschen|first=Doug|title=Oracle Makes Big Data Appliance Move With Cloudera. |url=http://www.informationweek.com/news/software/info_management/232400021|publisher=Information Week|accessdate=24 January 2012|date=10 Jan 2011}}&lt;/ref&gt;
&lt;ref name=&quot;BDE&quot;&gt;{{cite web|last=Dijcks|first=Jean-Pierre|title=Oracle: Big Data for the Enterprise|url=http://resources.idgenterprise.com/original/AST-0054994_DW_US_EN_WP_BigData.pdf|publisher=Oracle Corporation|accessdate=30 December 2011}}&lt;/ref&gt;
&lt;ref name=&quot;ORR&quot;&gt;{{cite web|last=Kanaracus|first=Chris|title=Oracle Rolls Out 'Big Data' Appliance|url=http://www.cio.com/article/690884/Oracle_Rolls_Out_Big_Data_Appliance?page=1&amp;taxonomyId=3007|publisher=CIO|accessdate=30 December 2011|date=3 Oct 2011}}&lt;/ref&gt;
&lt;ref name=&quot;OND&quot;&gt;{{cite web|last=Oracle Corporation|title=ORACLE NOSQL DATABASE 11G|publisher=Oracle Corporation|accessdate=30 December 2011}}&lt;/ref&gt;
&lt;ref=&quot;IBD&quot;&gt;{{cite web|last=Powell|first=Ron|title=Incorporating Big Data into an Enterprise Information Architecture - A Q&amp;A Spotlight with Oracle's George Lumpkin|url=http://www.b-eye-network.com/view/15480|publisher=BeyeNETWORK|accessdate=30 December 2011|date=24 Oct 2011}}&lt;/ref&gt;
&lt;ref name=&quot;BBOR&quot;&gt;{{cite web|last=Winter|first=Richard|title=Big Data :Business Opportunities, Requirements and Oracle's Approach|url=http://www.oracle.com/us/corporate/analystreports/infrastructure/winter-big-data-1438533.pdf|publisher=Winter Corporation|accessdate=30 December 2011|date=December 2011}}&lt;/ref&gt;
&lt;ref name=&quot;JavaSEHotSpot&quot;&gt;{{cite web|last=Oracle Corporation|title=Java SE HotSpot at a Glance|url=http://www.oracle.com/technetwork/java/javase/tech/index-jsp-136373.html|publisher=Oracle Technology Network|accessdate=4 January 2012|date=1 Jan 2011}}&lt;/ref&gt;
&lt;/references&gt;

[[Category:Oracle Corporation]]
[[Category:Business intelligence]]
[[Category:Big data]]</text>
      <sha1>369d7t287sjk1ve4t0ftbghlwdrxv4m</sha1>
    </revision>
  </page>
  <page>
    <title>Palantir Technologies</title>
    <ns>0</ns>
    <id>27197818</id>
    <revision>
      <id>672920094</id>
      <parentid>669606333</parentid>
      <timestamp>2015-07-24T19:34:40Z</timestamp>
      <contributor>
        <ip>76.9.206.154</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="25961">{{Infobox company
| name             = Palantir Technologies
| logo             = [[File:Palantir Technologies company logo.jpg|200px]]
| type             = [[Privately held company|Private]]
| foundation       = 2004
| founders          = [[Peter Thiel]], [[Joe Lonsdale]], [[Alex Karp]], [[Stephen Cohen (entrepreneur)|Stephen Cohen]], Nathan Gettings
| location_city    = [[Palo Alto, California]]
| products         = Palantir Gotham, Palantir Metropolis
| num_employees    = 1,500&lt;ref&gt;{{cite web |title = Unlocking Secrets, if Not Its Own Value |publisher = New York Times|url = http://www.nytimes.com/2014/06/01/business/unlocking-secrets-if-not-its-own-value.html |accessdate = 2015-01-19}}&lt;/ref&gt; 
| homepage         = {{URL |https://www.palantir.com}}
}}
'''Palantir Technologies, Inc.''' is a [[private company|private]] American [[software]] and services company, specializing in [[data analysis]].  Founded in 2004, Palantir's original clients were federal agencies of the [[United States Intelligence Community]]. It has since expanded its customer base to serve state and local governments, as well as private companies in the financial and healthcare industries.&lt;ref name=&quot;historypalantir&quot;/&gt; The company is known for two software projects in particular: '''Palantir Gotham''' is used by counter-terrorism analysts at offices in the United States Intelligence Community and [[United States Department of Defense]], fraud investigators at the [[Recovery Accountability and Transparency Board]], and cyber analysts at [[Information Warfare Monitor]] (responsible for the [[GhostNet]] and the [[Shadow Network]] investigation). '''Palantir Metropolis''' is used by hedge funds, banks, and financial services firms.&lt;ref name=&quot;bare_url&quot; /&gt;&lt;ref name=&quot;npr&quot; /&gt;

CEO Alex Karp announced in 2013 that the company would not be pursuing an [[IPO]], as going public would make “running a company like ours very difficult.”&lt;ref name=&quot;teafunding&quot;/&gt; As of early 2014 the company was valued at $9 billion, according to ''[[Forbes]]'', with the magazine further explaining that the valuation made Palantir &quot;among [[Silicon Valley]]’s most valuable private technology companies.&quot;&lt;ref name=&quot;teafunding&quot;/&gt; As of December 2014 the company continued to have diverse private funders, such as [[Kenneth Langone]] and [[Stanley Druckenmiller]], [[In-Q-Tel]] of the [[CIA]], [[Tiger Global Management]], and [[Founders Fund]]. As of December 2014, Peter Thiel was Palantir's largest shareholder.&lt;ref name=&quot;teafunding&quot;/&gt; In January 2015, the company was valued at 15 billion USD after an undisclosed round of funding with 50 million USD in November 2014.&lt;ref&gt;{{Cite web|title = SEC FORM D|url = http://www.sec.gov/Archives/edgar/data/1321655/000132165514000005/xslFormDX01/primary_doc.xml|website = www.sec.gov|accessdate = 2015-05-31}}&lt;/ref&gt;

== History ==
===2003-2009: Founding and early years===
[[File:Peter Thiel TechCrunch50.jpg|thumb|right|200px|Four of the five founders had formerly been involved with [[PayPal]]. Founder and chairman [[Peter Thiel]] is the company's largest shareholder as of late 2014]]
Officially incorporated in May 2003,&lt;ref name=&quot;historypalantir&quot;&gt;{{cite web |title = A (Pretty) Complete History of Palantir |publisher = Maus Strategic Consulting |url =http://www.mausstrategicconsulting.com/1/post/2014/04/a-pretty-complete-history-of-palantir.html |accessdate = 2014-04-27}}&lt;/ref&gt; Palantir is generally considered to have been founded in 2004 by [[Peter Thiel]], [[Alex Karp]],&lt;ref&gt;{{cite web |url= http://media.palantirtech.com/videos/charlierose.html |title=charlierose |publisher= Media.palantirtech.com |date= |accessdate=2012-01-30}}&lt;/ref&gt; [[Joe Lonsdale]],&lt;ref&gt;{{cite web |url= http://www.crunchbase.com/company/palantir-technologies |title=Palantir Technologies &amp;#124; CrunchBase Profile |publisher=Crunchbase.com |date= |accessdate=2012-01-30}}&lt;/ref&gt; [[Stephen Cohen (entrepreneur)|Stephen Cohen]], and Nathan Gettings. Early investments were $2 million from the US [[Central Intelligence Agency]] venture arm [[In-Q-Tel]], and $30 million from Thiel and his firm, [[Founders Fund]].&lt;ref name=&quot;bare_url&quot;&gt;{{cite news| url=http://online.wsj.com/article/SB125200842406984303.html | work=The Wall Street Journal | title=How Team of Geeks Cracked Spy Trade | first=Siobhan | last=Gorman | date=September 4, 2009}}&lt;/ref&gt;&lt;ref name=&quot;npr&quot;&gt;{{cite web|url=http://www.npr.org/templates/story/story.php?storyId=106479613 |title=A Tech Fix For Illegal Government Snooping? |publisher=NPR |date= |accessdate=2012-01-30}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.foundersfund.com/palantir.php |title=Palantir « Founders Fund |publisher= Foundersfund.com |date= |accessdate=2012-01-30}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Evelyn Rusli |url=http://techcrunch.com/2010/06/25/palantir-the-next-billion-dollar-company-raises-90-million/ |title=Palantir: The Next Billion-Dollar Company Raises $90 Million |publisher=TechCrunch |date=2010-06-25 |accessdate=2012-01-30}}&lt;/ref&gt; Alex Karp is Palantir’s CEO.&lt;ref name=&quot;charlierose&quot;&gt;{{cite web|url=http://www.charlierose.com/guest/view/6717 |title=Alexander Karp |publisher=Charlie Rose |date=2009-08-11 |accessdate=2012-01-30}}&lt;/ref&gt; Palantir’s name comes from the &quot;[[Palantír|seeing stones]]&quot; in [[J. R. R. Tolkien]]'s fantasy books ''[[The Lord of the Rings]]'' and ''[[The Silmarillion]]''. Headquartered in [[Palo Alto, California]], the company has nine international offices and four in the United States.&lt;ref&gt;{{cite web |url=http://www.palantir.com/contact |title=contact information}}&lt;/ref&gt;

Palantir developed its technology by computer scientists and analysts from intelligence agencies over three years, through pilots facilitated by In-Q-Tel.&lt;ref&gt;{{cite web| author=Jeff Widman |url=http://entrepreneur.venturebeat.com/2009/06/05/palantir-keeps-it-lean-and-mean-on-five-year-journey-from-zero-to-150-employees/ |title=Palantir keeps it lean and mean on five-year journey from zero to 150 employees | publisher=VentureBeat |date=2009-06-05 |accessdate=2012-01-30}}&lt;/ref&gt; The software concept grew out of technology developed at [[PayPal]] to detect fraudulent activity, much of it conducted by Russian organized crime syndicates.&lt;ref name=&quot;bare_url&quot; /&gt; 
The company said computers alone using [[artificial intelligence]] could not defeat an adaptive adversary. 
Palantir  proposed using human analysts to explore data from many sources, called [[Intelligence Amplification|intelligence augmentation]].&lt;ref&gt;{{cite web|author=Ari Gesher |url= http://blog.palantirtech.com/2010/03/08/friction-in-human-computer-symbiosis-kasparov-on-chess/ |title=Palantir Technologies » Blog Archive » Friction in Human-Computer Symbiosis: Kasparov on Chess |publisher=Blog.palantirtech.com |date=2010-03-08 |accessdate=2012-01-30}}&lt;/ref&gt;

=== 2010: Ghostnet and the Shadow Network ===
Palantir partner [[Information Warfare Monitor]] used Palantir software to uncover both the [[Ghostnet]] and the [[Shadow Network]]. The Ghostnet was a China-based cyber espionage network targeting  1,295 computers in 103 countries, including the [[Dalai Lama]]’s office, a NATO computer and  embassies.&lt;ref&gt;{{cite news| url=http://www.cnn.com/video/#/video/tech/2009/03/30/vause.china.cyber.espionage.cnn | work=CNN | title=CNN.com Video}}&lt;/ref&gt; The Shadow Network  was also a China-based espionage operation that hacked into the Indian security and defense apparatus.  Cyber spies stole documents related to Indian security, embassies abroad, and NATO troop activity in Afghanistan.&lt;ref name=&quot;bare_url_a&quot; /&gt;&lt;ref&gt;{{cite news| url=http://www.nytimes.com/2009/03/29/technology/29spy.html | work=The New York Times | title=Vast Spy System Loots Computers in 103 Countries | first=John | last=Markoff | date=March 29, 2009}}&lt;/ref&gt;

===2010-2012: Expansion===
In April 2010, Palantir announced a partnership with [[Thomson Reuters]] to sell the Palantir Metropolis product as QA Studio.&lt;ref&gt;{{cite news| title=Press release: Thomson Reuters and Palantir Technologies enter exclusive agreement to create next-generation analytics platform for financial clients |url=http://thomsonreuters.com/content/press_room/financial/2010_04_12_palantir_technologies_agreement |date=2010-04-12 |publisher=Thomson Reuters}}&lt;/ref&gt;
On June 18, 2010, [[Vice President of the United States|Vice President]] [[Joe Biden]] and [[Office of Management and Budget]] Director [[Peter Orszag]] held a press conference at the White House announcing the success of fighting fraud in the stimulus by the [[Recovery Accountability and Transparency Board]] (RATB).  Biden credited the success to the software, Palantir, being deployed by the federal government.&lt;ref&gt;{{cite web|author=Tim Kauffman | url=http://www.federaltimes.com/article/20100627/AGENCY05/6270306/ |title=The new high-tech weapons against fraud |publisher= Federal Times |date=2010-06-27 |accessdate=2012-01-30}}&lt;/ref&gt;  He announced that the capability will be deployed at other government agencies, starting with [[Medicare (United States)|Medicare]] and [[Medicaid]].&lt;ref&gt;{{cite news| url=http://content.usatoday.com/communities/theoval/post/2010/06/obama-administration-to-create-do-not-pay-list-to-bar-shady-contractors/1 | newspaper=USA Today | title=Obama administration to create 'do not pay' list to bar shady contractors | date=2010-06-18 | first=Kathy | last=Kiely}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Peter Orszag, Director |url=http://www.whitehouse.gov/omb/blog/10/06/18/Do-Not-Pay-Do-Read-This-Post/?mkt_tok=3RkMMJWWfF9wsRonu63NZKXonjHpfsX66%2BgtWaOg38431UFwdcjKPmjr1YICTQ%3D%3D |title=Do Not Pay? Do Read This Post |publisher=Whitehouse.gov |date=2010-06-18 |accessdate=2012-01-30}}&lt;/ref&gt;&lt;ref name=&quot;bare_url_b&quot;&gt;{{cite news| url=http://politicalticker.blogs.cnn.com/2010/06/01/companies-capitalize-on-open-government/?fbid=ykqVZByQGPM |work= Political Ticker blog |author= Eric Kuhn |date= June 1, 2010 | publisher=CNN | title=Companies capitalize on 'open government' |accessdate= September 28, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.palantir.com/2010/05/govcon5-videos-now-available/ |title=Using Palantir with Open Source Data: Finding and Preventing Fraud in Stimulus Spending |publisher=Palantir Technologies |date=2010-05-04}}&lt;/ref&gt;

Estimates were $250 million in revenues in 2011.&lt;ref&gt;{{Cite news |title= Palantir, the War on Terror's Secret Weapon: A Silicon Valley startup that collates threats has quietly become indispensable to the U.S. intelligence community |work= Business Week Magazine |date= November 22, 2011 |authors=  Ashlee Vance and Brad Stone |url= http://www.businessweek.com/magazine/palantir-the-vanguard-of-cyberterror-security-11222011.html |accessdate= September 28, 2013 }}&lt;/ref&gt;

===2013===
{|class=&quot;toccolours&quot; style=&quot;float: right; margin-left: 1em; margin-right: 0em; font-size: 85%; background:offwhite; color:black; width:30em; max-width: 60%;&quot; cellspacing=&quot;5&quot;
|style=&quot;text-align: left;&quot;|&quot;[As of 2013] the U.S. spy agencies also employed Palantir to connect databases across departments. Before this, most of the databases used by the CIA and FBI were siloed, forcing users to search each database individually. Now everything is linked together using Palantir.&quot;
|-
|style=&quot;text-align: left;&quot;|— [[TechCrunch]] in January 2015&lt;ref name=&quot;tebtecnchrun&quot;/&gt;
|}
A document leaked to [[TechCrunch]] revealed that Palantir's clients as of 2013 included at least twelve groups within the US government, including the [[Central Intelligence Agency|CIA]], [[United States Department of Homeland Security|DHS]], [[National Security Agency|NSA]], [[Federal Bureau of Investigation|FBI]], [[Centers for Disease Control and Prevention|CDC]], the [[Marine Corps]], the [[Air Force]], [[United States Special Operations Command|Special Operations Command]], [[West Point]], the Joint IED-defeat organization and Allies, the [[Recovery Accountability and Transparency Board]] and the [[National Center for Missing and Exploited Children]]. However, at the time the [[US Army]] continued to use its own data analysis tool.&lt;ref name=&quot;tebtecnchrun&quot;/&gt; Also, according to [[TechCrunch]], the US spy agencies such as the CIA and FBI were linked for the first time with Palantir software, as their databases had previously been &quot;siloed.&quot;&lt;ref name=&quot;tebtecnchrun&quot;/&gt;

In September 2013, Palantir disclosed over $196 million in funding according to a US [[Securities and Exchange Commission]] filing.&lt;ref&gt;{{cite web | url = http://www.sec.gov/Archives/edgar/data/1321655/000132165513000002/xslFormDX01/primary_doc.xml | title = Notice of Exempt Offering of Securities |date= September 27, 2013 | accessdate = September 28, 2013 | publisher = The United States Securities and Exchange Commission}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Cutler|first=Kim-Mai|title=Palantir Is Raising $197M In Growth Capital, SEC Filing Shows|url=http://techcrunch.com/2013/09/27/palantir-197m-sec-filing/|publisher=TechCrunch}}&lt;/ref&gt; It was estimated that the company would likely close almost $1 billion in contracts in 2014.&lt;ref&gt;{{cite news |authors = Andy Greenberg, Ryan Mac | title = How A 'Deviant' Philosopher Built Palantir, A CIA-Funded Data-Mining Juggernaut | date = September 2, 2013 | url = http://www.forbes.com/sites/andygreenberg/2013/08/14/agent-of-intelligence-how-a-deviant-philosopher-built-palantir-a-cia-funded-data-mining-juggernaut/ | work = Forbes | accessdate = September 28, 2013 }}&lt;/ref&gt; CEO [[Alex Karp]] announced in 2013 that the company would not be pursuing an [[IPO]], as going public would make “running a company like ours very difficult.”&lt;ref name=&quot;teafunding&quot;/&gt; In December of 2013, the company began a round of financing, raising around $450 million from private funders. This raised the company's value to $9 billion, according to ''[[Forbes]]'', with the magazine further explaining that the valuation made Palantir &quot;among Silicon Valley’s most valuable private technology companies.&quot;&lt;ref name=&quot;teafunding&quot;/&gt;

===2014-2015: Additional funding===
{{outdated|date=March 2015}}
In December 2014, ''Forbes'' reported the Palantir was looking to raise $400,000,000 in an additional round of financing, after the company filed paperwork with the [[Securities and Exchange Commission]] the month before. The report was based on research by VC Experts. If completed, ''Forbes'' stated Palantir's funding could reach a total of $1.2 billion.&lt;ref name=&quot;teafunding&quot;/&gt; As of December 2014, the company continued to have diverse private funders, [[Kenneth Langone]] and [[Stanley Druckenmiller]], [[In-Q-Tel]] of the CIA, [[Tiger Global Management]], and [[Founders Fund]], which is a venture Firm operated by [[Peter Thiel]], the chairman of Palantir. As of December 2014, Thiel was Palantir's largest shareholder.&lt;ref name=&quot;teafunding&quot;/&gt;

The company was valued at $15 billion in November 2014.&lt;ref&gt;http://blogs.wsj.com/digits/2015/01/16/palantir-raising-more-money-after-tagged-with-15-billion-valuation/&lt;/ref&gt; In June 2015, Buzzfeed reported the company was raising up to $500 million in new capital at a valuation of $20 billion.&lt;ref&gt;{{cite news|url=http://www.buzzfeed.com/williamalden/palantir-valued-at-20-billion-in-new-funding-round#.yhOKZDdjG|title=Palantir Valued At $20 Billion In New Funding Round|author=William ALden|date=June 23, 2015|publisher=[[Buzzfeed]]|accessdate=June 23, 2015}}&lt;/ref&gt;

== Products ==

=== Palantir Gotham ===
Palantir Gotham (formerly known as Palantir Government) integrates structured and [[unstructured data]], provides advanced search and discovery capabilities, enables knowledge management, and facilitates secure collaboration. The Palantir platform includes the privacy and civil liberties protections mandated by legal requirements such as those in the 9/11 Commission Implementation Act.  Palantir’s privacy controls purportedly keep investigations focused, as opposed to the expansive [[data mining]] techniques that have drawn criticism from privacy advocates concerned about civil liberties protection. Palantir maintains security tags at a granular level.&lt;ref name=&quot;npr&quot; /&gt;&lt;ref name=&quot;charlierose&quot; /&gt;

Palantir runs the site AnalyzeThe.US,&lt;ref&gt;{{cite web|url=http://analyzethe.us |title= AnalyzeThe.US — Home |work= Web site |publisher= Palantir  |accessdate= September 28, 2013 }}&lt;/ref&gt; which allows Palantir customers and affiliates to use Palantir Gotham to perform analysis on publicly available data from data.gov, usaspending.gov, the Center for Responsive Politics’ Open Secrets Database, and Community Health Data from HHS.gov.&lt;ref&gt;{{cite web|url=http://www.fiercehealthcare.com/press-releases/palantir-technologies-showcase-analysis-community-health-data-initiative-forum-harnes |title=Palantir Technologies to Showcase Analysis at the Community Health Data Initiative Forum: Harnessing the Power of Information to |publisher=FierceHealthcare |date=2010-06-02 |accessdate=2012-01-30}}&lt;/ref&gt;

=== Palantir Metropolis ===
Palantir Metropolis (formerly known as Palantir Finance) is software for data integration, information management and quantitative [[analytics]].  The software connects to commercial, proprietary and public data sets and discovers trends, relationships and anomalies, including [[predictive analytics]].

===Other===
The company has been involved in a number of business and consumer products, designing in part of in whole. For example, in 2014, they premiered '''Insightics,''' which according to the ''[[Wall Street Journal]] '' &quot;extracts customer spending and demographic information from merchants’ credit-card records.&quot; It was created in tandem with large credit processing company [[First Data]].&quot;&lt;ref name=&quot;tezfirstprofit&quot;&gt;
{{cite news
| title       =First Data Reports First Quarterly Profit in More Than Seven Years
| first       =Robin
| last        =Sidel
| url         =http://www.wsj.com/articles/first-data-reports-first-quarterly-profit-in-more-than-seven-years-1423602902
| newspaper   =[[Wall Street Journal]] 
| date        =February 10, 2015
| accessdate  = 2015-02-25
}}&lt;/ref&gt;

== Customers ==
===Private civilian use===
{{See also|Information Warfare Monitor}}
Palantir Metropolis is used by hedge funds, banks, and financial services firms.&lt;ref name=&quot;bare_url&quot; /&gt;&lt;ref name=&quot;npr&quot; /&gt;&lt;ref&gt;{{cite web|url=http://www.gov2expo.com/gov2expo2010/public/schedule/detail/13996 |title=A Human Driven Data-centric Approach to Accountability: Analyzing Data to Prevent Fraud, Waste and Abuse in Stimulus Spending: Gov 2.0 Expo 2010 - Co-produced by UBM TechWeb &amp; O'Reilly Conferences, May 25 - 27, 2010, Washington, DC |publisher=Gov2expo.com |date= |accessdate= 2012-01-30}}&lt;/ref&gt;&lt;ref name=&quot;bare_url_a&quot;&gt;{{cite web| title=PayPal-Based Technology Helped Bust India's And The Dalai Lama's Cyberspies |url=http://www.forbes.com/sites/firewall/2010/04/30/paypal-based-technology-helped-bust-indias-and-the-dalai-lamas-cyberspies/ | publisher=Forbes | first= Oliver | last=Chiang |date=2010-04-30}}&lt;/ref&gt;

Palantir partner [[Information Warfare Monitor]] used Palantir software to uncover both the [[Ghostnet]] and the [[Shadow Network]].&lt;ref&gt;{{cite news| url=http://www.cnn.com/video/#/video/tech/2009/03/30/vause.china.cyber.espionage.cnn | work=CNN | title=CNN.com Video}}&lt;/ref&gt;&lt;ref name=&quot;bare_url_a&quot; /&gt;&lt;ref&gt;{{cite news| url=http://www.nytimes.com/2009/03/29/technology/29spy.html | work=The New York Times | title=Vast Spy System Loots Computers in 103 Countries | first=John | last=Markoff | date=March 29, 2009}}&lt;/ref&gt;

===US civil entities===
Palantir’s software is used by the [[Recovery Accountability and Transparency Board]] to detect and investigate fraud and abuse in the American Recovery and Reinvestment Act.  Specifically, the Recovery Operations Center (ROC) used Palantir to integrate transactional data with open-source and private data sets that describe the entities receiving stimulus funds.{{clarify|date=March 2015}}&lt;ref name=&quot;bare_url_b&quot; /&gt; Other clients as of 2013 included the [[Centers for Disease Control and Prevention]] and the [[National Center for Missing and Exploited Children]].&lt;ref name=&quot;tebtecnchrun&quot;/&gt;

===US military, intelligence, and police===
Palantir Gotham is used by counter-terrorism analysts at offices in the [[United States Intelligence Community]] and [[United States Department of Defense]], fraud investigators at the [[Recovery Accountability and Transparency Board]], and cyber analysts at [[Information Warfare Monitor]] (responsible for the [[GhostNet]] and the [[Shadow Network]] investigation).  

Other clients as of 2013 included [[DHS]], [[NSA]], [[FBI]], [[Centers for Disease Control and Prevention|CDC]], the [[Marine Corps]], the [[Air Force]], [[United States Special Operations Command|Special Operations Command]], [[West Point]], the [[Joint IED-defeat]] organization and Allies. However, at the time the [[US Army]] continued to use its own data analysis tool. &lt;ref name=&quot;tebtecnchrun&quot;/&gt; Also, according to [[TechCrunch]], &quot;The U.S. spy agencies also employed Palantir to connect databases across departments. Before this, most of the databases used by the CIA and [[FBI]] were siloed, forcing users to search each database individually. Now everything is linked together using Palantir.&quot;&lt;ref name=&quot;tebtecnchrun&quot;/&gt;

U.S. [[military intelligence]] used the Palantir product to improve their ability to predict locations of [[improvised explosive devices]] in its [[war in Afghanistan (2001–present)|war in Afghanistan]]. A small number of practitioners reported it to be more useful than the U.S. Army's program of record, the [[Distributed Common Ground System]] (DCGS-A). California Congressman [[Duncan D. Hunter]] complained of [[United States Department of Defense|US DoD]] obstacles to its wider use in 2012.&lt;ref&gt;{{cite news |newspaper= [[The Washington Times]]  |title=Military has to fight to purchase lauded IED buster |date= July 16, 2012 |url= http://p.washingtontimes.com/news/2012/jul/16/military-has-to-fight-to-purchase-lauded-ied-buste/ |authorlink=Rowan Scarborough |first=Rowan |last =Scarborough |accessdate= September 29, 2013 }}&lt;/ref&gt;

Palantir has also been reported to be working with various US police departments, for example accepting a contract in 2013 to help the [[Northern California Regional Intelligence Center]] build a controversial license plates database for California.&lt;ref&gt;http://www.theverge.com/2013/6/29/4478748/california-license-plate-reader-database-palantir&lt;/ref&gt;

== Palantir Night Live event==
Palantir hosts Palantir Night Live at Palantir’s McLean and Palo Alto offices. The event brings speakers from the intelligence community and technology space to discuss topics of common interest. Past speakers include [[Garry Kasparov]], [[Nart Villeneuve]] from Information Warfare Monitor, [[Andrew McAfee]], author of Enterprise 2.0, and [[Michael Chertoff]].&lt;ref&gt;{{cite web |url= http://www.washingtonlife.com/2010/04/09/society-2-0-tenet-chertoff-and-beer-oh-my/ |title=Society 2.0: Tenet, Chertoff and Beer, Oh My! &amp;#124; Washington Life Magazine |publisher=Washingtonlife.com |date=2010-04-09 |accessdate=2012-01-30}}&lt;/ref&gt;

==Controversies==
===WikiLeaks proposals (2010)===
In 2010 [[Hunton &amp; Williams]] LLP allegedly asked [[Berico Technologies]], Palantir, and [[HBGary Federal]] to draft a response plan to “the [[WikiLeaks]] Threat.” In early 2011 [[Anonymous (group)|Anonymous]] publicly released HBGary-internal documents, including the plan. The plan proposed Palantir software would “serve as the foundation for all the data collection, integration, analysis, and production efforts.”&lt;ref name=&quot;Harris&quot;&gt;{{cite web|last=Harris|first=Shane|title=Killer App|url=http://www.washingtonian.com/articles/people/killer-app/|work=Washingtonian|date=31 January 2012|accessdate=14 March 2012}}&lt;/ref&gt;  The plan also included slides, allegedly authored by HBGary CEO Aaron Barr, which suggested “[spreading] disinformation” and “disrupting” [[Glenn Greenwald]]’s support for WikiLeaks.&lt;ref&gt;{{cite web|author=James Wray and Ulf Stabe |url=http://www.thetechherald.com/article.php/201106/6798/Data-intelligence-firms-proposed-a-systematic-attack-against-WikiLeaks?page=2 |title=Data intelligence firms proposed a systematic attack against WikiLeaks |publisher=Thetechherald.com |date= |accessdate=2012-01-30}}&lt;/ref&gt;

Palantir CEO Karp ended all ties to HBGary and issued a statement apologizing to “progressive organizations… and Greenwald … for any involvement that we may have had in these matters.&quot; Palantir placed an employee on leave pending a review by a third-party law firm. The employee was later reinstated.&lt;ref name=&quot;Harris&quot;/&gt;

== See also ==
* [[Recorded Future]]

== References ==
{{reflist|30em|refs=

&lt;ref name=&quot;teafunding&quot;&gt;
{{cite news
| title       =Palantir Aiming To Raise $400 Million In New Round
| first       =Ryan
| last        =Mac
| url         =http://www.forbes.com/sites/ryanmac/2014/12/11/palantir-aiming-to-raise-400-million-in-new-round/
| newspaper   =[[Forbes]]
| date        =December 11, 2014
| accessdate  = 2015-02-21
}}&lt;/ref&gt;
&lt;ref name=&quot;tebtecnchrun&quot;&gt;
{{cite news
| title       =Leaked Palantir Doc Reveals Uses, Specific Functions And Key Clients
| first       =Matt
| last        =Burns
| url         =http://techcrunch.com/2015/01/11/leaked-palantir-doc-reveals-uses-specific-functions-and-key-clients/
| publisher   =[[TechCrunch]]
| date        =January 11, 2015
| accessdate  = 2015-02-21
}}&lt;/ref&gt;
}}

== External links ==
* [http://www.palantir.com/ Palantir Technologies]
* [http://www.forbes.com/sites/andygreenberg/2013/06/07/startup-palantir-denies-its-prism-software-is-the-nsas-prism-surveillance-system/ Startup Palantir Denies Its 'Prism' Software Is The NSA's 'PRISM' Surveillance System] by Andy Greenberg, Forbes, 2013-06-07, accessed 2013-06-07.

[[Category:Business software companies]]
[[Category:Big data]]
[[Category:Companies established in 2004]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Companies based in Palo Alto, California]]</text>
      <sha1>q7fud2hvtncoc4au557dsqmokw39zre</sha1>
    </revision>
  </page>
  <page>
    <title>Hibari (database)</title>
    <ns>0</ns>
    <id>28886888</id>
    <revision>
      <id>637554802</id>
      <parentid>577292426</parentid>
      <timestamp>2014-12-11T01:05:10Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Speedily moving category Erlang programming language to [[:Category:Erlang (programming language)]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2488">{{Infobox software
| name                   = Hibari
| logo                   =
| screenshot             =
| caption                =
| author                 =
| developer              = Hibari developers
| released               = 2010
| status                 = Active
| latest release version = v0.1.10
| latest release date    = {{release date|2013|02|04}}
| latest preview version = 
| latest preview date    = 
| frequently updated     = yes
| programming language   = [[Erlang (programming language)|Erlang]]
| operating system       = [[Cross-platform]]
| language               = English, Japanese
| genre                  = [[Key-value pair|Key-value]] store
| license                = [[Apache License]] 2.0
| website                = https://github.com/hibari/hibari
}}
{{Portal|Free software}}
'''Hibari''' is a strongly consistent, highly available, distributed, key-value [[Big Data]] store. ([[NoSQL]] database) &lt;ref&gt;[https://github.com/hibari/hibari Hibari project homepage]&lt;/ref&gt; It was developed by [[Cloudian, Inc.]], formerly Gemini Mobile Technologies to support its mobile messaging and email services and released as [[open source]] on July 27, 2010.

Hibari, a Japanese name meaning &quot;Cloud Bird&quot;, can be used in [[cloud computing]] with services&amp;mdash;such as [[social networking]]&amp;mdash;requiring the daily storage of potentially [[terabyte]]s or [[petabyte]]s of new data.

==Distinctive Features==
Hibari uses chain replication for strong consistency, high-availability, and durability. Unlike many other [[NoSQL]] variants, Hibari support micro-transaction, which is ACID transaction within a certain range of keys.

Hibari has excellent performance especially for read and large value (around 200KB) operations.

==Interfaces==
Hibari supports APIs such as [[Amazon S3]], [[JSON-RPC]] and Universal Binary Protocol; plans have been announced for support of [[Apache Thrift]]; in addition to [[Erlang (programming language)|Erlang]], the language it was developed in. Hibari supports [[language binding]]s such as [[Java (software platform)|Java]], [[C (programming language)|C]], [[C++]], [[Python (programming language)|Python]], and [[Ruby (programming language)|Ruby]].

==References==
{{Reflist}}

{{DEFAULTSORT:Hibari (Database)}}
[[Category:Free database management systems]]
[[Category:2010 software]]
[[Category:Erlang (programming language)]]
[[Category:Cloud storage]]
[[Category:Distributed data storage]]
[[Category:NoSQL]]
[[Category:Big data]]</text>
      <sha1>a3yqnr6rlt4gq6oj00t20u0flmtyof5</sha1>
    </revision>
  </page>
  <page>
    <title>SAP HANA</title>
    <ns>0</ns>
    <id>40383082</id>
    <revision>
      <id>671249113</id>
      <parentid>665614647</parentid>
      <timestamp>2015-07-13T13:04:36Z</timestamp>
      <contributor>
        <username>BjoernJ</username>
        <id>25759776</id>
      </contributor>
      <comment>Updated version info</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13280">{{Infobox software

| name = SAP HANA
| logo = &lt;!-- logo removed since it was for SAP the company not this product --&gt;
| screenshot =
| caption =
| developer = [[SAP SE]]
| released =
| latest_release_version =  1.0 SPS10 &lt;ref&gt;http://global.sap.com/germany/news-reader/index.epx?articleId=24637&lt;/ref&gt;
| latest_release_date = {{start date and age|2015|06|16}}
| latest_preview_version =
| latest_preview_date =
| release_location =
| genre = [[in-memory database|In-memory]] [[RDBMS]]
| status = Active
| programming language = [[C (programming language)|C]], [[C++]]
| language = Multi-lingual&lt;!-- Should state the number of languages. --&gt;
| license = [[proprietary software|Proprietary]]
| website = {{URL|hana.sap.com}}
}}

'''SAP HANA''' is an [[In memory database|in-memory]], [[Column-oriented DBMS|column-oriented]],  [[relational database management system]] developed and marketed by [[SAP SE]].&lt;ref&gt;{{Cite web |title= Primer on SAP HANA |author= Jeff Kelly |date= July 12, 2013 |accessdate= October 9, 2013 |work= Wikibon |url=  http://wikibon.org/wiki/v/Primer_on_SAP_HANA }}&lt;/ref&gt;&lt;ref&gt;{{YouTube |id=adv25iZmgQc |title=  SAP HANA - The Column Oriented (Based) Database }} (December 8, 2012)&lt;/ref&gt; HANA's architecture is designed to handle both high transaction rates and complex query processing on the same platform.&lt;ref&gt;http://www.saphana.com/docs/DOC-3817&lt;/ref&gt; SAP HANA was previously called SAP High-Performance Analytic Appliance.&lt;ref&gt;http://searchsap.techtarget.com/definition/HANA-SAP-HANA&lt;/ref&gt;

In addition to the database engine, HANA includes an embedded web server and version control repository that can be used for application development. HANA applications can be created using [[server-side JavaScript]] and HTML.

==History==
'''SAP HANA''' developed or acquired technologies, including [[TREX search engine]] -an [[In-memory database|in-memory]] [[column-oriented DBMS|column-oriented search engine]]-, P*TIME -an in-memory [[OLTP]] database acquired by SAP in 2005-, and [[MaxDB]] with its in-memory liveCache engine.&lt;ref&gt;{{cite web | url= http://www.redbooks.ibm.com/redpapers/pdfs/redp4814.pdf | title= SAP In-Memory Computing on IBM eX5 Systems | first1= Gereon | last1= Vey | first2= Ilya | last2= Krutov | date= January 2012 | accessdate= 22 October 2013}}&lt;/ref&gt;&lt;ref name=&quot;line&quot;&gt;{{Cite web |title= SAP HANA Timeline |date= June 17, 2012 |author= SAP SE |publisher= SlideShare |url= http://www.slideshare.net/SAPTechnology/sap-hana-timeline |accessdate= October 9, 2013 }}&lt;/ref&gt;
In 2008, teams from [[SAP SE]] working with [[Hasso Plattner Institute]] and [[Stanford University]] demonstrated an application architecture for real-time analytics and aggregation, mentioned as &quot;HAsso's New Architecture&quot; in SAP executive [[Vishal Sikka]]'s blog. Before the name &quot;HANA&quot; stabilised, people referred to this product as &quot;New Database&quot;.&lt;ref&gt;{{Cite web |work= Gucons web site |year= 2011 |title= What is SAP HANA Database |url= http://www.gucons.com/what-is-sap-hana-database/ |accessdate= October 9, 2013 }}&lt;/ref&gt;

The product provides [[Business Intelligence]] allowing real-time response.&lt;ref name=&quot;set&quot;&gt;{{Cite news |title= SAP's in-memory analytics boxes set for November release |work= Info World |date= October 19, 2010 |author= Chris Kanas |url= http://www.infoworld.com/d/applications/saps-in-memory-analytics-boxes-set-november-release-117 |accessdate= October 9, 2013 }}&lt;/ref&gt;
The first product shipped in late November 2010.&lt;ref name=&quot;line&quot; /&gt;&lt;ref&gt;{{Cite news |title= SAP launches HANA for in-memory analytics: The in-memory analytic appliance will compete with next-generation data-processing platforms such as Oracle's Exadata machines |author= Chris Kanaracus |date= December 1, 2010 |work= Info World |url= http://www.infoworld.com/d/data-management/sap-launches-hana-in-memory-analytics-300 |accessdate= September 24, 2013 }}&lt;/ref&gt;
By mid-2011, the technology had attracted interest but the conservative business customers still considered it &quot;in early days&quot;.&lt;ref&gt;{{Cite news |title= SAP's HANA is hot, but still in early days |work= Network World |date= September 15, 2011 |author= Chris Kanaracus |url= http://www.networkworld.com/news/2011/091511-saps-hana-is-hot-but-250942.html |accessdate= October 15, 2013 }}&lt;/ref&gt;
HANA support for [[SAP NetWeaver Business Warehouse]] was announced in September 2011 for availability by November.&lt;ref&gt;{{Cite news |title= SAP Begins BW on HANA Ramp-Up, First Big Test for the HANA Database |author= Courtney Bjorlin |work= ASUG News |date= November 9, 2011 |url= http://www.asugnews.com/article/sap-begins-bw-on-hana-ramp-up-first-big-test-for-the-hana-database |accessdate= October 15, 2013  }}&lt;/ref&gt;

In 2012, SAP promoted aspects of [[cloud computing]].&lt;ref&gt;{{Cite news |title= SAP Headed For $71 On Cloud, Mobile And HANA Growth |author= Trevis Team |work= Forbes |date= April 30, 2012 |url= http://www.forbes.com/sites/greatspeculations/2012/04/30/sap-headed-for-71-on-cloud-mobile-and-hana-growth/ |accessdate= October 9, 2013 }}&lt;/ref&gt;
In October 2012, SAP announced a variant called HANA One that used a smaller amount of memory on [[Amazon Web Services]] for an hourly fee.&lt;ref&gt;{{Cite news |title= SAP Launches Cloud Platform Built On Hana |author= Doug Henschen |date= October 17, 2012 |work= Information Week |url= http://www.informationweek.com/software/enterprise-applications/sap-launches-cloud-platform-built-on-han/240009198&lt;!-- pay site  --&gt; |deadurl=no |archiveurl= http://web.archive.org/web/20121019234617/http://www.informationweek.com/software/enterprise-applications/sap-launches-cloud-platform-built-on-han/240009198 |archivedate= October 19, 2012 |accessdate= October 15, 2013 }}&lt;/ref&gt;

In January 2013, [[SAP ERP|SAP enterprise resource planning]] software from its [[SAP Business Suite|Business Suite]] was announced for HANA, and became available by May.&lt;ref&gt;{{Cite news |title= SAP puts Business Suite on HANA, joins transactional to analytical |work= Computer Weekly |author= Brian McKenna |date= January 11, 2013 |url= http://www.computerweekly.com/news/2240175913/SAP-puts-Business-Suite-on-HANA-joins-transactional-to-analytical |accessdate= October 15, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= Sapphire 2013: Business Suite on HANA goes to general availability |work= Computer Weekly |date= May 15, 2013 |url= http://www.computerweekly.com/news/2240184187/Sapphire-2013-Business-Suite-on-HANA-goes-to-general-availability |accessdate= October 15, 2013 }}&lt;/ref&gt;
In May 2013, a [[software as a service]] offering called the HANA Enterprise Cloud service was announced.&lt;ref&gt;{{Cite news |title= 
SAP unveils HANA Enterprise Cloud service: Customers will be able to run their applications on the HANA-powered cloud |work= Network World |date= May 7, 2013 |author=  Chris Kanaracus |url= http://www.networkworld.com/news/2013/050713-sap-unveils-hana-enterprise-cloud-269505.html |accessdate= October 15, 2013 }}&lt;/ref&gt;

Rather than [[software versioning|versioning]], the software utilizes [[service pack]]s.&lt;ref name=&quot;faq&quot;&gt;{{Cite web |title= Update III: The SAP HANA FAQ - answering key SAP In-Memory questions |date= May 28, 2012 |author= John Appleby |accessdate= October 9, 2013 |publisher=  Bluefin Solutions |url= http://www.bluefinsolutions.com/insights/blog/the_sap_hana_faq_answering_key_sap_in_memory_questions/ 
}}
&lt;/ref&gt;

==Architecture==
The main process, called the index server, has a structure shown in the diagram to the right.&lt;ref&gt;http://www.saphana.com/community/blogs/blog/2012/12&lt;/ref&gt; [[File:Hana.jpg|right|500px|HANA indexer components]] The indexer performs session management,  authorization, transaction management and command processing. Note that HANA has both a row store and a column store. Users can create tables using either store, but the column store has more capabilities. The index server also manages persistence between cached memory images of database objects, log files and permanent storage files.

The Authorization manager provides authentication and authorization services. The Authorization Manager can provide security based on SAML, OAuth or Kerberos authentication protocols.

The Extended Services (XS) Engine is a web server with privileged access to the database. Applications written with server-side JavaScript or as Java Servlets can be deployed to the XS Engine.  These can either be HTML web applications or REST web service endpoints. Server-side JavaScript includes jQuery-based extensions for database access and to access HTTP request and response messages.  The JavaScript engine is based on the Mozilla SpiderMonkey project.

Client applications access the HANA database directly using JDBC or via the Extended Services Engine using HTTP.

To overcome input/output bottlenecks, HANA was designed as an in-memory server, meaning that the first access to a table causes the entire table to be read and maintained in memory. Background processes maintain log files and the long-term disk storage. The query processor uses massively parallel processing. The column data store reduces the amount of data that needs to be read and eliminates the need for indexing.

Applications can bypass the SQL processor and access the Calculation Engine directly by composing XML-based queries. There are three types of non-SQL query objects: Attribute Views, Calculation Views and Analytic Views. In many cases, application performance can be increased by using these views instead of SQL queries.

==Application Development==

Applications are developed using a plugin to the [[Eclipse (software)|Eclipse]] development environment called HANA Studio. Using this environment, the database can be managed and applications can be developed. Database objects (tables, views, stored procedure, etc.) can be created, deployed and debugged using the Eclipse plugin. Extended Services applications can be written, deployed and debugged using HANA Studio. Standard database management functions such as database creation, user account management, backups and data import/export can be performed from HANA Studio.

The HANA server includes a version management system called the repository. Applications developed in HANA Studio are deployed via the repository which maintains a version history of each deployment artifact (table, view, procedure, JavaScript file, etc.).

Applications may be moved from one server to another by creating a Distribution Unit or DU. DU's are essentially compressed archives of the files that make up an application. This may be needed when there are separate development, test and production environments. They can be created, exported and imported using HANA Studio.

While traditional [[client-server model|client-server]] or [[Multi-tier architecture|multitier]] applications are supported via JDBC, SAP is recommending &lt;ref&gt;https://open.sap.com/courses/hana1&lt;/ref&gt; that business logic contained in the client or middle tier application be moved to the HANA server as XS application(s). This tends to increase performance by reducing the amount of data transmitted from the database server to the middle tier. It also allows the application to use the CPU power found on the large servers that HANA requires.&lt;ref&gt;http://help.sap.com/hana/SAP_HANA_Developer_Guide_en.pdf&lt;/ref&gt;

==Developers Community==
The focal point of the community of developers on SAP HANA platform is SAP HANA Developer Center or &quot;the DevCenter&quot;. The DevCenter offers general information, education materials, community forums, plus access to SAP HANA database with free licenses hosted in the public cloud (like CloudShare, Amazon Web Services, Microsoft Azure).
Access to some materials and features may require free registration.

==Physical Deployment==

HANA is available as an appliance (hardware with software preloaded) from a number of vendors such as [[IBM]], [[Dell]] and  [[HP]].  HANA is also available as a cloud appliance (called HANA One) from a number of cloud service providers such as [[Amazon Web Services|Amazon]] and [[Microsoft Azure]]. A free developer's edition is available from several cloud providers. HANA is not available as software alone.

An SAP account is required to deploy SAP HANA and machines. To deploy a HANA instance, you must log into your SAP account and initiate the deployment from there. When all requirements are met at the SAP site, virtual machine(s) will be created by SAP on the selected cloud service provider. This is in contrast to the more conventional approach where virtual machines are created directly on the cloud service provider's web site.

HANA requires substantial hardware resources.  For example, on Amazon, the recommended instance size is 8xlarge, which has 32 CPUs, 60GB memory and 640GB disk storage.&lt;ref&gt;https://aws.amazon.com/marketplace/pp/B009KA3CRY&lt;/ref&gt;

HANA can be deployed on a single machine or in a high-availability cluster. A single machine can run multiple instances of HANA.&lt;ref&gt;http://help.sap.com/hana/SAP_HANA_Administration_Guide_en.pdf&lt;/ref&gt;

==References==
{{Reflist|2}}

== External links ==
* {{Official website|hana.sap.com}}

[[Category:2010 software]]
[[Category:Proprietary database management systems]]
[[Category:Big data]]
[[Category:Column-oriented DBMS software for Linux]]
[[Category:Proprietary commercial software for Linux]]</text>
      <sha1>qh1vnt0nylpgj0ug59gir43k97qvemq</sha1>
    </revision>
  </page>
  <page>
    <title>Sense Networks</title>
    <ns>0</ns>
    <id>27370710</id>
    <revision>
      <id>651539718</id>
      <parentid>651538941</parentid>
      <timestamp>2015-03-15T21:17:48Z</timestamp>
      <contributor>
        <ip>75.119.255.218</ip>
      </contributor>
      <comment>/* History */ sorry I can't copy and paste on my computer. and Im lazy, but a google will show a new york times article and a tech ccrunc article talkign about the merger. also tech crunch says that this was founded in 2003, not 6</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8316">{{Infobox dot-com company
| name         = Sense Networks, Inc.
| company_slogan       = Indexing the real world using location data for predictive analytics
| owner                =
| logo         = [[File:Sensenetworks logo.png|240px]]
| company_type         = [[Private company|Private (venture backed)]]
| foundation           = [[New York City]], [[New York]], USA
| founder              = [[Greg Skibiski]]
| area_served          = [[World]]wide
| location_city        = [[New York City, New York]]
| location_country     = [[United States]]
| key_people           = [[Alex Pentland]]&lt;small&gt; ([[Chief privacy officer|CPO]])&lt;/small&gt;&lt;br /&gt;[[Tony Jebara]]&lt;small&gt; (Chief Scientist)&lt;/small&gt;&lt;br /&gt;[[Christine Lemke]]&lt;small&gt; ([[Chief operating officer|COO]])&lt;/small&gt; &lt;br/&gt;[[Mikki Nasch]]&lt;small&gt; (EVP BD)&lt;/small&gt;
| revenue              =
| homepage             = [http://www.sensenetworks.com]
| screenshot           =
| caption              =
| num_employees        =
| url                  =
| registration         =
| launch_date          = February 2006
| current_status       = Active
| language             =
| advertising          =
| alexa                =
| website_type         =
}}
'''Sense Networks''' is a New York City based company with a focus on applications that analyze [[big data]] from [[mobile phone]]s, [[carrier network]]s, and [[taxicabs]], particularly by using [[machine learning]] technology to make sense of large amounts of location (latitude/longitude) data.&lt;ref&gt;Fitzgerald, Michael. [http://www.nytimes.com/2008/06/22/technology/22proto.html?_r=2&amp;scp=1&amp;sq=sense+networks&amp;st=nyt&amp;oref=slogin &quot;Predicting Where You’ll Go and What You’ll Like&quot;], ''[[The New York Times]]'', New York, 22 June 2008. Retrieved on 2010-05-12.&lt;/ref&gt;&lt;ref&gt;Sheridan, Barrett. [http://www.newsweek.com/id/186970/page/1 &quot;A Trillion Points of Data&quot;], ''[[Newsweek]]'', New York, 28 Feb 2009. Retrieved on 2010-05-12.&lt;/ref&gt;&lt;ref&gt;Baker, Stephen. [http://www.businessweek.com/magazine/content/09_10/b4122042889229.htm &quot;Mapping a New, Mobile Internet&quot;], ''[[Bloomberg Businessweek]]'', New York, 26 February 2009. Retrieved on 2010-05-12.&lt;/ref&gt;&lt;ref&gt;Markoff, John. [http://www.nytimes.com/2008/11/30/business/30privacy.html?scp=2&amp;sq=Greg+Skibiski&amp;st=nyt &quot;You're Leaving a Digital Trail. What About Privacy?&quot;], ''[[The New York Times]]'', New York, 29 November 2008. Retrieved on 2010-05-12.&lt;/ref&gt;

In 2009, Sense was named one of &quot;The 25 Most Intriguing Startups in the World&quot; by [[Bloomberg Businessweek]]&lt;ref&gt;Ante, Spencer. [http://images.businessweek.com/ss/09/11/1112_most_intriguing_companies/index.htm &quot;The World's Most Intriguing Startups&quot;], ''[[Bloomberg BusinessWeek]]'', New York, 12 November 2009. Retrieved on 2010-05-12.&lt;/ref&gt; and was called &quot;The Next Google&quot; on the cover of [[Newsweek]].&lt;ref&gt;[http://hd.media.mit.edu/newsweek2_03.09.09.pdf]&lt;/ref&gt;

In 2014, Sense Networks was acquired by [[Yp.com|YP]], &quot;the local search and advertising company owned by Cerberus Capital Management and AT&amp;T.&quot;&lt;ref&gt;[http://dealbook.nytimes.com/2014/01/06/yp-a-mobile-ad-firm-buys-a-rival-sense-networks/ &quot;YP, a Mobile Search Firm, Buys Sense Networks&quot;], New York Times.&lt;/ref&gt;

==History==
Sense Networks was founded by [[Greg Skibiski]] in February 2006 (2003?) near his home in [[Northampton, Massachusetts]]. After establishing an office in [[NoHo]], [[New York City]] near [[Silicon Alley]], Skibiski recruited [[Alex Pentland]], Director of Human Dynamics Research and former Academic Head of the [[MIT Media Lab]], [[Tony Jebara]], Associate Professor and Head of the Machine Learning Laboratory at [[Columbia University]], and [[Christine Lemke]], who would later become co-founders.&lt;ref&gt;Junietz, Erika. [http://docs.google.com/viewer?a=v&amp;q=cache:hKKbZBAfYHYJ:www.sensenetworks.com/press/mit-tech-insider.pdf+skibiski+idea+created&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEESheMj8iT2tzutepbEVsOwFl3_EAsOKF-hZSF2RwUWQzE12nAX8N7-iUegcn_ooA1VchAzO_42c7qaDJcXIsQ1qBTV8H-1JUKdHyI1tj68FgQ5A3dw5o5MIxiH7ldPMLHfIEUCVz&amp;sig=AHIEtbT2N32A-ERj0s16J8Fx0LSD30FxZg &quot;A Sense of Place&quot;], ''MIT Technology Insider'', Boston, August 2008. Retrieved on 2010-05-14.&lt;/ref&gt;

Sense Networks investors include [[Intel Capital]], [http://javelinvp.com Javelin Venture Partners], and [[Kenan Altunis]].&lt;ref&gt;[http://www.intel.com/capital/news/releases/090630.htm &quot;Press Release: Sense Networks Secures Series B Funding for Location Analytics, Led by Intel Capital&quot;], ''[[Intel Capital]]'', New York, 30 June 2009. Retrieved on 2010-05-13.&lt;/ref&gt;

Founder [[Greg Skibiski]] was pushed out by [[lead investor]] [[Intel Capital]]&lt;ref&gt;Baker, Stephen. [http://thenumerati.net/index.cfm?postID=489 &quot;Data correlation: Used-car customers drop cell-phone service?&quot;], ''[[The Numerati]]'', New York, 8 January 2010. Retrieved on 2010-05-14.&lt;/ref&gt; in November 2009 following the company's B round of financing, the same week company won the Emerging Communications Conference &quot;Company to Watch&quot; Award.&lt;ref&gt;[http://www.sensenetworks.com/pr_11022009.php &quot;Sense Networks Wins the Emerging Communications Conference &amp; Awards Inaugural &quot;Company to Watch&quot; Award&quot;], ''[[Press Release]]'', New York, 2 November 2009. Retrieved on 2010-05-16.&lt;/ref&gt;

The company has three published [[patent applications]] for analyzing [[sensor]] data streams, System and Method of Performing Location Analytics (US 20090307263), Comparing Spatial-Temporal Trails in Location Analytics (US 20100079336), and Anomaly Detection in Sensor Analytics (US 20100082301).&lt;ref&gt;[http://appft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=/netahtml/PTO/search-bool.html&amp;r=0&amp;f=S&amp;l=50&amp;TERM1=skibiski&amp;FIELD1=&amp;co1=AND&amp;TERM2=&amp;FIELD2=&amp;d=PG01 US Patent &amp; Trademark Office]. Retrieved on 2010-05-12.&lt;/ref&gt;

The company was acquired by the [[Yellow Pages]] in 2014. This is a marketing conglomerate under AT&amp;T and Cerberus Capital Management.&lt;ref&gt;techcrunch/newyork times&lt;/ref&gt;

==Products and services==
The Citysense consumer application, that shows hotspots of human activity in real-time from mobile phone location and taxicab GPS data,&lt;ref&gt;Silver, James. [http://www.wired.co.uk/wired-magazine/archive/2009/06/features/the-hidden-persuaders-mining-your-mobile-phone-log.aspx &quot;The Hidden Persuaders&quot;], ''[[Wired Magazine]]'', London, 22 June 2009. Retrieved on 2010-05-12.&lt;/ref&gt;    was named by [[ReadWriteWeb]] (in The [[New York Times]]) as &quot;Top 10 Internet of Things Products of 2009&quot;.&lt;ref&gt;Macmanus, Richard. [http://www.nytimes.com/external/readwriteweb/2009/12/08/08readwriteweb-top-10-internet-of-things-products-of-2009-74048.html &quot;Top 10 Internet of Things Products of 2009&quot;], ''[[The New York Times]]'', New York, 8 December 2009. Retrieved on 2010-05-13.&lt;/ref&gt;

The Cabsense consumer application, that shows the best place to catch a New York City taxicab based GPS data from the vehicles, launched in March 2010.&lt;ref&gt;Grynbaum, Michael. [http://www.nytimes.com/2010/04/03/nyregion/03icab.html &quot;Need a Cab? New Analysis Shows Where to Find One&quot;], ''[[The New York Times]]'', New York, 2 April 2010. Retrieved on 2010-05-16.&lt;/ref&gt;

The Macrosense platform is for mobile application providers and mobile phone carriers to analyze billions of customer location data points for predictive analytics in advertising and churn management applications.

==Privacy &amp; Data Ownership==
The company allows users to opt-out of their service through their website, and users may monitor their profile through their application.
The company does not collect identifiable data (such as phonenumbers or names); it collects data received from cellphone to construct anonymous profiles of consumers. This anonymous data/profiles may then be sold to third parties.

The company's privacy and data ownership policies are based on The New Deal on Data, as advocated by Alex &quot;Sandy&quot; Pentland, head of the Human Dynamics group at MIT. &lt;ref&gt;needs citation&lt;/ref&gt;

==See also==
*[[Geosocial Networking]]
*[[Location-based service]]
*[[Location awareness]]

==References==
{{reflist}}

==External links==
* [http://www.sensenetworks.com Sense Networks website]
* [http://www.cabsense.com CabSense website]
* [http://www.citysense.com CitySense website]

[[Category:Android (operating system) software]]
[[Category:IOS software]]
[[Category:Mobile software]]
[[Category:Applied machine learning]]
[[Category:Big data]]</text>
      <sha1>shqsqd9bziked9s1zfe53lsnkwf4hhi</sha1>
    </revision>
  </page>
  <page>
    <title>Sumo Logic</title>
    <ns>0</ns>
    <id>35061359</id>
    <revision>
      <id>669807366</id>
      <parentid>669802935</parentid>
      <timestamp>2015-07-03T17:49:02Z</timestamp>
      <contributor>
        <username>Rsrikanth05</username>
        <id>295080</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/117.198.137.84|117.198.137.84]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11193">{{buzzword|date=April 2015}}
{{Infobox company
| name       = Sumo Logic
| type       = [[Privately held company]]
| logo       = 
| industry   = [[Computer software]], [[Security management]], [[Enterprise software]]
| foundation = 2010
| founder    = Kumar Saurabh, Christian Beedgen
| location   = [[Redwood City, California|Redwood City]], [[California]], [[United States|USA]]
| key_people = {{unbulleted list
 | Ramin Sayar {{small|([[CEO]])}}
 | Kumar Saurabh {{small|([[Entrepreneurship|Co-Founder]] &amp; Vice President of Engineering)}}
 | Christian Beedgen {{small|([[Chief technology officer|CTO]], [[Entrepreneurship|Co-founder]])}}
 | Bruno Kurtic {{small|(Founding Vice President of Product and Strategy)}}
}}
| num_employees = 250+ (2015)
| revenue       = 
| net_income    = 
| services      = [[Log management and intelligence|Log Management and Intelligence]], [[Log analysis|Log Analysis]]
| homepage    = [http://www.sumologic.com// sumologic.com]
| footnotes     = 
}}

'''Sumo Logic''' is a [[Cloud computing|cloud-based]] [[Log management and intelligence|log management]] and [[Log analysis|analytics]] service that leverages [[Machine-generated data|machine-generated]] [[Big Data|big data]] to deliver [[Real-time computing|real-time]] [[Information technology|IT]] insights.&lt;ref name=&quot;eweek&quot;&gt;{{cite news|url=http://www.eweek.com/c/a/Application-Development/Cloud-Analytics-Startup-Sumo-Logic-Exits-Stealth-Raises-15M-574795/|title=Cloud Analytics Startup Sumo Logic Exits Stealth, Raises $15M|first=Darryl|last=Taft|date=January 31, 2012| work=EWeek}}&lt;/ref&gt; Headquartered in [[Redwood City, California|Redwood City]], [[California]], Sumo Logic was founded in April 2010 by [[ArcSight]] veterans Kumar Saurabh and Christian Beedgen, and has received funding from [[Accel Partners]], [[Draper_Fisher_Jurvetson|DFJ Growth]], [[Greylock Partners]], [[Institutional Venture Partners]], [[Sequoia Capital]], [[Sutter Hill Ventures]] and angel investor Shlomo Kramer.&lt;ref&gt;{{cite news|url=http://www.marketwire.com/press-release/sumo-logic-raises-15m-series-b-round-next-generation-log-management-analytics-1612992.htm|title=Sumo Logic Raises $15M Series B Round for Next Generation Log Management and Analytics|first=Hope|last=Nicora|date=January 31, 2012|work=Market Wired}}&lt;/ref&gt; While Sumo Logic remained in [[stealth mode]] for two years, it unveiled its cloud-based log management platform with [[Venture round|Series B]] funding of $15 million in January 2012.&lt;ref name=&quot;eweek&quot; /&gt; The round of [[Venture round|Series E]] funding announced in June 2015 brings the company’s total venture capital backing to $160.5 million.&lt;ref&gt;{{cite news| url=http://www.marketwired.com/press-release/sumo-logic-secures-80-million-investment-support-continued-customer-growth-platform-2024969.htm/|title=Sumo Logic Secures $80 Million Investment to Support Continued Customer Growth and Platform Expansion|first=Caitlin|last=Haskins|date=June 1, 2015|work=Market Wired}}&lt;/ref&gt;

==Technology==
Sumo Logic’s architecture features an elastic [[petabyte]] scale platform that collects, manages, and analyzes enterprise [[Logfile|log data]], reducing millions of log lines into operational and security insights in [[Real-time computing|real time]].&lt;ref&gt;{{cite news|url=http://techcrunch.com/2012/01/31/log-data-management-and-analytics-startup-sumo-logic-raises-15m-from-greylock-and-others/|title=Log Data Management And Analytics Startup Sumo Logic Raises $15M From Greylock And Others|first=Leena|last=Rao|date=January 31, 2012|work=TechCrunch}}&lt;/ref&gt; Their [[Cloud computing|cloud-based]] approach overcomes the inherent problems of [[On-premise software|premise-based solutions]], including limits on scalability, inefficient or haphazard analysis, and uncontrolled costs.&lt;ref name=&quot;dailydisruption&quot;&gt;{{cite news|url=http://www.dailydisruption.com/2012/02/disruptor-of-the-day-sumo-logic-harnessing-the-power-of-big-data-for-real-time-it/|title=Disruptor of the Day: Sumo Logic – Harnessing the Power of Big Data for Real-Time IT|first=Bill|last=Klump|date=February 14, 2012|work=Daily Disruption}}&lt;/ref&gt;

Sumo Logic is built around a globally distributed data retention architecture that keeps all log data available for instant analysis, eliminating the need for an enterprise to manage the cost and complexity of data archiving, backups and restoration.&lt;ref name=&quot;dailydisruption&quot;/&gt;

The service is entirely cloud-based and is maintenance free.&lt;ref&gt;{{cite news|url=http://venturebeat.com/2012/01/31/sumo-logic-raises-15m-drops-its-stealthy-status/|title=Sumo Logic raises $15M, drops its stealthy status|first=Sarah|last=Mitroff|date=January 31, 2012|work=VentureBeat}}&lt;/ref&gt; Instead of inelastic [[security information and event management]] systems, Sumo Logic employs elastic processing to collect, manage, and analyze log data, regardless of type, volume, or location.&lt;ref&gt;{{cite news|url= http://www.forbes.com/sites/benkepes/2014/02/21/sumo-logic-delivers-enterprise-security-analytic-product/| title=Sumo Logic Delivers Enterprise Security Analytic Product|first=Ben|last=Kepes|date=February 21, 2014|work=Forbes}}&lt;/ref&gt;

Sumo Logic modeled its approach on that of [[Google]], according to Christian Beedgen, the company’s [[Chief technology officer|CTO]] and one of its [[Entrepreneurship|co-founders]]. Using advanced [[Machine learning|machine-learning]] algorithms to whittle down mountains of log file data into common groupings, Sumo Logic's platform mirrors [[Google News]]'s categorization of news stories distributed across the web. In doing so, Sumo Logic is able to ease the process for administrators to synthesize and analyze their data.&lt;ref&gt;{{cite news|url=http://servicesangle.com/blog/2012/01/31/sumo-logic-emerges-from-stealth-to-take-on-splunk-log-data-analytics/|title=Sumo Logic Emerges From Stealth To Take On Splunk, Log Data Analytics|first=Jeffrey|last=Kelly|date=January 31, 2012|work=Services Angle}}&lt;/ref&gt;

In June 2012, Sumo Logic announced Sumo Logic Free, a [[freemium]] full functionality edition of its analytical solution that is deployed on [[Amazon Web Services]],&lt;ref&gt;{{cite web|url=http://siliconangle.com/blog/2012/06/26/sumo-logic-goes-freemium-with-data-analysis/|title=Sumo Logic Goes Freemium with Data Analysis|first=Maria|last=Deutscher|date=June 26, 2012|work=Silicon Angle}}&lt;/ref&gt; and in August 2012, the company announced Sumo Logic for [[VMware]], which allows enterprises to search, visualize and analyze all VMware logs in real time so they can monitor and detect events within VMware virtual environments.&lt;ref&gt;{{cite news|url=http://www.talkincloud.com/sumo-logic-for-vmware-digs-deep-into-real-time-insights/|title=Sumo Logic for VMware Digs Deep into Real-Time Insights|first=Chris|last=Talbot|date=August 24, 2012|work=TalkinCloud}}&lt;/ref&gt;

==Leadership==
Sumo Logic was founded in 2010 by a technical leadership team with expertise in [[Log management and intelligence|log management]], [[Scalability|scalable systems]], [[Big Data]] and [[Computer security|security]], including:
* Kumar Saurabh, Acting [[Chief executive officer|CEO]], [[Entrepreneurship|Co-Founder]] &amp; [[Vice president|Vice President]] of [[Analytics]], formerly of [[ArcSight]]
* Christian Beedgen, [[Entrepreneurship|Co-Founder]] &amp; [[Chief technology officer|CTO]], formerly of [[ArcSight]]
* Bruno Kurtic, Founding [[Vice president|Vice President]] of [[Product management|Product and Strategy]], formerly of [[Sensage]]

Ramin Sayar, formerly of [[VMWare]], is Sumo Logic's CEO since December 2014.&lt;ref&gt;{{cite news|url=http://www.marketwired.com/press-release/sumo-logic-surpasses-all-goals-hires-industry-veteran-ramin-sayar-as-ceo-lead-rapid-1973294.htm|title=Sumo Logic Surpasses All Goals and Hires Industry Veteran Ramin Sayar as CEO to Lead Rapid Growth Phase|first=Caitlin|last=Haskins|date=December 2, 2014|work=Market Wired}}&lt;/ref&gt;
Previously, Vance Loiselle, formerly of [[BladeLogic]], was CEO between May 2012&lt;ref&gt;{{cite news|url=http://www.marketwire.com/press-release/sumo-logic-appoints-vance-loiselle-as-chief-executive-officer-1660745.htm|title=Sumo Logic Appoints Vance Loiselle as Chief Executive Officer|first=Melissa|last=Hick|date=May 23, 2012|work=Market Wired}}&lt;/ref&gt; and December 2014.

In April 2012, Sumo Logic formed a new advisory board, bringing in three [[Silicon Valley]] veterans: [[DJ Patil]] of [[Greylock Partners]], Gerhard Eschelbeck of [[Sophos]], and Nir Zuk of [[Palo Alto Networks]].&lt;ref&gt;{{cite news|url=http://siliconangle.com/blog/2012/04/12/sumo-logic-brings-data-rock-stars-to-advisory-board/|title=Sumo Logic Brings Data Rock Stars to Advisory Board|first=Kristen|last=Nicole|date=April 12, 2012|work=Silicon Angle}}&lt;/ref&gt;

==Patents==
Sumo Logic’s service is powered by patent-pending Elastic Log Processing, LogReduce, and Push Analytics technologies.&lt;ref name=&quot;eweek&quot; /&gt;

==Awards and Recognition==
In January, 2012, [[RSA Conference|RSA]] named Sumo Logic one of its top 10 finalists for the Most Innovative Company at RSA.&lt;ref&gt;{{cite news|url=http://www.businesswire.com/news/home/20120105005264/en/Top-10-Finalists-Announced-Innovative-Company-RSA®|title=Top 10 Finalists Announced for the &quot;Most Innovative Company at RSA® Conference 2012&quot; Contest|work=Business Wire|date=January 5, 2012|first=Mike|last=Fearon}}&lt;/ref&gt; At the April 2012 Under the Radar Conference, Sumo Logic received the Judge's Choice and the Audience Choice Awards for [[System Monitor|Performance Monitoring]].&lt;ref&gt;{{cite news|url=http://utrconf.com/announcing-the-winners-of-under-the-radar-consumerization-of-it-2/|title=Announcing the Winners of Under the Radar: Consumerization of It|first=Clare|last=Ryan|date=April 26, 2012|work=Under the Radar}}&lt;/ref&gt; In May 2012, Sumo Logic was named a [[Red Herring (magazine)|Red Herring]] Americas 2012 Top 100 Winner.&lt;ref&gt;{{cite web|url=http://www.redherring.com/red-herring-americas/americas-2012-finalists/|title=2012 Red Herring North America: Finalists|date=2012|work=Red Herring}}&lt;/ref&gt;

In May 7, 2014, Sumo Logic was named as a Cool Vendor by [[Gartner]] in the [[Application performance management|Application Performance Monitoring (APM)]] and [[Software analytics|IT Operations Analytics (ITOA)]] categories.&lt;ref&gt;{{cite web|url=http://blogs.gartner.com/jonah-kowall/2014/04/30/cool-vendors-in-application-performance-monitoring-apm-and-it-operations-analytics-itoa/|title=Cool Vendors in Application Performance Monitoring (APM) and IT Operations Analytics (ITOA)|first=Jonah|last=Kowall|date=April 30, 2014|work=Gartner Blog}}&lt;/ref&gt;

==See also==
* [[Stackify]]
* [[AppDynamics]]
* [[Logentries]]
* [[Loggly]]
* [[New Relic]]
* [[Splunk]]

==References==
{{reflist}}

[[Category:Big data]]
[[Category:Companies established in 2010]]
[[Category:Companies based in Redwood City, California]]
[[Category:Computer security companies]]
[[Category:Computer data]]
[[Category:Data security]]
[[Category:Information technology management]]
[[Category:Information technology companies of the United States]]
[[Category:Network management]]
[[Category:Privately held companies based in California]]
[[Category:Security companies]]
[[Category:System administration]]</text>
      <sha1>j562v1v5wgnhm2ktwmo5cdstcolit87</sha1>
    </revision>
  </page>
  <page>
    <title>Talend</title>
    <ns>0</ns>
    <id>24143094</id>
    <revision>
      <id>668302582</id>
      <parentid>667185659</parentid>
      <timestamp>2015-06-23T15:33:57Z</timestamp>
      <contributor>
        <username>Faltenin</username>
        <id>855472</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14316">{{Infobox company
| name             = Talend
| logo             = [[File:Talend logo.png|170px]]
| type             = Private
| genre            =
| fate             =
| predecessor      =
| successor        =
| foundation       = 2006
| founder          =
| defunct          =
| location_city    = Redwood City, CA USA
| location_country =
| location         =
| locations        =
| area_served      = Worldwide
| key_people       = Mike Tuchen, CEO
| industry         = Computer Software
| products         = Big Data, Data Integration, Data Quality, MDM, ESB, BPM
| production       =
| services         =
| revenue          =
| operating_income =
| net_income       =
| aum              =
| assets           =
| equity           =
| owner            =
| num_employees    = 400
| parent           =
| divisions        =
| subsid           =
| homepage         = http://www.talend.com/
| footnotes        =
| intl             =
}}
'''Talend''' (''Pronunciation: TAL-end'') is an [[open source]] software vendor that provides [[data integration]], [[data management]], [[enterprise application integration]] and [[big data]] software and services. Headquartered in Redwood City, California, Talend has offices in North America, Europe and Asia, and an international network of technical and service partners.  Customers include [[eBay]], [[Virgin Mobile]], [[Sony Online Entertainment]], [[Deutsche Post]] and [[Allianz]].{{citation needed|date=October 2013}} It has 400 employees in 14 offices in 7 countries.{{citation needed|date=October 2013}}

== History ==
Talend was founded in 2005 by Bertrand Diard and Fabrice Bonan. It was the first [[commercial open source software|commercial open source]] vendor of data integration software{{fact|date=October 2014}}. Other vendors have since entered this market, including [[Apatar]], [[Jitterbit]], and [[Pentaho]]. Non open source data integration vendors include [[Software AG]], [[Ab Initio (company)|Ab Initio]], [[SAS Institute]], [[Pervasive Software]], [[IBM]], [[Informatica]], [[SAP AG|SAP]], [[RedHat]].

The company's first product, [[Talend Open Studio for Data Integration]],&lt;ref&gt;[http://www.networkworld.com/news/2007/042407-talend-data-integration.html?page=1 Open source start-up upgrades data integration software]. Networkworld.com (2007-04-24). Retrieved on 2013-01-10.&lt;/ref&gt; was launched in October 2006, under its previous name: ''Talend Open Studio''. In January 2008, it had been downloaded over 1 million times. In January 2012, the product totaled 20 million downloads and the company had over 3500 clients around the world.&lt;ref&gt;[http://apache.ulitzer.com/node/2294767 Talend’s Enterprise Manageability Capabilities Streamline Hadoop Deployments]. Apache.ulitzer.com. Retrieved on 2013-01-10.&lt;/ref&gt;

In September 2009, Talend acquired a [[master data management]] (MDM) solution Xtentis, which was developed by a French software vendor [[Amalto]].&lt;ref name=&quot;iweek09&quot;&gt;{{cite news |last=Babcock |first=Charles |date=September 28, 2009 |title=Talend Acquires Master Data Management Firm |url=http://www.informationweek.com/software/operating-systems/talend-acquires-master-data-management-firm-/d/d-id/1083503? |newspaper=[[InformationWeek]] |location= |publisher= |accessdate=18 November 2014}}&lt;/ref&gt;&lt;ref name=&quot;iweek09_2&quot;&gt;{{cite news |last=Henschen |first=Doug |date=September 28, 2009 |title=Talend Plans Open-Source Master Data Management Product |url=http://www.informationweek.com/software/information-management/talend-plans-open-source-master-data-management-product/d/d-id/1083496|newspaper=[[Information Week]] |location= |publisher= |accessdate=18 November 2014}}&lt;/ref&gt;&lt;ref name=&quot;chn09&quot;&gt;{{cite news |last=Basyn |first=Dirk |date=28 September 2009 |title=Amalto revend sa brique MDM a Talend |url=http://www.channelnews.fr/actu-societes/fournisseurs/4431-amalto-revend-sa-brique-mdm-a-talend.html |language=French |newspaper=Channel News |location= |publisher= |accessdate=18 November 2014}}&lt;/ref&gt; In January 2010, the system was released as an open source product under name ''Talend MDM''&lt;ref name=&quot;iweek10&quot;&gt;{{cite news |last=Henschen |first=Doug |date=November 10, 2010 |title=Talend Acquires Open Source Service Bus |url=http://www.informationweek.com/software/talend-acquires-open-source-service-bus/d/d-id/1094048 |newspaper=[[InformationWeek]] |location= |publisher= |accessdate=18 November 2014}}&lt;/ref&gt; (delivered as a free open source version ''Talend Open Studio for MDM'' and subscription-based ''Talend Platform for MDM''). It became the first open source MDM solution on the market.&lt;ref name=&quot;netcom10&quot;&gt;{{cite news |last=Babcock |first=Charles |date=January 25, 2010 |title=Talend Enters Master Data {{sic|nolink=y|Managment|expected=Management}} Arena |url=http://www.networkcomputing.com/storage/talend-enters-master-data-managment-arena/d/d-id/1086410 |newspaper=Network Computing |location= |publisher=[[InformationWeek]] |accessdate=18 November 2014}}&lt;/ref&gt;

Talend is backed by six [[venture capital]] firms. The first two rounds of funding were provided by AGF Private Equity and Galileo Partners. In January 2009, [[Bernard Liautaud]], the founder of Business Objects, led a $12 million round C&lt;ref&gt;[http://www.latribune.fr/journal/archives/edition-du-2301/126471/bernard-liautaud-finance-lediteur-de-logiciels-talend.html Bernard Liautaud finance l'éditeur de logiciels Talend]. LaTribune.fr. Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.techcrunch.com/2009/01/26/more-validation-for-open-source-software-talend-raises-12-million/ More Validation For Open Source Software: Talend Raises $12 Million]. TechCrunch (2009-01-26). Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.itbusinessedge.com/cm/community/news/inf/blog/talends-fundraising-validates-open-source-data-integration/?cs=30028 Talend's Fundraising Validates Open Source Data Integration]. Itbusinessedge.com (2009-01-26). Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.sdtimes.com/DATA_INTEGRATOR_TALEND_SCORES_12M_IN_FUNDING/About_DATAINTEGRATION_and_OPENSOURCE_and_TALEND/33208 Data integrator Talend scores $12M in funding]. Sdtimes.com. Retrieved on 2013-01-10.&lt;/ref&gt; for his firm [[Benchmark Capital|Balderton Capital]]. In November 2010, [[Silver Lake Partners|Silver Lake Sumeru]] led a $34 million round and Talend announced at the same time the acquisition of Sopera, a strategic member of the [[Eclipse Foundation]].&lt;ref&gt;Flinn, Ryan. (2010-11-10) [http://www.bloomberg.com/news/2010-11-10/silver-lake-sumeru-invests-in-data-management-company-talend.html Silver Lake Sumeru Invests in Data Management Company Talend]. Bloomberg.com. Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://techcrunch.com/2010/11/10/open-source-software-company-talend-raises-34m-acquires-sopera/ Open Source Software Company Talend Raises $34M; Acquires Sopera]. Techcrunch.com (2010-11-10). Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.france24.com/en/20101113-bertrand-diard-ceo-talend-internet-data-softwar-open-source Bertrand DIARD, CEO and Co-founder of Talend]. France24.com. Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.latribune.fr/blogs/blog-initie/20101110trib000571525/nouvelle-etape-de-croissance-pour-talend.html Nouvelle étape de croissance pour Talend]. Latribune.fr. Retrieved on 2013-01-10.&lt;/ref&gt; In May 2011 Talend introduced its Unified Integration Platform – an important milestone subsequent to the acquisition.  In June 2011 the company also released Talend Cloud to support all forms of Cloud-based computing. In February 2012, Talend Open Studio for Big Data, an integration application for big data, was released. In December 2013, Talend raised $40 million from Bpifrance, [[Iris Capital]] and [[Silver Lake Partners|Silver Lake Sumeru]].&lt;ref&gt;[http://techcrunch.com/2013/12/11/talend-raises-40m-to-more-aggressively-extend-into-big-data-market-sets-sights-on-ipo/ Talend Raises $40M To More Aggressively Extend Into Big Data Market, Sets Sights On IPO]&lt;/ref&gt;

In 2011 the company was listed on the Momentum Index of venture-backed companies with an open source business model.&lt;ref&gt;http://momentumindex.com/96-open-source-start-ups-ranked-by-momentum-index-3/&lt;/ref&gt;

Today, Talend has more than 4000 paying customers including eBay, Virgin Mobile, Sony Online Entertainment, Deutsche Post and Allianz.{{citation needed|date=October 2013}}

== Products ==
{{ad|date=February 2015}}
=== Data management ===
[[File:Talend Open Studio.jpg|thumb|[[software]] Talend Open Studio for Data Integration Screenshot]]
* ''Talend Open Studio for Big Data'': combining big data technologies into a unified open source environment simplifying the loading, extraction, transformation and processing of large and diverse data sets
* ''Talend Enterprise Big Data'': a big data integration solution that extends Talend Open Studio for Big Data with teamwork and management features
* ''Talend Platform for Big Data'': a powerful and versatile big data integration and data quality [[solution]] that simplifies the loading, extraction and processing of large and diverse data sets so you can make more informed and timely decisions

* ''[[Talend Open Studio for Data Integration]]'': an open source application for data integration job design with a graphical development environment
* ''Talend Enterprise Data Integration'': extends Talend Open Studio for Data Integration with technical support and additional features
* ''Talend Platform for Data Management'': turn disparate, duplicate sources of data into trusted stores of consolidated information
* ''Talend Platform for Data Services'': a comprehensive unified data, application and service integration solution that lessens the impact of changing data structures while making the management of data across domains easier.

* ''Talend Open Studio for MDM'': a set of functions for master data management that provides functionality for integration, quality, governance, mastering and collaborating on enterprise data
* ''Talend Platform for Master Data Management'': turn disparate, inconsistent information across a business into a single, reliable “version of the truth”, providing increased confidence in decisions made
* ''[[Talend Open Studio for Data Quality]]'': an open source data profiling tool that examines the content, structure and quality of complex data structures
{{buzzword|date=February 2015}}
=== Application integration ===
[[File:Talend Open Profiler.gif|thumb|[[software]] Talend Open Studio for Data Quality Screenshot]]
* ''Talend Integration Cloud'': a Cloud-based integration Platform-as-a-Service ([[Cloud-based_integration|iPaaS]]) for [[enterprise service bus|application integration]] and [[data integration]]
* ''Talend ESB Standard Edition'': an Apache-based open source [[enterprise service bus]]
* ''Talend Open Studio for [[Enterprise service bus|ESB]]'': an [[enterprise service bus]] and a standards-based connectivity layer used to integrate distributed systems across functional, enterprise and geographic boundaries. It is powered by the [[Apache CXF]], [[Apache Camel]] and [[Apache ActiveMQ]] open source integration projects
* ''Talend Enterprise ESB'': extends Talend Open Studio for ESB with advanced deployment and management functions
* ''Talend Platform for Enterprise Integration'': unifies business process management, application integration and data management allowing firms to increase business productivity, deliver projects faster, and lower operating costs

== Community ==
Talend is an [[Apache Software Foundation]] sponsor and many of its engineers are contributors to Apache including [[CXF]], [[Apache Camel|Camel]], [[ServiceMix]], Karaf, Santuario, and [[ActiveMQ]].{{citation needed|date=October 2013}} The company is also a member of the [[Java Community Process]] (JCP) and is a Strategic Developer Member of the [[Eclipse Foundation]], a Corporate Member of [[OW2]], and has active involvement in the [[OASIS (organization)|OASIS]] organization.{{citation needed|date=October 2013}}

Talend publishes the code of its core modules under the [[GNU Public License]] or the [[Apache License]]. Talend is also a contributor to key open source projects{{which|date=October 2013}}.

[[Java (programming language)|Java]] is the development language of Talend’s products and services.

Its commercial partners include [[Bonitasoft]], [[Cloudera]], [[Greenplum]], [[Google]], [[Hortonworks]], [[Impetus Technologies]],&lt;ref&gt;http://www.talend.com/ecosystem/company/impetus&lt;/ref&gt; [[Jaspersoft]], [[Netezza]], [[Teradata]] and [[Vertica]].  Uniserv&lt;ref&gt;http://www.uniserv.com/en/news-data-quality/news2011/aktuelles_040.php&lt;/ref&gt;

==Talendforge==
Talendforge.org is Talend’s technical community site. Sections available for users include a support [[Internet forum|forum]], a [[wiki]], [[Bugtracker]], an exchange, components, tutorials and the translation tool Babili.&lt;ref&gt;http://www.talendforge.org/ Talendforge&lt;/ref&gt;

== License ==
Talend uses the [[open core]] [[business model]].&lt;ref&gt;Aslett,Matthew (2009-07-08) [http://web.archive.org/web/20100801004103/http://blogs.the451group.com/opensource/2009/07/08/what-is-open-core-licensing-and-what-isnt/ What is Open Core Licensing (and what isn’t)]. blogs.the451group.com&lt;/ref&gt; Talend publishes the code of its core modules under open source licenses including the [[GNU Public License]], LGPL and the [[Apache License]], and value added features and services under a commercial subscription license.

The commercial subscription license includes:
* Access to value added features (such as teamwork, [[Load balancing (computing)|load balancing]], [[Network monitoring|monitoring]])
* Technical support
* IP indemnification (legal protection)

== See also ==
* [[Talendforge]]

== References ==
{{reflist|35em}}

==External links==
{{Commons|Talend}}
* {{official website|http://www.talend.com/}}
* Talend Help Center: https://help.talend.com/
* Official Community Support site, TalendForge: http://www.talendforge.org/
* TalendByExample.com, tips &amp;amp; tutorials: http://www.talendbyexample.com/
* DwETL.com, tutorials, Custom components: http://dwetl.com/ 

[[Category:Enterprise application integration]]
[[Category:Business intelligence]]
[[Category:Software companies based in California]]
[[Category:Data warehousing products]]
[[Category:Talend|*]]
[[Category:Big data]]</text>
      <sha1>ovqybc6wsm0sasb4yigmvaljj2yw5vk</sha1>
    </revision>
  </page>
  <page>
    <title>Teradata</title>
    <ns>0</ns>
    <id>168680</id>
    <revision>
      <id>673314255</id>
      <parentid>673313994</parentid>
      <timestamp>2015-07-27T14:55:46Z</timestamp>
      <contributor>
        <username>Mjdono</username>
        <id>7616528</id>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27603">{{multiple issues|
{{news release |date=September 2013}}
{{advert|date=September 2013}}
{{Original research|date=September 2013}}
}}
 {{Use mdy dates|date=January 2013}}
{{Infobox company
| name   = Teradata Corporation
| logo   = TeradataLogoVertical.gif
| type   = [[Public company|Public]]
| traded_as      = {{nyse|TDC}}&lt;br /&gt;[[S&amp;P 500|S&amp;P 500 Component]]
| company_slogan = Raising Intelligence
| foundation     = 1979
| location       = [[Miamisburg, Ohio]]
| key_people     = President and CEO
| num_employees  = 10,200 (January 2014)&lt;ref name=macroaxis&gt;{{cite web |url=http://www.macroaxis.com/invest/ratio/TDC--Number_of_Employees |title=Teradata Number of Employees TDC NYSE |accessdate=2014-01-12}}&lt;/ref&gt;
| industry       = [[Data warehousing]]
| products       = Integrated Data Warehouse Hardware and Software, Professional Services, Customer Services
| revenue = {{unbulleted list|{{nowrap|{{increase}} US$ 2,692.0&amp;nbsp;million (2013) &lt;ref name='xbrlus_1'&gt;{{cite web|url=http://www.sec.gov/Archives/edgar/data/816761/000119312514071369/0001193125-14-071369-index.htm |title=TERADATA CORP /DE/ 2013 Annual Report Form (10-K) |publisher=United States Securities and Exchange Commission |format=XBRL |date=February 27, 2014}}&lt;/ref&gt;}}|{{nowrap|{{increase}} US$ 2,665.0&amp;nbsp;million (2012) &lt;ref name='xbrlus_1'/&gt;}}}}
| net_income = {{unbulleted list|{{nowrap|{{decrease}} US$ 377.0&amp;nbsp;million (2013) &lt;ref name='xbrlus_1'/&gt;}}|{{nowrap|{{increase}} US$ 419.0&amp;nbsp;million (2012) &lt;ref name='xbrlus_1'/&gt;}}}}
| equity = {{unbulleted list|{{nowrap|{{increase}} US$ 1,857.0&amp;nbsp;million (2013) &lt;ref name='xbrlus_2'/&gt;}}|{{nowrap|{{increase}} US$ 1,779.0&amp;nbsp;million (2012) &lt;ref name='xbrlus_1'/&gt;}}}}
| assets = {{unbulleted list|{{nowrap|{{increase}} US$ 3,096.0&amp;nbsp;million (2013) &lt;ref name='xbrlus_2'&gt;{{cite web|url=http://www.sec.gov/Archives/edgar/data/816761/000119312514192949/0001193125-14-192949-index.htm |title=TERADATA CORP /DE/ 2014 Q1 Quarterly Report Form (10-Q) |publisher=United States Securities and Exchange Commission |format=XBRL |date=May 12, 2014}}&lt;/ref&gt;}}|{{nowrap|{{increase}} US$ 3,066.0&amp;nbsp;million (2012) &lt;ref name='xbrlus_1'/&gt;}}}}
| operating_income = {{unbulleted list|{{nowrap|{{decrease}} US$ 532.0&amp;nbsp;million (2013) &lt;ref name='xbrlus_1'/&gt;}}|{{nowrap|{{increase}} US$ 580.0&amp;nbsp;million (2012) &lt;ref name='xbrlus_1'/&gt;}}}}
| homepage       = {{URL|https://www.teradata.com}}
}}

'''Teradata Corporation''' is a publicly-held international computer company that sells analytic data platforms, marketing applications and related services. Its analytics products are meant to consolidate data from different sources and make the data available for analysis. Teradata marketing applications are meant to support marketing teams that use data analytics to inform and develop programs. In early 2015, the company formed two divisions: Data &amp; Analytics for its data analytics platforms and related services and Marketing Applications for its marketing software and related services.&lt;ref&gt;{{cite news|title=Teradata Appoints Co-Presidents - 2/26/2015|url=http://www.teradata.com/News-Releases/2015/Teradata-Appoints-Co-Presidents/?LangType=1033&amp;LangSelect=true|accessdate=27 July 2015|publisher=Teradata|date=26 February 2015|ref=1}}&lt;/ref&gt; The corporate headquarters are in [[Miamisburg, Ohio|Miamisburg]], [[Ohio]].

==Corporate overview==
Teradata is an [[enterprise software]] company that develops and sells a [[relational database management system]] (RDBMS) with the same name. Teradata is publicly traded on the New York Stock Exchange (NYSE) under the stock symbol TDC.&lt;ref name=NCRspinoff&gt;{{cite press release |url=http://www.ncr.com/about_ncr/media_information/news_releases/2007/october/100107a.jsp |publisher=NCR |date=October 1, 2007 |title=NCR Completes Teradata Spin Off}}&lt;/ref&gt;

The Teradata product is referred to as a &quot;data warehouse system&quot; and stores and manages data. The data warehouses use a &quot;shared nothing&quot; architecture, which means that each server node has its own memory and processing power.&lt;ref&gt;http://www.teradatatech.com/?p=103&lt;/ref&gt; Adding more [[server (computing)|server]]s and nodes increases the amount of data that can be stored. The [[database software]] sits on top of the servers and spreads the workload among them.&lt;ref name=&quot;Lawson&quot;&gt;{{cite web|last=Lawson|first=Loraine|title=How Business Logic Modeling Helps Data Warehouse Integration|url=http://www.itbusinessedge.com/cm/community/features/interviews/blog/how-business-logic-modeling-helps-data-warehouse-integration/?cs=43396|work=IT Business Edge|accessdate=December 13, 2011}}&lt;/ref&gt; Teradata sells applications and software to process different types of data. In 2010, Teradata added [[text analytics]] to track [[unstructured data]], such as word processor documents, and semi-structured data, such as spreadsheets.&lt;ref&gt;{{cite web|last=Sambandaraksa|first=Don|title=Teradata launches new text analytics|url=http://www.bangkokpost.com/tech/computer/210208/teradata-launches-new-text-analytics|date=August 12, 2010|work=Bangkok Post|accessdate=December 13, 2011}}&lt;/ref&gt;

Teradata's product can be used for business analysis. Data warehouses can track company data, such as sales, customer preferences, product placement, etc.&lt;ref name=&quot;Lawson&quot;/&gt;

Teradata has a supplier diversity program that designates a minimum of 3 to 5% of spending on minority, women, veteran, or small business vendors.&lt;ref&gt;{{cite news|title=Teradata Targets Diverse Suppliers|url=http://www.bizjournals.com/dayton/stories/2009/11/02/story1.html |work=Dayton Business Journal|accessdate=December 13, 2011|first=Joe|last=Cogliano|date=November 2, 2009}}&lt;/ref&gt;

In 2013, the [[Ethisphere Institute]] named Teradata as one of the &quot;World's Most Ethical Companies.&quot;&lt;ref&gt;http://ethisphere.com/worlds-most-ethical/wme-honorees/&lt;/ref&gt;

==History==
Timeline information taken from Teradata company history unless otherwise cited.&lt;ref&gt;{{cite web|title=History|url=http://www.teradata.com/history/|publisher=Teradata company website|accessdate=December 13, 2011}}&lt;/ref&gt;

*1976–1979: concept of Teradata grows from research at [[California Institute of Technology]] (Caltech) and from the discussions of [[Citibank]]'s advanced technology group.&lt;ref name=&quot;tdhist&quot;&gt;[http://www.teradata.com/t/go.aspx/page.html?id=42649 Teradata Milestones]&lt;/ref&gt;
*Incorporated in 1979 in [[Brentwood, CA]] by Dr. Jack E. Shemer, Dr. Philip M. Neches, Walter E. Muir, Jerold R. Modes, William P. Worth, and Carroll Reed.
*1984: Teradata releases the world's first parallel data warehouses and data marts.&lt;ref name=&quot;Pereira&quot;&gt;{{cite web|last=Pereira|first=Brian|title=Marrying Strategic Intelligence with Operational Intelligence|url=http://www.informationweek.in/Archive/10-01-01/Marrying_Strategic_Intelligence_with_Operational_Intelligence.aspx|date= January 1, 2010|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;
*1986: [[Fortune Magazine]] names Teradata &quot;Product of the Year.&quot;
*1987: Teradata [[initial public offering]] in August
*1989: Teradata partners with NCR to build new database computers.
*September 1991: [[AT&amp;T Corporation]] acquires [[NCR Corporation|NCR]].&lt;ref name=&quot;Andrews&quot;&gt;{{cite news|last=Andrews|first=Edmund|title=AT&amp;T Acquisition, Soon to be Spun Off, Regains NCR Name|url=http://www.nytimes.com/1996/01/11/business/at-t-acquisition-soon-to-be-spun-off-regains-ncr-name.html |work=The New York Times|accessdate=December 13, 2011|date=January 11, 1996}}&lt;/ref&gt;
*December 1991: NCR announces acquisition of Teradata.&lt;ref&gt;{{cite news|title=NCR, Teradata to split up|url=http://money.cnn.com/2007/01/08/news/companies/ncr/index.htm|publisher=CNN Money|accessdate=December 13, 2011|date=January 8, 2007}}&lt;/ref&gt;
*1992: Teradata creates the first system over 1 [[terabyte]], which goes live at [[Wal-Mart]].&lt;ref&gt;{{cite web|last=Hubler|first=David|title=Teradata sees revenue growth in data consolidation|url=http://washingtontechnology.com/articles/2010/12/13/teradata-data-consolidation.aspx |publisher=Washington Technology|accessdate=December 13, 2011}}&lt;/ref&gt;
*1996: A Teradata database becomes the world's largest database at 11 terabytes.
*1997: NCR becomes independent from AT&amp;T.&lt;ref name=&quot;Andrews&quot;/&gt;
*1997: Teradata customer creates world's largest production database at 24 terabytes.
*1999: Teradata customer has world's largest database with 130 terabytes.
*2000: NCR acquires Ceres Integrated Solutions and its customer relationship management software.&lt;ref&gt;{{cite news|title=Company News; NCR Acquires Ceres Integrated in a $90 Million Deal.|url=http://www.nytimes.com/2000/04/12/business/company-news-ncr-acquires-ceres-integrated-in-a-90-million-deal.html|work=The New York Times|accessdate=December 13, 2011 |date= April 12, 2000}}&lt;/ref&gt;
*2000: NCR acquired Stirling Douglas Group and its demand chain management software.&lt;ref name=&quot;stirling&quot;&gt;{{cite news |work= Press release |title=NCR Completes Acquisition of Sterling Douglas Group |publisher= NCR |url= http://investor.ncr.com/phoenix.zhtml?c=83840&amp;p=irol-newsArticle&amp;ID=104395 |date=July 14, 2000|accessdate= August 16, 2013}}&lt;/ref&gt;
*2003: Teradata University Network is created as an independent teaching portal led by university professors. Nearly 170 universities in 27 countries included in network.
*2005: Teradata acquires DecisionPoint software&lt;ref&gt;{{cite web|title=Teradata Acquires DecisionPoint Software|url=http://www.information-management.com/news/1043424-1.html|date= December 5, 2005|publisher=Information Management Online}}&lt;/ref&gt; and rebrands it as Teradata Decision Experts.
*2007: Teradata established as an independent public company. &lt;ref&gt;{{cite news|title=NCR, Teradata to split up|url=http://money.cnn.com/2007/01/08/news/companies/ncr/index.htm|date= January 8, 2007|publisher=CNN Money|accessdate=December 13, 2011}}&lt;/ref&gt;
*2007: Mike Koehler becomes the CEO of Teradata.&lt;ref&gt;{{cite news|last=Hagerty|first=John|title=NCR to Spinoff Teradata|url=http://www.forbes.com/2007/01/09/amr-teradata-ncr-split-biz-logistics-cx_jh_0109tera.html|date=January 9, 2007|work=Forbes|accessdate=December 13, 2011}}&lt;/ref&gt;
*2008: Teradata acquires Claraview, a professional services company, establishing it as a division of Teradata.&lt;ref&gt;{{cite web|last=Grimes|first=Seth|title=Teradata has Acquired BI/DW Firm Claraview|url=http://www.informationweek.com/blog/228900615|date=March 20, 2008|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;
*2011: Teradata acquires [[Aprimo]]&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata eats Aprimo for $550m.|url=http://www.channelregister.co.uk/2010/12/22/teradata_buys_aprimo/|date=December 22, 2010|work=The Register|accessdate=December 13, 2011}}&lt;/ref&gt; and [[Aster Data Systems]].&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata snaps up Aster Data for $263m|url=http://www.theregister.co.uk/2011/03/03/teradata_buys_aster_data/|date=March 3, 2011|work=The Register|accessdate=December 13, 2011}}&lt;/ref&gt;
*2012: Teradata acquires [[eCircle]], a direct marketing company with focus on email.&lt;ref&gt;{{cite news|last1=Henschen|first1=Doug|title=Teradata Acquires eCircle For Social, Mobile Marketing Push|url=http://www.informationweek.com/software/information-management/teradata-acquires-ecircle-for-social-mobile-marketing-push/d/d-id/1104164?|accessdate=27 July 2015|publisher=InformationWeek|date=2 May 2012}}&lt;/ref&gt;
*2014: Teradata acquires Revelytix, a provider of information management products for big data, and Hadapt, a company that developed a platform integrating SQL with Apache Hadoop.&lt;ref&gt;{{cite news|last1=Kanaracus|first1=Chris|title=Teradata acquires assets of Hadapt, Revelytix for big data|url=http://www.pcworld.com/article/2456860/teradata-acquires-assets-of-hadapt-revelytix-for-big-data.html|accessdate=27 July 2015|publisher=PCWorld|date=22 July 2014}}&lt;/ref&gt;
*2014: Teradata acquires [[RainStor|Rainstor]], a company specializing in online big data archiving on Hadoop.&lt;ref&gt;{{cite news|last1=Deutscher|first1=Maria|title=Teradata acquires RainStor to super-charge Hadoop archiving|url=http://siliconangle.com/blog/2014/12/17/teradata-acquired-rainstor-to-super-charge-hadoop-archiving/|accessdate=27 July 2015|publisher=SiliconAngle|date=17 December 2014}}&lt;/ref&gt;
*2015: Teradata acquires Appoxxee, a mobile marketing Software-as-a-Service provider.&lt;ref&gt;{{cite news|last1=Lunden|first1=Ingrid|title=Teradata Buys App Marketing Platform Appoxee For $20-25M, Sets Up Israeli R&amp;D Center|url=http://techcrunch.com/2015/01/13/teradata-buys-app-push-marketing-platform-appoxee-for-20-25m-sets-up-israeli-rd-center/|accessdate=27 July 2015|publisher=TechCrunch|date=3 January 2015}}&lt;/ref&gt;

==Technology and products==
Teradata is a [[Massively parallel (computing)|massively parallel processing]] system running a [[shared-nothing architecture]].&lt;ref&gt;http://searchdatamanagement.techtarget.com/answer/How-to-select-an-MPP-database-DB2-vs-Teradata&lt;/ref&gt; Its technology consists of [[computer hardware|hardware]], [[software]], database, and consulting. The system moves data to a [[data warehouse]] where it can be recalled and analyzed.&lt;ref name=&quot;Pereira&quot;/&gt;

The systems can be used as back-up for one another during downtime, and in normal operation balance the work load across themselves.&lt;ref&gt;{{cite web|last=Howard|first=Philip|title=Dual Loading for Teradata|url=http://www.businesscomputingworld.co.uk/dual-loading-for-teradata/|date=September 8, 2010|publisher=BCW IT Leadership|accessdate=December 13, 2011}}&lt;/ref&gt;

In 2009, Forrester Research issued a report, &quot;The Forrester Wave: Enterprise Data Warehouse Platform,&quot; by James Kobielus,&lt;ref name=&quot;Kobielus&quot;&gt;[http://www.teradata.com/t/WorkArea/DownloadAsset.aspx?id=10115 &quot;The Forrester Wave: Enterprise Data Warehouse Platform,&quot;] by James Kobielus, February 6, 2009.&lt;/ref&gt; rating Teradata the industry's number one enterprise data warehouse platform in the &quot;Current Offering&quot; category.

Marketing research company [[Gartner Group]] placed Teradata in the &quot;leaders quadrant&quot; in its 2009, 2010, and 2012 reports, &quot;Magic Quadrant for Data Warehouse Database Management Systems&quot;.&lt;ref name=&quot;Gartner&quot;&gt;{{Cite news |title= Magic Quadrant for Data Warehouse Database Management Systems |author= Donald Feinberg, Mark A. Beyer |publisher= Gartner Group |date= January 28, 2011 |url= http://www.gartner.com/technology/media-products/reprints/teradata/vol3/article1/article1.html |accessdate= October 25, 2011 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Magic Quadrant for Data Warehouse Database Management Systems|url=http://www.gartner.com/technology/reprints.do?id=1-196T8S5&amp;ct=120207&amp;st=sb|date=February 6, 2012|publisher=Gartner Group|accessdate=February 29, 2012}}&lt;/ref&gt;

Teradata is the most popular data warehouse DBMS in the DB-Engines database ranking.&lt;ref&gt;{{cite web|title=DB-Engines Ranking of database management systems|url=http://db-engines.com/en/ranking|accessdate=April 11, 2013}}&lt;/ref&gt;

In 2010, Teradata was listed in [[Fortune Magazine|Fortune’s]] annual list of Most Admired Companies.&lt;ref&gt;{{cite news |url=http://money.cnn.com/magazines/fortune/mostadmired/2010/snapshots/11465.html |title=WORLD'S MOST ADMIRED COMPANIES |work=Fortune |accessdate=October 27, 2010}}&lt;/ref&gt;

===Active enterprise data warehouse===
Teradata Active Enterprise Data Warehouse is the platform that runs the Teradata Database, with added data management tools and [[data mining]] software.

The data warehouse differentiates between “hot and cold” data – meaning that the warehouse puts data that is not often used in a slower storage section.&lt;ref&gt;{{cite web|last=Whitehorn|first=Mark|title=What your database needs is a good thermometer.|url=http://www.theregister.co.uk/2009/09/14/hot_and_cold_data/|date=September 14, 2009|work=The Register|accessdate=December 13, 2011}}&lt;/ref&gt; As of October 2010, Teradata uses Xeon 5600 processors for the server nodes.&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata pumps data warehouses with six-core Xeons|url=http://www.theregister.co.uk/2010/10/25/teradata_appliance_refresh/|date=October 25, 2010|work=The Register|accessdate=December 13, 2011}}&lt;/ref&gt;

Teradata Database 13.10 was announced in 2010 as the company’s database software for storing and processing data.&lt;ref&gt;Dignan, Larry. “Teradata rolls out latest database, pushes time aware analysis.” ZDnet. October 25, 2010. http://www.zdnet.com/blog/btl/teradata-rolls-out-latest-database-pushes-time-aware-analysis/40865&lt;/ref&gt;&lt;ref&gt;Vizard, Mike. [http://www.ctoedge.com/content/teradata-extends-analytics-engine “Teradata Extends Analytics Engine.”] CTOEdge. October 25, 2010.&lt;/ref&gt;

Teradata Database 14 was sold as the upgrade to 13.10 in 2011 and runs multiple data warehouse workloads at the same time.&lt;ref&gt;Russom, Philip. [http://tdwi.org/blogs/philip-russom/2011/09/big-data-analytics-news-from-teradata.aspx “Big Data Analytics:  The News from Teradata.”] TDWI blog. September 22, 2011.&lt;/ref&gt; It includes column-store analyses.&lt;ref&gt;Henschen, Doug. [http://www.informationweek.com/news/software/bi/231601992?pgno=1 “Teradata Upgrades Break Down Database Barriers.”] ''InformationWeek''. September 22, 2011.&lt;/ref&gt;

Teradata Integrated Analytics is a set of tools for data analysis that resides inside the data warehouse.&lt;ref&gt;Winter, Richard. InformationWeek. September 18, 2010. [http://www.informationweek.com/news/software/info_management/227500132 “Research: State of Enterprise Databases.”]&lt;/ref&gt;

===Backup, archive, and restore===
BAR is Teradata’s backup and recovery system.&lt;ref&gt;Teradata brochure. [http://www.teradata.com/brochures/Teradata-Backup-Archive-Restore-eb5737/ “Teradata Backup Archive Restore.”]&lt;/ref&gt;

The Teradata Disaster Recovery Solution is automation and tools for data recovery and archiving. Customer data can be stored in an offsite recovery center.&lt;ref&gt;Fratto, Mike. [http://www.networkcomputing.com/backup-recovery/229503239?pgno=1 “Teradata Disaster Recovery Solution Helps Reduce the Panic of Catastrophic Events.”] Network Computing. June 29, 2009.&lt;/ref&gt;

===Platform family ===
Teradata Platform Family is a set of products that include the Teradata Data Warehouse, Database, and a set of analytic tools. The platform family is marketed as smaller and less expensive than the other Teradata solutions.&lt;ref&gt;IT Reseller Magazine. [http://www.itrportal.com/absolutenm/templates/article-storage.aspx?articleid=4911&amp;zoneid=21 “Teradata announces new family of powerful analytic platforms.”]&lt;/ref&gt;

===Acquisitions===
{| class=&quot;wikitable&quot;
|-
! Acquisition date
! Company
! Valuation in millions USD
! Purpose
! References
|-
| July 17, 2014
| Hadapt
| 50
| Big Data, SQL-on-Hadoop
|&lt;ref name=&quot;Guess&quot;&gt;{{cite web|last=Keohane|first=Dennis|title=Teradata Has Acquired Revelytix and Hadapt|url=http://betaboston.com/news/2014/07/16/source-hadapt-acquired-by-teradata-will-add-lead-to-more-employees-in-boston/|date=July 16, 2014|publisher=betaboston.com|accessdate=July 16, 2014}}&lt;/ref&gt;
|-
| July 16, 2014
| Revelytix
| 
| Big Data, Metadata
|&lt;ref name=&quot;Guess&quot;/&gt;
|-
| May 2, 2012
| [[eCircle]]
|
| Marketing Tools
|
|-
| March 3, 2011
| [[Aster Data Systems]]
| 263
| MapReduce, Big Data
|&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata Snaps Up Aster Data for $263m|url=http://www.theregister.co.uk/2011/03/03/teradata_buys_aster_data/|date=March 3, 2011|publisher=Channel Register|accessdate=December 13, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Vizard|first=Mike|title=Diversity Comes to Data Management|url=http://www.itbusinessedge.com/cm/blogs/vizard/diversity-comes-to-data-management/?cs=48670|date=September 22, 2011|publisher=IT Business Edge|accessdate=December 13, 2011}}&lt;/ref&gt;
|-
| December 22, 2010
| [[Aprimo]]
| 550
| Marketing tools
|&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata Eats Aprimo for $550m|url=http://www.channelregister.co.uk/2010/12/22/teradata_buys_aprimo/ |date= December 22, 2010 |publisher= Channel Register |accessdate= December 13, 2011 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Henschen|first=Doug|title=IT and Marketing: How Digital Media’s Changing the Relationship |url= http://www.informationweek.com/news/software/bi/229400641?pgno=1 |date= April 9, 2011|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;
|-
| August 10, 2010
| [[Kickfire]]
|
|
|&lt;ref&gt;{{cite web|last=Kanaracus|first=Chris|title=Teradata buys analytics vendor Kickfire|url=http://www.infoworld.com/d/the-industry-standard/teradata-buys-analytics-vendor-kickfire-496|date=August 10, 2010|publisher=Info World|accessdate=December 13, 2011}}&lt;/ref&gt;
|-
| March 20, 2008
| Claraview
|
|
|&lt;ref&gt;{{cite web|last=Grimes|first=Seth|title=Teradata Has Acquired BI/DW Firm Claraview|url=http://www.informationweek.com/blog/228900615|date=March 20, 2008|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;
|-
| November 30, 2005 under NCR
| DecisionPoint
|
| Sourcing and integration of data
|&lt;ref&gt;{{cite news |title=Teradata Acquires DecisionPoint(R) Software; Acquisition Extends Teradata's Financial Management Portfolio |work= Press release |url= http://investor.ncr.com/phoenix.zhtml?c=83840&amp;p=irol-newsArticle&amp;ID=791311 |date= November 30, 2005 |accessdate= December 13, 2011}}&lt;/ref&gt;
|-
| July 14, 2000 under NCR
| Stirling Douglas Group
|
| Demand chain management
|&lt;ref name=&quot;stirling&quot; /&gt;
|}

==Partners==
Below is a partial list of Teradata partners.

*[[TIBCO Spotfire]] &lt;ref name=&quot;Browse Our Partners&quot;&gt;{{cite web|title=Browse Our Partners|url=http://www.teradata.com/templates/Partners/BrowsePartners.aspx|publisher=Teradata |accessdate=April 26, 2013}}&lt;/ref&gt;
*[[Capgemini]]
*[[Cloudera]]&lt;ref name=&quot;Browse Our Partners&quot;/&gt;
*[[IBM]]
*[[Informatica]]: Dual Load solution&lt;ref&gt;{{cite web|last=Howard|first=Philip|title=Dual Loading for Teradata|url=http://www.businesscomputingworld.co.uk/dual-loading-for-teradata/|date=September 8, 2010|publisher=BCW|accessdate=December 13, 2011}}&lt;/ref&gt;
*[[Intel]]
*[[Kalido]]: Teradata and Kalido Accelerate product&lt;ref&gt;{{cite press release |title=Kalido and Teradata Team to Deliver an Agile Data Warehouse for the Mid-Market|url=http://www.kalido.com/7d0a8eed-dab8-4c38-a6b7-e657276c9b91/news-and-events-press-center-press-releases-detail.htm|publisher=Kalido |date=October 25, 2010|accessdate=December 13, 2011}}&lt;/ref&gt;
*[[MapR]]
*[[Microsoft]]
*[[MicroStrategy]]: joint business intelligence products&lt;ref&gt;{{cite web|title=Solutions: MicroStrategy and Teradata|url=http://www.microstrategy.com/bi-applications/bydatasource/teradata/index.asp|work=MicroStrategy website|accessdate=December 13, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite press release|title=MicroStrategy Announces Teradata Support for Business Intelligence Cloud|date=January 24, 2012|url=http://www.microstrategy.com/about-us/press/release/?ctry=167&amp;id=2402|publisher=MicroStrategy}}&lt;/ref&gt;
*[[NetApp]]
*[[Oracle Corporation|Oracle]]
*[[SAP AG|SAP]]: [[Netweaver]] datawarehouse&lt;ref&gt;{{cite web|last=Grant|first=Ian|title=Users welcome integration of Netweaver and Teradata|url=http://www.computerweekly.com/Articles/2009/04/27/235806/Users-welcome-integration-of-Netweaver-and-Teradata.htm|date=April 27, 2009|work=Computer Weekly|accessdate=December 13, 2011}}&lt;/ref&gt;
*[[SAS (software)|SAS]]
*[[Symantec]]&lt;ref name=&quot;:0&quot;&gt;{{cite web|title=Browse Our Partners|url=http://www.teradata.com/templates/Partners/BrowsePartners.aspx|publisher=Teradata |accessdate=December 13, 2011}}&lt;/ref&gt;
*[[Tableau Software]]: interactive data visualization products focused on business intelligence&lt;ref name=&quot;:0&quot; /&gt;
*[[Attunity]]&lt;ref&gt;{{cite web|title=Attunity|url=http://www.attunity.com}}&lt;/ref&gt;
*[[WhereScape]]: data warehouse development and management tools

===Teradata Partners conference===
Teradata holds an annual user group conference and [[Trade fair|expo]] known as ''Teradata Partners'' with keynote industry speakers, educational sessions led by customers and other vendors.&lt;ref&gt;{{cite web|url=http://www.teradata-partners.com/|title=Teradata PARTNERS User Group}}&lt;/ref&gt; The Teradata Partners Conference has been an annual event since 1985.&lt;ref&gt;{{cite web|title=Learning how small fish can eat big fish|url=http://english.peopledaily.com.cn/90001/90778/90858/90864/7199036.html|date=November 15, 2010|work=Chin noa Daily|accessdate=December 13, 2011}}&lt;/ref&gt; The conference involves lectures and speeches on technical and business topics and announcements about new products.&lt;ref&gt;{{cite web|last=Wise|first=Lyndsay|title=Wrapping Up at Teradata Partners User Group Conference|url=http://www.dashboardinsight.com/news/news-articles/wrapping-up-at-teradata-partners-user-group-conference.aspx|date=October 22, 2009|publisher=Dashboard Insight|accessdate=December 13, 2011}}&lt;/ref&gt;

==Big data==
Teradata began to associate itself with the term “[[Big Data]]” in 2010. CTO Stephen Brobst attributes the rise of big data to “new media sources, such as [[social media]].”&lt;ref&gt;{{cite web|last=Salah-Ahmed|first=Amira|title=One-on-One with Teradata’s CTO Stephen Brobst|url=http://thedailynewsegypt.com/it-a-telecom/one-on-one-with-teradatas-cio-stephen-brobst.html|date=May 4, 2011|work=The Daily Egypt News|accessdate=December 13, 2011}}&lt;/ref&gt; The increase in semi-structured and unstructured data gathered from online interactions prompted Teradata to form the “Petabyte club” in 2011 for its heaviest big data users.&lt;ref name=&quot;Grant&quot;&gt;{{cite web|last=Grant|first=Ian|title=Big data boosts Teradata growth|url=http://www.computerweekly.com/Articles/2011/04/11/246287/Big-data-boosts-Teradata-growth.htm|date=April 11, 2011|work=Computer Weekly|accessdate=December 13, 2011}}&lt;/ref&gt;

The rise of big data resulted in many traditional data warehousing companies updating their products and technology.&lt;ref&gt;{{cite web|last=Miniman|first=Stuart|title=The Emerging Big Data Vendor Ecosystem|url=http://wikibon.org/wiki/v/The_Emerging_Big_Data_Vendor_Ecosystem|date=March 28, 2011|publisher=Wikibon|accessdate=December 13, 2011}}&lt;/ref&gt; For Teradata, big data prompted the acquisition of [[Aster Data Systems]] in 2011 for the company’s [[MapReduce]] capabilities and ability to store and analyze semi-structured data.&lt;ref&gt;{{cite web|last=Kanaracus|first=Chris|title=Teradata Buys Aster Data, Boosts &quot;Big Data&quot; Wares|url=http://www.cio.com/article/672663/Teradata_Buys_Aster_Data_Boosts_big_Data_Wares|date=March 3, 2011|publisher=CIO|accessdate=December 13, 2011}}&lt;/ref&gt;

Public interest in big data resulted in a 13% increase in Teradata’s global sales.&lt;ref name=&quot;Grant&quot;/&gt;

==Competition==
Teradata's main competitors are similar products from vendors such as [[Oracle database|Oracle]], [[IBM DB2|IBM]], [[Microsoft SQL Server|Microsoft]] and [[Sybase IQ]]. Also, competitors include data warehouse appliance vendors such as [[Netezza]]&lt;ref&gt;[http://www.dbms2.com/2008/09/15/teradata-data-warehouse-appliance/ Teradata decides to compete head-on as a data warehouse appliance vendor]&lt;/ref&gt; (acquired in November 2010 by [[IBM]]), [[DATAllegro]] (acquired in August 2008 by [[Microsoft]]), [[ParAccel]], [[Pivotal Greenplum Database]], and [[Vertica|Vertica Systems]] (acquired in February 2011 by [[HP]]), and from packaged data warehouse applications such as [[SAP BW]] and [[Kalido]].

==References==
{{Reflist|colwidth=30em}}

==External links==
*[https://www.teradata.com Company Home page]

{{NCR Corp}}
{{AT&amp;T Spinoffs}}

[[Category:Companies listed on the New York Stock Exchange]]
[[Category:Software companies based in Ohio]]
[[Category:Data warehousing products]]
[[Category:Teradata]]
[[Category:Companies based in Dayton, Ohio]]
[[Category:NCR Corporation]]
[[Category:Big data]]</text>
      <sha1>2sk1bqkvwt9d4vxu880b4nrppivl03l</sha1>
    </revision>
  </page>
  <page>
    <title>Business analytics</title>
    <ns>0</ns>
    <id>10147369</id>
    <revision>
      <id>673634135</id>
      <parentid>666338941</parentid>
      <timestamp>2015-07-29T13:37:00Z</timestamp>
      <contributor>
        <username>Rasaxen</username>
        <id>25883531</id>
      </contributor>
      <minor/>
      <comment>/* Further reading */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11087">{{Distinguish|Business analysis}}
{{Refimprove|date=October 2010}}
'''Business analytics''' ('''BA''') refers to the skills, technologies, practices for continuous iterative exploration and investigation of past business performance to gain insight and drive business planning.&lt;ref&gt;{{cite web 
 |title= Next Generation Business Analytics
 |url= http://www.docstoc.com/docs/7486045/Next-Generation-Business-Analytics-Presentation 
 |last= Beller
 |first= Michael J.
 |author2=Alan Barnett
 |date= 2009-06-18 
 |publisher= Lightship Partners LLC 
 |accessdate=2009-06-20
}}&lt;/ref&gt;  Business analytics focuses on developing new insights and understanding of business performance based on [[data]] and [[Statistics|statistical methods]]. In contrast, [[business intelligence]] traditionally focuses on using a consistent set of metrics to both measure past performance and guide business planning, which is also based on data and statistical methods.

Business [[analytics]] makes extensive use of [[Statistics|statistical]] analysis, including explanatory and [[predictive modeling]],&lt;ref&gt;{{cite web| url=http://www.citi.uconn.edu/cist07/5c.pdf |title = Predictive vs. Explanatory Modeling in IS Research| author = Galit Schmueli and Otto Koppius }}&lt;/ref&gt; and fact-based management to drive [[decision making]]. It is therefore closely related to [[management science]]. Analytics may be used as input for human decisions or may drive fully automated decisions. Business intelligence is [[Information retrieval|querying]], [[report]]ing, [[online analytical processing]] (OLAP), and &quot;alerts.&quot;

In other words, querying, reporting, OLAP, and alert tools can answer questions such as what happened, how many, how often, where the problem is, and what actions are needed. Business analytics can answer questions like why is this happening, what if these trends continue, what will happen next (that is, predict), what is the best that can happen (that is, optimize).&lt;ref name=Davenport2007&gt;{{Cite book
 | last1 = Davenport | first1 = Thomas H.
 | last2 = Harris | first2 = Jeanne G.
 | year = 2007
 | title = Competing on analytics : the new science of winning
 | isbn = 978-1-4221-0332-6
 | publisher = Harvard Business School Press
 | location = Boston, Mass.
}}&lt;/ref&gt;

==Examples of application==
Banks, such as [[Capital One]], use [[data analysis]] (or ''[[analytics]]'', as it is also called in the business setting),  to differentiate among customers based on [[credit risk]], usage and other characteristics and then to match customer characteristics with appropriate product offerings. [[Harrah's Entertainment|Harrah’s]], the gaming firm, uses analytics in its [[customer loyalty]] programs. [[E &amp; J Gallo Winery]] quantitatively analyzes and predicts the appeal of its wines. Between 2002 and 2005, [[Deere &amp; Company]] saved more than $1 billion by employing a new analytical tool to better optimize inventory.&lt;ref name=&quot;Davenport2007&quot;/&gt;
Example : It can help you focus on the fundamental objectives of the business and the ways analytics can serve them. A telecoms company that pursues efficient call centre usage over customer service might save money.

==Types of analytics==
&lt;!-- Need to expand descriptions in this section --&gt;
* Decisive analytics: supports human decisions with visual analytics the user models to reflect reasoning.&lt;ref&gt;{{cite web|title=Analytics List|url=http://www.rhbs.me/about-rh/|accessdate=3 April 2015}}&lt;/ref&gt;
* Descriptive Analytics: Gain insight from historical data with [[reporting (disambiguation)|reporting]], scorecards, [[Cluster analysis|clustering]] etc.
* [[Predictive analytics]] (predictive modeling using statistical and [[machine learning]] techniques)
* [[Prescriptive analytics]] recommend decisions using optimization, simulation etc.

==Basic domains within analytics==
&lt;!-- Need to expand descriptions in this section --&gt;
* [[Behavioral analytics]]
* [[Cohort Analysis]]
* Collections analytics
* Contextual data modeling - supports the human reasoning that occurs after viewing &quot;executive dashboards&quot; or any other visual analytics
* [[Financial service]]s analytics
* [[Fraud]] analytics
* [[Marketing]] analytics
* [[Pricing]] analytics 
* [[Retail sales]] analytics
* [[credit risk|Risk &amp; Credit]] analytics
* [[Supply Chain]] analytics
* Talent analytics
* [[Telecommunications]]
* [[Transportation]] analytics

==History==
Analytics have been used in business since the management exercises were put into place by [[Frederick Winslow Taylor]] in the late 19th century. [[Henry Ford]] measured the time of each component in his newly established assembly line. But analytics began to command more attention in the late 1960s when computers were used in [[decision support systems]]. Since then, analytics have changed and formed with the development of [[enterprise resource planning]] (ERP) systems, [[data warehouses]], and a large number of other software tools and processes.&lt;ref name=&quot;Davenport2007&quot;/&gt;

In later years the business analytics have exploded with the introduction to computers. This change has brought analytics to a whole new level and has made the possibilities endless. As far as analytics has come in history, and what the current field of analytics is today many people would never think that analytics started in the early 1900s with Mr. Ford himself.

==Challenges==
Business analytics depends on sufficient volumes of high quality data. The difficulty in ensuring data quality is integrating and reconciling data across different systems, and then deciding what subsets of data to make available.&lt;ref name=&quot;Davenport2007&quot;/&gt;

Previously, analytics was considered a type of after-the-fact method of [[forecasting]] [[consumer behavior]] by examining the number of units sold in the last quarter or the last year. This type of data warehousing required a lot more storage space than it did speed. Now business analytics is becoming a tool that can influence the outcome of customer interactions.&lt;ref&gt;{{cite web|url= http://content.dell.com/us/en/enterprise/d/large-business/best-storage-business-analytic.aspx |title= Choosing the Best Storage for Business Analytics|publisher=Dell.com | accessdate=2012-06-25}}&lt;/ref&gt; When a specific customer type is considering a purchase, an analytics-enabled enterprise can modify the sales pitch to appeal to that consumer. This means the storage space for all that data must react extremely fast to provide the necessary data in real-time.

==Competing on analytics==
[[Thomas H. Davenport|Thomas Davenport]], professor of information technology and management at [[Babson College]] argues that businesses can optimize a distinct business capability via analytics and thus better compete. He identifies these characteristics of an organization that are apt to compete on analytics:&lt;ref name=&quot;Davenport2007&quot;/&gt;
* One or more senior executives who strongly advocate fact-based decision making and, specifically, analytics
* Widespread use of not only [[descriptive statistics]], but also predictive modeling and complex [[optimization (mathematics)|optimization]] techniques
* Substantial use of analytics across multiple business functions or processes
* Movement toward an enterprise level approach to managing analytical tools, data, and organizational skills and capabilities

== See also ==
*[[Analytics]]
*[[Business analysis]]
*[[Business analyst]]
*[[Business intelligence]]
*[[Business process discovery]]
*[[Customer dynamics]]
*[[Data mining]]
*[[OLAP]]
*[[Statistics]]
*[[Test and learn]]

== References ==
{{reflist}}

== Further reading ==

* {{cite book |last=Bartlett|first=Randy |title=A Practitioner’s Guide To Business Analytics: Using Data Analysis Tools to Improve Your Organization’s Decision Making and Strategy |date=February 2013|publisher=McGraw-Hill |location= |isbn=978-0071807593}}
* {{cite book |last=Saxena |first=Rahul |author2=Anand Srinivasan |title= Business Analytics: A Practitioner's Guide (International Series in Operations Research &amp; Management Science) |date=December 2012  |publisher=Springer |location= |isbn= 978-1461460794}}
* {{cite book |last=Davenport |first=Thomas H. |authorlink=Thomas H. Davenport |author2=Jeanne G. Harris |title=Competing on Analytics: The New Science of Winning |date=March 2007 |publisher=Harvard Business School Press |location= |isbn= }}
* {{cite book |last=McDonald |first=Mark |author2=Tina Nunno |title=Creating Enterprise Leverage: The 2007 CIO Agenda |date=February 2007  |publisher=Gartner, Inc. |location=Stamford, CT |isbn= }}
* {{cite book |last=Stubbs |first=Evan |title=The Value of Business Analytics |date=July 2011 |publisher=John Wiley &amp; Sons |location= |isbn=}}
* {{cite book |last=Ranadive |first=Vivek |title=The Power to Predict: How Real Time Businesses Anticipate Customer Needs, Create Opportunities, and Beat the Competition |date=2006-01-26 |publisher=McGraw-Hill |location= |isbn= }}
* {{cite book |last=Zabin |first=Jeffrey |author2=Gresh Brebach |title=Precision Marketing |date=February 2004 |publisher=John Wiley |location= |isbn= }}
* {{cite journal |last=Baker |first=Stephen |date=January 23, 2006 |title=Math Will Rock Your World |journal=BusinessWeek |volume= |issue= |pages= |id= |url=http://www.businessweek.com/print/magazine/content/06_04/b3968001.htm?chan=gl |accessdate=2007-09-19 |quote= }}
* {{cite journal |last=Davenport |first=Thomas H. |date=January 1, 2006 |title=Competing on Analytics |journal=Harvard Business Review |volume= |issue= |pages= |id= |url= |accessdate= |quote= }}
* {{cite journal |last=Pfeffer |first=Jeffrey |authorlink=Jeffrey Pfeffer |author2=[[Robert I. Sutton]] |date=January 2006 |title=Evidence-Based Management |journal=Harvard Business Review |volume= |issue= |pages= |id= |url= |accessdate= |quote= }}
* {{cite journal |last=Davenport |first=Thomas H. |author2=Jeanne G. Harris |date=Summer 2005 |title=Automated Decision Making Comes of Age |journal=MIT Sloan Management Review |volume= |issue= |pages= |id= |url= |accessdate= |quote= }}
* {{cite book |last=Lewis |first=Michael |title=Moneyball: The Art of Winning an Unfair Game |date=April 2004 |publisher=W.W. Norton &amp; Co. |location= |isbn= }}
* {{cite journal |last=Bonabeau |first=Eric |date=May 2003 |title=Don’t Trust Your Gut |journal=Harvard Business Review |volume= |issue= |pages= |id= |url= |accessdate= |quote= }}
* {{cite journal |last=Davenport |first=Thomas H. |author2=Jeanne G. Harris |author3=David W. De Long |author4=Alvin L. Jacobson  |title=Data to Knowledge to Results: Building an Analytic Capability |journal=California Management Review |volume=43 |issue=2 |pages=117–138 |id= |url= |accessdate= |quote= |doi=10.2307/41166078}}

* Indian Bank Need Business Analysis Executives | [http://imarticus.org/programs/business-analysis-certified-professional/ Business Analytics Course]


{{DEFAULTSORT:Business Analytics}}
[[Category:Business terms]]
[[Category:Data warehousing]]
[[Category:Applied data mining]]
[[Category:Business intelligence]]
[[Category:Management science]]
[[Category:Big data|analytics]]

[[de:Business Analytics]]</text>
      <sha1>ods69i7bd8jccyrggvekcqk7pzlduhg</sha1>
    </revision>
  </page>
  <page>
    <title>Hue (Hadoop)</title>
    <ns>0</ns>
    <id>40564013</id>
    <revision>
      <id>672867267</id>
      <parentid>632403269</parentid>
      <timestamp>2015-07-24T11:35:41Z</timestamp>
      <contributor>
        <ip>61.8.146.66</ip>
      </contributor>
      <comment>/* Interface */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2015">{{multiple issues|
{{notability|Products|date=September 2013}}
{{refimprove|date=September 2013}}
}}

{{Infobox software
| name                   = Cloudera Hue
| status                 = Active
| programming language   = [[Python (programming language)|Python]]
| genre                  = Web platform
| license                = [[Apache License]] 2.0
| website                = {{URL|http://gethue.com/}}
| logo                   = [[File:Hue official logo.png|frameless|Hue Logo]]
| latest release version = 3.7.0
| latest release date    = {{Start date and age|2014|10|09}}
}}

'''Hue''' is an [[open source|open-source]] Web interface that supports [[Apache Hadoop]] and its ecosystem, licensed under the Apache v2 license.&lt;ref name=&quot;Hue license&quot;&gt;{{cite web|url=https://github.com/cloudera/hue#license|title=Apache v2 License}}&lt;/ref&gt;

== Features ==

Hue aggregates the most common [[Apache Hadoop]] components into a single interface and targets the user experience. Its main goal is to have the users &quot;just use&quot; Hadoop without worrying about the underlying complexity or using a command line.

== Applications ==

Hadoop
* File Browser for [[HDFS]]
* Job Browser for [[MapReduce]]/YARN
* [[Apache HBase]] Browser
* Query editors for [[Apache Hive]], [[Apache Pig]], Impala
* Apache Sqoop2 editor
* Apache Oozie editor and dashboard
* An interactive [[Apache Solr]] dashboard builder
* [[Apache ZooKeeper]] Browser

Generic
* Visual search dashboard for [[Apache Solr]]
* Spark Editor
* SQL Editor for traditional databases

== Distribution ==

The Hue team works with upstream [[Apache Hadoop]] and provides Hue releases on its website.&lt;ref name=&quot;Hue releases&quot;&gt;{{cite web|url=http://gethue.com/category/release/|title=Hue releases}}&lt;/ref&gt; Hue is also present in some major Hadoop distributions (CDH, HDP, BigTop) and demo VM.

=== Striling software===
[[File:Hue_3.6_interface.png|thumb|thumb|center|upright=4|Hue interface]]
103-66209

==References==
{{Reflist|2}}

[[Category:Hadoop]]
[[Category:Big data]]</text>
      <sha1>ncswtdz5emb8lpah93r495bq4w07m6c</sha1>
    </revision>
  </page>
  <page>
    <title>Manuel Aparicio</title>
    <ns>0</ns>
    <id>35815630</id>
    <revision>
      <id>630904961</id>
      <parentid>630778349</parentid>
      <timestamp>2014-10-24T08:14:03Z</timestamp>
      <contributor>
        <username>Postcard Cathy</username>
        <id>1744116</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1517">{{multiple issues|
{{BLP sources|date=June 2014}}
{{notability|date=June 2014}}
}}

Dr. '''Manuel Aparicio''' is a recognized expert in the field of Associative Memories and [[big data analytics]], holding several patents for [[associative memory base]] technology {{US patent|7,908,438}}

Aparicio co-founded [[Saffron Technology, Inc.]] a company specializing in [[Associative Memory Base]] Technology and he serves as its Chief Executive Officer.&lt;ref&gt;http://investing.businessweek.com/research/stocks/private/person.asp?personId=627899&amp;privcapId=99166&amp;previousCapId=99166&amp;previousTitle=Saffron%20Technology,%20Inc.&lt;/ref&gt; Aparicio served as Chief Scientist of IBM Knowledge Management and Intelligent Agent Center. He serves as Member of the Advisory Board at Sociocast Networks LLC.

==Footnotes==
{{Reflist}}

==External links==
* [http://www.saffrontech.com/ Saffron Technology]
* [http://www.sociocast.com/ Sociocast Networks LLC]

{{Persondata
| NAME              = Aparicio, Manuel
| ALTERNATIVE NAMES =
| SHORT DESCRIPTION = American businessman
| DATE OF BIRTH     =
| PLACE OF BIRTH    =
| DATE OF DEATH     =
| PLACE OF DEATH    =
}}
{{DEFAULTSORT:Aparicio, Manuel}}
[[Category:Living people]]
[[Category:American technology chief executives]]
[[Category:American computer scientists]]
[[Category:Big data]]
[[Category:American technology company founders]]
[[Category:Year of birth missing (living people)]]
[[Category:IBM employees]]
[[Category:Place of birth missing (living people)]]


{{US-CEO-stub}}</text>
      <sha1>9pnbuyocbqzgnkktqgig3ib5g6u5n1f</sha1>
    </revision>
  </page>
  <page>
    <title>Flutura Decision Sciences and Analytics</title>
    <ns>0</ns>
    <id>40765199</id>
    <revision>
      <id>671697923</id>
      <parentid>671235941</parentid>
      <timestamp>2015-07-16T11:39:47Z</timestamp>
      <contributor>
        <username>Judederick</username>
        <id>23830259</id>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4279">{{Orphan|date=October 2013}}

{{Infobox company
| name             = Flutura Decision Sciences and Analytics    
| logo             =[[Image:Flutura-Logo.png|150px]]
| type             =[[Private company|Private Ltd.]]  
| location_city    ={{Unbulleted list|[[Bangalore]], [[Karnataka]], [[India]]|[[San Jose, California|San Jose]], [[California]], [[USA]]}}
 
| founder          ={{Unbulleted list|Krishnan Raman|Derick Jose|Srikanth Muralidhara}}   
| foundation       =February 2012  
| area_served      =Worldwide
| industry         =[[Machine to Machine]](M2M), [[Internet of Things]], [[Big Data]] [[Analytics]], [[Decision science]]s
| products = {{Unbulleted list|Cerebra}}
| homepage         ={{URL|flutura.com/}}   
| intl             =yes
}}

'''Flutura Decision Sciences and Analytics''' is a Decision Sciences Company that is on a mission to  impact Energy and Asset Outcomes by curating Intelligent Data Products for the Energy and Heavy Engineering industry. It does so by mining IOT Sensor data usings its Data science platform - Cerebra .  Its main offices are located in Houston, USA, Palo Alto, USA and Bengaluru, India. 

Marketwatch ( A Wall Street journal company ) and Bloomberg featured Flutura as one the companies to watch in the Machine intelligence space ( http://www.bloomberg.com/now/2014-12-11/current-state-machine-intelligence/ ). San Francisco based Gigaom also featured Flutura as one of the IOT Analytics pioneers ( http://research.gigaom.com/report/internet-of-things-the-influence-of-m2m-data-on-the-energy-industry/ ). California based technology magazine ''CIO Review'' has recognized Flutura as one of the Top 20 Most Promising Big Data Companies Globally. 

== History ==
Flutura Decision Sciences fills a gap in the marketplace with respect to mining machine data for intelligent patterns which impact industrial outcomes. Flutura means  butterfly in Albanian and symbolizes the most profound transformation in nature which is that of a caterpillar to a butterfly.

== Recognition ==

* Bloomberg : Machine Intelligence Companies to watch in 2015 . Please click http://www.bloomberg.com/now/2014-12-11/current-state-machine-intelligence/ looked at
2,500 startups and selected Flutura”

* Marketwatch ( A Wall Street Journal Subsidiary) features Flutura in Industrial Internet Analytics companies in energy sector . Please click http://www.marketwatch.com/story/big-data-and-analytics-to-drive-real-time-intelligence-from-field-to-enterprise-in-emerging-digital-oilfield-operations-2015-02-24-8203112

* CIO Review magazine featured Flutura among the &quot;20 Most Promising Manufacturing Tech Solution Providers 2015&quot;. Please click http://manufacturing.cioreview.com/vendor/2015/flutura

* The First and only boutique analytics company in Houston to focus on Industrial Internet Analytics / IOT
* The Only company at Distributech to focus on REP ( Retail Energy Provider ) Analytics for Deregulated Energy Markets
* The first company to focus on niche big data use cases in Oil n gas industry
* Siemens TTB : Selected at Siemens TTB event in Palo Alto for its game changing Cerebra Platform
* CAFEET : California-French Energy consortium 
* In October 2013, CIOReview Magazine rated Flutura as one of top 20 promising Big data companies globally &lt;ref&gt;http://www.cioreview.com/magazine/Flutura-Solutions-Help-See-Previously-Unseen-Patterns-OUJL830948901.html &quot;CIOReview&quot;, October 2013&lt;/ref&gt;

== Products ==
To monetize machine generated data , Flutura developed a patent pending platform called '''Cerebra'''. Cerebra does not only harvest signals from machines in real time, it gives the product users absolute visibility of their operations and empowers them with the ability to choose how they want to act in different scenarios. The unique ability of Cerebra to capture important signals from complex machine generated logs and harness previously untapped data makes it the future generation intelligent Big Data platform.

== References ==
{{reflist}}

== External links ==
*[http://www.flutura.com  Official Website]
*[http://utilities.flutura.com  Product Microsite]
*[http://blog.fluturasolutions.com  Blog]

[[Category:Analytics]]
[[Category:Big data]]
[[Category:Machine to machine]]
[[Category:Organisations based in Bangalore]]</text>
      <sha1>r7ix28cqmc5bn07khg4cpbf2zt1gkw7</sha1>
    </revision>
  </page>
  <page>
    <title>Rocket Fuel Inc.</title>
    <ns>0</ns>
    <id>41309184</id>
    <revision>
      <id>667537149</id>
      <parentid>666502240</parentid>
      <timestamp>2015-06-18T19:54:38Z</timestamp>
      <contributor>
        <username>Stesmo</username>
        <id>98915</id>
      </contributor>
      <comment>Removed external links from body of article and pruned EL section to meet [[WP:EL]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3926">{{Infobox company
| name             = Rocket Fuel Inc.
| logo             = [[File:Rocket Fuel Inc. logo.png|220px]]
| traded_as        = {{NASDAQ link|FUEL}}
| industry         = Marketing and Advertising
| foundation       = {{start date|2008}}
| founders          = George John, Richard Frankel, Abhinav Gupta
| location_city    = [[Redwood City, California|Redwood City, CA]]&lt;ref name=one&gt;{{cite news | title = In marketing, embracing a world where data rules | author = Courtney Subramanian | work = CNN Money | date = March 17, 2014 | accessdate = May 1, 2014 | url = http://tech.fortune.cnn.com/2014/03/17/in-marketing-embracing-a-world-where-data-rules/?section=money_topstories&amp;utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed:+rss/money_topstories+(Top+Stories) }}&lt;/ref&gt;
| location_country = US
| locations        = 20
| area_served      = North America, Europe, and Japan
| key_people       = George John, CEO&lt;br&gt; Richard Frankel, President&lt;br&gt; Abhinav Gupta, Senior Vice President of Engineering. 
| num_employees    = 800
| homepage         = {{URL|http://rocketfuel.com/}}
}}

'''Rocket Fuel''' is an ad technology company based in [[Redwood City]], California.&lt;ref name=&quot;Seeking Alpha&quot;&gt;{{cite web|url=http://seekingalpha.com/article/1756932-rocket-fuels-ipo-soars|title=Rocket Fuel's IPO Soars|publisher=Seeking Alpha|author=Sramana Mitra|date=21 October 2013|accessdate=3 December 2013}}&lt;/ref&gt; 
It was founded in 2008 by alumni of [[Yahoo!]].&lt;ref name=&quot;Seeking Alpha&quot; /&gt;

Rocket Fuel completed an [[initial public offering]] in September 2013.&lt;ref&gt;{{cite web|url=http://www.adotas.com/2014/02/rocket-fuel-exec-offers-reasons-for-optimism-about-the-future-of-mobile-advertising/|title=Rocket Fuel Exec Offers Reasons For Optimism About The Future Of Mobile Advertising|work=Adotas|accessdate=9 March 2014}}&lt;/ref&gt; The initial stock offering was $29 per share and reached a high of $59.95 per share when the stock opened. The stock closed at $56.10 per share on opening day, giving the company a value of $942.5 million.&lt;ref&gt;{{Cite web | url = http://www.cnbc.com/id/101203198# | title = Cramer: This stock is literally Rocket Fuel | publisher = ''[[CNBC]]'' | date = 13 November 2013 | accessdate = 13 March 2015 | author = Brodie, Lee }}&lt;/ref&gt; By February 24, 2015, the stock had dropped to $9.63, or 86 percent below its record high and 67 percent below its IPO price.&lt;ref&gt;Kilgore, Tomi (March 6, 2015) [http://www.marketwatch.com/story/11-stock-bubbles-that-have-already-popped-2015-03-04?page=6 &quot;11 stock bubbles that have already popped.&quot;] ''Marketwatch.'' (Retrieved 6-11-2015.)&lt;/ref&gt;

Rocket Fuel acquired the New York-based ad tech company [x+1] in August 2014 for $230 million.&lt;ref&gt;{{citeweb | url = http://www.adexchanger.com/platforms/rocket-fuel-to-buy-x1-for-estimated-230m/|title = Rocket Fuel To Buy [X+1] For Estimated $230M| author=Zach Rogers| work =ad exchanger | date =5 August 2014 | accessdate = 24 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{citeweb | url =http://venturebeat.com/2014/08/05/ad-tech-player-rocket-fuel-buys-programmatic-marketing-company-x1/ |title =Ad-tech player Rocket Fuel buys programmatic marketing company [x+1] | author=Jordan Novet| work = Venture Beat| date =5 August 2014 | accessdate = 24 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{citeweb | url =http://www.adweek.com/news/technology/rocket-fuels-deal-buy-x1-propels-it-marketing-cloud-territory-159317 |title = Rocket Fuel's Deal to Buy [x+1] Propels It Into Marketing Cloud Territory| author=Garett Sloane| work =Adweek | date =5 August 2014 | accessdate = 24 August 2014}}&lt;/ref&gt;

==References==
{{reflist|33em}}

==External links==
*{{Official website|http://www.rocketfuel.com}}

[[Category:Big data]]
[[Category:Technology companies established in 2008]]
[[Category:2008 establishments in California]]
[[Category:Companies based in Redwood City, California]]
[[Category:Companies listed on NASDAQ]]
[[Category:Internet advertising]]</text>
      <sha1>63bmqv1dj5y62cse44msh363otdsvjf</sha1>
    </revision>
  </page>
  <page>
    <title>RelateIQ</title>
    <ns>0</ns>
    <id>39853660</id>
    <revision>
      <id>673271996</id>
      <parentid>671164574</parentid>
      <timestamp>2015-07-27T07:50:22Z</timestamp>
      <contributor>
        <username>Miszatomic</username>
        <id>16119042</id>
      </contributor>
      <comment>+[[Category:Software companies based in California]]; +[[Category:Software companies established in 2011]]; +[[Category:2011 establishments in California]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5231">{{Infobox company
| name             = RelateIQ
| logo             = [[File:RelateIQ logo April 14, 2014.png|200px|RelateIQ]]
| foundation       = 2011
| founder          = Adam Evans, Steve Loughlin
| num_employees    = 128
| location_city    = [[Palo Alto, California]] 
| location_country = [[United States]]
| industry         = [[Customer Relationship Management]]
| products         = [[Software as a service|Cloud-Based]] [[customer relationship management]]
| homepage         = {{URL|http://relateiq.com}}
}}

'''RelateIQ,''' a subsidiary of [[Salesforce.com]], is an American enterprise software company based in [[Palo Alto, California]]. The company's software is a relationship intelligence platform that combines data from email systems, smartphone calls, and appointments to augment or replace standard relationship management tools or database solutions. It scans &quot;about 10,000 emails, calendar entries, and other data points per minute at first run&quot;.&lt;ref&gt;{{cite news|last=Rusli|first=Evelyn|title=Your New Secretary: An Algorithm|url=http://online.wsj.com/article/SB10001424127887323949904578539983425941490.html?KEYWORDS=relateiq|accessdate=7/8/13|newspaper=The Wall Street Journal|date=6/12/13}}&lt;/ref&gt;

In July 2014, salesforce.com announced plans to acquire the company for close to $400 million.&lt;ref&gt;{{cite web|author= |url=http://online.wsj.com/articles/salesforce-com-agrees-to-acquire-relateiq-in-390-million-deal-1405096071/ |title=Salesforce.com Agrees to Acquire RelateIQ for About $390 Million |publisher=Wall Street Journal |date=2014-07-11 |accessdate=2014-08-15}}&lt;/ref&gt; Industry insiders predict that the acquisition will bulk up salesforce.com's data science offering, enabling the world's #1 CRM provider to compete amid rapidly changing enterprise software requirements. One article on the acquisition states, &quot;If RelateIQ fulfills its promises of stopping team members from accidentally emailing the same queries to clients or customers, or allows salespeople to decipher the corporate hierarchies of their potential contacts through algorithms, there could be some lasting changes to the world of sales.&quot;&lt;ref&gt;{{cite web|author= |url=http://www.fastcolabs.com/3033881/linkedins-data-gurus-second-act-at-salesforce/ |title=LinkedIn's Data Guru's Second Act At Salesforce |publisher=Fast Company |date=2014-08-01 |accessdate=2014-08-15}}&lt;/ref&gt;

== Team ==
The company was founded in July 2011 by Adam Evans and Steve Loughlin. At the end of July 2013, the company made headlines by bringing on data scientist [[DJ Patil]] as VP of Product.&lt;ref&gt;{{cite web|author= |url=http://techcrunch.com/2013/07/31/greylocks-data-scientist-in-residence-dj-patil-joins-enterprise-relationship-manager-relateiq-as-vp-of-product/ |title=Greylock’s Data Scientist In Residence DJ Patil Joins Enterprise Relationship Manager RelateIQ As VP Of Product |publisher=TechCrunch |date=2013-07-31 |accessdate=2013-10-02}}&lt;/ref&gt; The leadership team also includes VP of Engineering Ruslan Belkin,&lt;ref&gt;{{cite web|author= |url=http://techcrunch.com/2014/06/25/relateiq-snags-ex-twitter-vp-of-search/ |title=RelateIQ Snags Ex-Twitter VP Of Search |publisher=TechCrunch |date=2014-06-25 |accessdate=2014-08-15}}&lt;/ref&gt; VP of Sales Armando Mann (the recent head of sales at Dropbox and a former Google executive), and VP of Marketing Elise Bergeron (formerly of Facebook).

== Technology ==
Unlike traditional relationship management systems, which rely on data input by users to keep their teams informed and run predictive analytics, RelateIQ's platform automatically isolates and analyzes a user's professional emails and other interactions. In combining this data with information gleaned from other sources, such as LinkedIn and Facebook, RelateIQ leverages data science &quot;to comb through emails, analyze them, and offer reminders and suggestions to busy salespeople.&quot;&lt;ref&gt;{{cite news|last=Novet|first=Jordan|title=Startups fatten up Salesforce.com with analytics and data|url=http://venturebeat.com/2013/12/03/startups-fatten-up-salesforce-com-with-analytics-and-data/|accessdate=12/10/13|newspaper=VentureBeat|date=12/3/13}}&lt;/ref&gt;

In March 2014, the company released &quot;Closest Connections&quot;, a feature that &quot;automates a normally time-consuming—and potentially erroneous—sales process&quot; by identifying warm introductions for potential prospects based on real activity happening within a team's email inboxes, phones, calendars, and social networks.&lt;ref&gt;{{cite news|last=Novet|first=Jordan|title=RelateIQ identifies the best employee to contact each new sales lead|url=http://venturebeat.com/2014/03/22/relateiq-identifies-the-best-employee-to-contact-each-new-sales-lead/|newspaper=VentureBeat|date=2014-03-22}}&lt;/ref&gt;

== Post Acquisition ==
Following the acquisition by [[Salesforce.com]], RelateIQ has stopped support for certain geographies, including the United Kingdom and South Africa.

==References==
{{Reflist}}

[[Category:Big data]]
[[Category:Data management software]]
[[Category:Enterprise software]]
[[Category:Business software]]
[[Category:Software companies based in California]]
[[Category:Software companies established in 2011]]
[[Category:2011 establishments in California]]


{{software-company-stub}}</text>
      <sha1>nn7391uer8s0ewxk9nqulm9dl66q4ca</sha1>
    </revision>
  </page>
  <page>
    <title>AdNear</title>
    <ns>0</ns>
    <id>39585396</id>
    <revision>
      <id>661506172</id>
      <parentid>635015641</parentid>
      <timestamp>2015-05-09T03:50:55Z</timestamp>
      <contributor>
        <username>Midas02</username>
        <id>12205148</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[Magnum]] (link removed) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3788">{{Orphan|date=September 2014}}

{{Infobox company
| name             = AdNear
| logo             = 
| caption          = 
| type             = [[Private company|Private]]
| genre            = 
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = [[Bangalore]], [[India]] ({{Start date|2012}})
| founder          = Anil Mathews
| defunct          = 
| location_city    = 
| location_country = [[Singapore]]
| location         = 
| locations        = [[Singapore]], [[Bangalore]], [[Mumbai]], [[Sydney]], [[Jakarta]], [[USA]]
| area_served      = [[Earth|Global]]
| key_people       = 
| industry         = Big Data
| products         = 
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| aum              = 
| assets           = 
| equity           = 
| owner            = 
| num_employees    = 45
| parent           = 
| divisions        = 
| subsid           = 
| homepage         =  {{URL|www.adnear.com}}
| footnotes        = 
| intl             = 
| Series A         = 6.3M$
| Series B         = 19M$
}}

'''AdNear''' is a [[Privately held company|privately held]] location intelligence platform founded in Nov 2012 having direct presence in [[Singapore]], [[Australia]], [[India]], [[Indonesia]], [[Japan]] and [[USA]]. AdNear is headquartered in Singapore with its subsidiary ('''Imere Technologies''')&lt;ref&gt;{{cite web |url=http://www.adnear.com/about |title=About AdNear at adnear.com}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.linkedin.com/company/2811352?trk=tyah&amp;trkInfo=tas%3Aadnear%2Cidx%3A1-1-1|title=AdNear Linkedin Profile}}&lt;/ref&gt; based in [[Bangalore]]. It is funded by venture capital firms [[Sequoia Capital]],&lt;ref&gt;{{cite web |url=http://www.sequoiacap.com/india/adnear/info |title=Adnear : Sequoia Capital : India}}&lt;/ref&gt; [[Canaan Partners]],&lt;ref&gt;{{cite news |url=http://techcircle.vccircle.com/2012/11/28/location-based-mobile-advertising-platform-adnear-raises-6-3m-from-sequoia-canaan-partners/ |title=AdNear raises $6.3M from Sequoia &amp; Canaan Partners}}&lt;/ref&gt; [[Telstra Ventures]] and Global Brain Corporation.&lt;ref&gt;{{cite news |url=http://www.businesstimes.com.sg/technology/spore-mobile-ad-startup-raises-record-series-b/ |title=AdNear raises $19M from Telstra Ventures, Global Brain &amp; Existing Partners in Series B}}&lt;/ref&gt;

It is the leading location intelligence platform that leverages location and context to power data driven marketing for advertisers. AdNear's data platform is built on top of proprietary technology giving it the strength of location awareness without the need of GPS or operator assistance. This helps AdNear reach a massive 530+ million users in the region we operate.

&lt;ref name=&quot;investvine&quot;&gt;{{cite web|url=http://investvine.com/mobile-advertising-platform-targets-asean/|title=Mobile advertising platform targets ASEAN|first=Todd|last=Watson|work=Inside Investor|date=30 July 2013|accessdate=31 July 2013}}&lt;/ref&gt; AdNear's platform is used by major brands such as [[P&amp;G]], [[Unilever]], [[Google]], [[Intel]], [[Microsoft]], [[Nimbuzz]], [[SONY]], [[Audi]], [[Toyota]], [[Ford]], [[Titan Company|Titan]], [[Pizza Hut]], [[KFC]], [[McDonalds]], [[Red Bull]], [[Samsung]], [[Nokia]], [[Bharti Airtel|Airtel]], [[Vodafone]], [[Amobee]], [[MAC Cosmetics|MAC]], [[Toblerone]], [[Ambi Pur]], [[Starhub]], and Magnum for mobile ad targeting and audience insights.&lt;ref&gt;{{cite news |url=http://www.moneycontrol.com/news/business/adnear-woos-advertiserslocation-based-advertising_884110.html |title=AdNear woos advertisers with location-based advertising}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* [http://www.crunchbase.com/company/adnear AdNear | Crunbase profile]
* [http://www.adnear.com/ AdNear] (company website)

[[Category:Big data]]


{{Tech-company-stub}}</text>
      <sha1>ijrnut8s88qjplrx7svla1pjnmn05bh</sha1>
    </revision>
  </page>
  <page>
    <title>Datafication</title>
    <ns>0</ns>
    <id>41731546</id>
    <revision>
      <id>638433518</id>
      <parentid>592164813</parentid>
      <timestamp>2014-12-17T01:15:08Z</timestamp>
      <contributor>
        <username>Tristan Sterk</username>
        <id>11537274</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1972">'''Datafication''' is a modern technological trend turning many aspects of our life into computerised data &lt;ref name=&quot;CukierMayer-Schoenberger2013&quot;&gt;{{cite journal | last = Cukier | first =Kenneth | last2 = Mayer-Schoenberger | first2 = Viktor  | title =The Rise of Big Data | journal =Foreign Affairs, | issue =May/June | pages = 28-40. | date =2013  | url = http://www.foreignaffairs.com/articles/139104/kenneth-neil-cukier-and-viktor-mayer-schoenberger/the-rise-of-big-data | accessdate = 24 January 2014}}&lt;/ref&gt; and transforming this information into new forms of value. 
&lt;ref name=&quot;SchuttOneil2014&quot;&gt;
{{cite book
 | last = O'Neil | first =Cathy
 | last2 =Schutt
 | first2 = Rachel
| title =Doing Data Science
 | publisher =O’Reilly Media
 | date =2013
 | pages =406
  | isbn =978-1-4493-5865-5
 }}
&lt;/ref&gt;
Examples of datafication as applied to social and communication media are how [[Twitter]] datafies stray thoughts or datafication of [[Human_resource_management|HR]] by [[LinkedIn]] and others.  Alternative examples are diverse and include aspects of the built environment, and design via engineering and or other tools that tie data to formal, functional or other physical media outcomes of which [[Formsolver]]&lt;ref&gt;https://www.formsolver.com&lt;/ref&gt; is an example.

[[File:Shape optimization for buildings by formsolver.jpg|thumbnail|right|Example: Datafication of the skin and form of a building to assist engineers, designers and architects determine the performance of particular building geometries. Example provided courtesy of Formsolver.com ]]
[[File:Emerging Shape Optimization Families for buildings by formsolver.jpg|thumbnail|right|Example: Shape families resulting from differing goals when data is used for the purposes of shape optimization. Example provided courtesy of Formsolver.com]]
==References==
{{Reflist}}
[[Category:Information science]]
[[Category:Technology forecasting]]
[[Category:Data management]]
[[Category:Big data]]
{{Tech-stub}}</text>
      <sha1>dzdgwmdguio6no9q6bv181j0ip8k0tn</sha1>
    </revision>
  </page>
  <page>
    <title>MonetDB</title>
    <ns>0</ns>
    <id>1862647</id>
    <revision>
      <id>671870182</id>
      <parentid>671776237</parentid>
      <timestamp>2015-07-17T15:36:42Z</timestamp>
      <contributor>
        <ip>175.139.112.131</ip>
      </contributor>
      <comment>/* Query Recycling */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23435">{{Infobox software|
  name = MonetDB |
  logo =  [[Image:Monetdb-logo.png|181px|MonetDB logo]] |
  developer = MonetDB Developer Team |
  latest_release_version = Oct2014 |
  latest_release_date = {{release date |2014|1031}}|
  programming language   = [[C (programming language)|C]] |
  operating_system = [[Cross-platform]] |
  genre = [[Column-oriented DBMS]]&lt;br /&gt;[[RDBMS]] |
  license = MonetDB License (based on the [[Mozilla Public License|MPL]] 1.1) |
  website = {{URL |www.monetdb.org}}
}}

'''MonetDB''' is an [[open source]] [[Column-oriented DBMS|column-oriented]] [[database management system]] developed at the [[Centrum Wiskunde &amp; Informatica]] (CWI) in the [[Netherlands]].
It was designed to provide high performance on complex queries against large databases, such as combining [[table (database)|tables]] with hundreds of columns and multi-million rows.
MonetDB has been applied in high-performance applications for [[online analytical processing]] (OLAP), [[data mining]], [[geographic information system|GIS]],&lt;ref name=gis&gt;{{cite web | url = http://www.monetdb.org/Documentation/Extensions/GIS | title = GeoSpatial - MonetDB | date = 4 March 2014}}&lt;/ref&gt; [[Resource Description Framework|RDF]],&lt;ref name=rdf&gt;{{cite web | url = http://lod2.eu/Project/MonetDB.html | title = MonetDB - LOD2 - Creating Knowledge out of Interlined Data | date = 6 March 2014}}&lt;/ref&gt; streaming data processing,&lt;ref name=datacell&gt;{{cite web | url = http://www.monetdb.org/Documentation/Extensions/Streams | title = Streaming - MonetDB | date = 4 March 2014}}&lt;/ref&gt; text retrieval and [[sequence alignment]] processing.&lt;ref name=sambam&gt;{{cite web | url = https://www.monetdb.org/Documentation/Extensions/LifeScience | title = Life Sciences in MonetDB | date = 24 November 2014}}&lt;/ref&gt;

== History ==
[[Image:Monetdb logo.png|thumb|right|The older MonetDB logo]]

Data mining projects in the 1990s required improved analytical database support. This resulted in a [[Centrum Wiskunde &amp; Informatica|CWI]] [[Corporate spin-off|spin-off]] called Data Distilleries, which used early MonetDB implementations in its analytical suite. Data Distilleries eventually became a subsidiary of [[SPSS]] in 2003, which in turn was acquired by [[IBM]] in 2009.&lt;ref name=datacell&gt;{{cite web | url = http://www.monetdb.org/AboutUs | title = A short history about us - MonetDB | date = 6 March 2014}}&lt;/ref&gt;

MonetDB in its current form was first created in 2002 by doctoral student [http://www.cwi.nl/~boncz/ Peter Alexander Boncz] and professor [[Martin L. Kersten]] as part of the 1990s MAGNUM research project at [[University of Amsterdam]].&lt;ref&gt;{{Cite book |title= Monet: A Next-Generation DBMS Kernel For Query-Intensive Applications |work= Ph.D. Thesis |publisher= Universiteit van Amsterdam |date= May 2002 |url= http://oai.cwi.nl/oai/asset/14832/14832A.pdf }}&lt;/ref&gt; It was initially called simply Monet, after the French impressionist painter [[Claude Monet]]. The first version under an [[open-source software]] license (a modified version of the [[Mozilla Public License]]) was released on September 30, 2004. When MonetDB version 4 was released into the open-source domain and many extensions to the code base were added by the MonetDB/CWI team. These included a new SQL frontend, supporting the [[SQL:2003]] standard.&lt;ref name=history&gt;[https://www.monetdb.org/AboutUs MonetDB historic background]&lt;/ref&gt;

MonetDB introduced innovations in all layers of the [[database management system|DBMS]]: a storage model based on vertical fragmentation, a modern [[CPU]]-tuned query execution architecture that often gave MonetDB a speed advantage over the same [[algorithm]] over a typical [[interpreter (computing)|interpreter-based]] [[RDBMS]]. It was one of the first database systems to tune query optimization for [[CPU cache]]s. MonetDB includes automatic and self-tuning indexes, run-time query optimization, and a modular software architecture.&lt;ref&gt;{{Cite journal |author= Stefan Manegold |title= An Empirical Evaluation of XQuery Processors |work= Proceedings of the International Workshop on Performance and Evaluation of Data Management Systems (ExpDB) |publisher= ACM |date= June 2006 |url= http://oai.cwi.nl/oai/asset/19337/19337B.pdf |accessdate= December 11, 2013 |doi= 10.1016/j.is.2007.05.004 }}&lt;/ref&gt;&lt;ref&gt;P. A. Boncz, T. Grust, M. van Keulen, S. Manegold, J. Rittinger, J. Teubner. [http://www.cwi.nl/htbin/ins1/publications?request=pdf&amp;key=BoGrKeMaRiTe:SIGMOD:06 MonetDB/XQuery: A Fast XQuery Processor Powered by a Relational Engine]. In Proceedings of the ACM SIGMOD International Conference on Management of Data, Chicago, IL, USA, June 2006.&lt;/ref&gt;

By 2008, a follow-on project called X100 (MonetDB/X100) started, which evolved into the [[Vectorwise|VectorWise]] technology. VectorWise was acquired by [[Actian Corporation]], integrated with the [[Ingres (database)|Ingres database]] and sold as a commercial product.&lt;ref&gt;{{Cite journal |title= From x100 to vectorwise: opportunities, challenges and things most researchers do not think about |authors=  Marcin Zukowski and Peter Boncz |publisher= ACM |work= Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data |date= May 20, 2012 |pages= 861â€“862 |doi= 10.1145/2213836.2213967 |isbn= 978-1-4503-1247-9 }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | title = Integration of VectorWise with Ingres | authors =  Inkster, D. and Zukowski, M. and Boncz, P. A. |publisher= ACM | work= ACM SIGMOD Record |date= September 20, 2011 | url = http://www.sigmod.org/publications/sigmod-record/1109/pdfs/08.industry.inkster.pdf }}&lt;/ref&gt;

In 2011 a major effort to renovate the MonetDB codebase was started. As part of it, the code for the MonetDB 4 kernel and its XQuery components were frozen. In MonetDB 5, parts of the SQL layer were pushed into the kernel.&lt;ref name=history /&gt; The resulting changes created a difference in internal [[Application programming interface|APIs]], as it transitioned from MonetDB Instruction Language (MIL) to MonetDB Assembly Language (MAL). Older, no-longer maintained top-level query interfaces were also removed. First was [[XQuery]], which relied on MonetDB 4 and was never ported to version 5.&lt;ref&gt;{{cite web | url = https://www.monetdb.org/XQuery/ | title = XQuery | date = 12 December 2014}}&lt;/ref&gt; The experimental [[Jaql]] interface support was removed with the October 2014 release.&lt;ref&gt;{{cite web | url=https://www.monetdb.org/Downloads/ReleaseNotes | title= MonetDB Oct2014 Release | date=12 December 2014}}&lt;/ref&gt;

== Architecture ==

MonetDB architecture is represented in three layers, each with its own set of optimizers.&lt;ref name=monetdb-20&gt;{{Cite journal |author= Idreos, S. and Groffen, F. E. and Nes, N. J. and Manegold, S. and Mullender, K. S. and Kersten, M. L. |title= MonetDB: Two Decades of Research in Column-oriented Database Architectures |work= IEEE Data Engineering Bulletin |publisher= IEEE |date= March 2012 |url= http://oai.cwi.nl/oai/asset/19929/19929B.pdf |accessdate= March 6, 2014 |pages=  40–45}}&lt;/ref&gt;
The front-end is the top layer, providing query interfaces for [[SQL]], SciQL, [[SPARQL]]. Queries are parsed into domain-specific representations, like relational algebra for SQL, and optimized. The generated logical execution plans are then translated into MonetDB Assembly Language (MAL) instructions, which are passed to the next layer. The middle or back-end layer provides a number of cost-based optimizers for the MAL. The bottom layer is the database kernel, which provides access to the data stored in Binary Association Tables (BATs). Each BAT is a table consisting of an Object-identifier and value columns, representing a single column in the database.&lt;ref name=monetdb-20 /&gt;

MonetDB internal data representation also relies on the memory addressing ranges of contemporary CPUs using [[demand paging]] of memory mapped files, and thus departing from traditional DBMS designs involving complex management of large data stores in limited memory.

=== Query Recycling ===
Query recycling is an architecture for reusing the byproducts of the operator-at-a-time paradigm in a column store DBMS. Recycling makes use of the generic idea of storing and reusing the results of expensive computations. Unlike low level instruction caches, query recycling uses an optimizer to pre-select instructions to cache. The techniques is designed to improve query response times and throughput, while working in a self-organizing fashion.&lt;ref&gt;* {{cite journal
| title=An architecture for recycling intermediates in a column-store
| author=Ivanova, Milena G and Kersten, Martin L and Nes, Niels J and Goncalves, Romulo AP
| journal=ACM Transactions on Database Systems (TODS)
| volume=35
| number=4
| pages=24
| year=2010
| publisher=ACM
}}&lt;/ref&gt; The authors from the [[CWI Amsterdam|CWI]] Database Architectures group, composed of Milena Ivanova, [[Martin L. Kersten|Martin Kersten]], Niels Nes and Romulo Goncalves, won the &quot;Best Paper Runner Up&quot; at annual [[ACM SIGMOD]] conference for their work on Query Recycling.&lt;ref&gt;{{cite web
|url=https://www.cwi.nl/news/2009/1017/CWI-database-team-wins-Best-Paper-Runner-Up-at-SIGMOD-2009
|title=CWI database team wins Best Paper Runner Up at SIGMOD 2009
|publisher = CWI Amsterdam
|accessdate=2009-07-01
|date=
}}
&lt;/ref&gt;&lt;ref name=sigmod-awards&gt;
{{cite web
| url = http://www.sigmod.org/sigmod-awards/sigmod-awards#bestpaper
| title = SIGMOD Awards
| website = ACM SIGMOD
| accessdate = 2014-07-01
}}
&lt;/ref&gt;

=== Database Cracking ===
MonetDB was one of the first databases to introduce Database Cracking. Database Cracking is an incremental partial indexing and/or sorting of the data. It directly exploits the columnar nature of MonetDB. Cracking is a technique that shifts the cost of index maintenance from updates to query processing. The query pipeline optimizers are used to massage the query plans to crack and to propagate this information. The technique allows for improved access times and self-organized behavior.&lt;ref&gt;{{cite conference
| title=Database cracking
| author=Idreos, Stratos and Kersten, Martin L and Manegold, Stefan
| journal=Proceedings of CIDR
| year=2007
}}&lt;/ref&gt; Database Cracking received the [[ACM SIGMOD]] 2011 J.Gray best dissertation award.&lt;ref&gt;{{cite web
| url = http://www.sigmod.org/sigmod-awards/sigmod-awards#dissertation
| title = SIGMOD Awards
| website = ACM SIGMOD
| accessdate = 2014-12-12
}}&lt;/ref&gt;

== Components ==
A number of extensions exist for MonetDB that extend the functionality of the database engine. Due to the three-layer architecture, top-level query interfaces can benefit from optimizations done in the backend and kernel layers.

=== SQL ===
MonetDB/SQL is a top-level extension, which provides complete support for transactions in compliance with the [[SQL:2003]] standard.&lt;ref name=monetdb-20 /&gt;

=== GIS ===
MonetDB/GIS is an extension to MonetDB/SQL with support for the [[Simple Features Access]] standard of [[Open Geospatial Consortium]] (OGC).&lt;ref name=gis /&gt;

=== SciQL ===
SciQL an SQL-based query language for science applications with arrays as first class citizens. SciQL allows MonetDB to effectively function as an [[Array DBMS|array database]]. SciQL is used in the [[European Union]] [http://www.planet-data.eu PlanetData] and [http://www.earthobservatory.eu/ TELEIOS] project, together with the Data Vault technology, providing transparent access to large scientific data repositories.&lt;ref&gt;{{cite proceedings
|  author       = Zhang, Y. and Scheers, L. H. A. and Kersten, M. L. and Ivanova, M. and Nes, N. J.
|  title        = Astronomical Data Processing Using SciQL, an SQL Based Query Language for Array Data
|  booktitle    = Astronomical Data Analysis Software and Systems
|  year = 2011
}}&lt;/ref&gt; Data Vaults map the data from the distributed repositories to SciQL arrays, allowing for improved handling of [[Spatiotemporal database|spatio-temporal]] data in MonetDB.&lt;ref name=dv-1&gt;
{{cite book
| title=Data vaults: a symbiosis between database technology and scientific file repositories
| author=Ivanova, Milena and Kersten, Martin and Manegold, Stefan
| booktitle=Scientific and Statistical Database Management
| pages=485–494
| year=2012
| publisher=Springer Berlin Heidelberg
}}
&lt;/ref&gt; SciQL will be further extended for the [[Human Brain Project]].&lt;ref&gt;{{cite web | url = http://www.sciql.org | title = SCIQL.ORG | date = 4 March 2014}}&lt;/ref&gt;

=== Data Vaults ===
Data Vault is a database-attached external file repository MonetDB, similar to the [[SQL/MED]] standard. The Data Vault technology allows for transparent integration with distributed/remote repositories file repositories. It is designed for scientific data [[data exploration]] and [[data mining|mining]], specifically for [[remote sensing]] data.&lt;ref name=dv-1 /&gt; There is support for the [[GeoTIFF]] ([[Earth observation]]), [[FITS]] ([[astronomy]]), [[MiniSEED]] ([[seismology]]) and [[NetCDF]] formats.&lt;ref name=dv-1 /&gt;&lt;ref name=dv-2&gt;
{{cite news
| author = Ivanova, Milena and Kargin, Yagiz and Kersten, Martin and Manegold, Stefan and Zhang, Ying and Datcu, Mihai and Molina, Daniela Espinoza
| title = Data Vaults: A Database Welcome to Scientific File Repositories
| booktitle = Proceedings of the 25th International Conference on Scientific and Statistical Database Management
| series = SSDBM
| year = 2013
| isbn = 978-1-4503-1921-8
| doi = 10.1145/2484838.2484876
| publisher = ACM
}}
&lt;/ref&gt;
The data is stored in the file repository, in the original format, and loaded in the database in a [[Lazy evaluation|lazy]] fashion, only when needed. The system can also process the data upon ingestion, if the data format requires it.
&lt;ref&gt;
{{cite news
| author = Kargin, Yagiz and Ivanova, Milena and Zhang, Ying and Manegold, Stefan and Kersten, Martin
| title = Lazy ETL in Action: ETL Technology Dates Scientific Data
| journal = Proceedings VLDB Endowment
| volume = 6
| number = 12
|date=August 2013
| issn = 2150-8097
| pages = 1286–1289
| doi = 10.14778/2536274.2536297
| publisher = VLDB Endowment
}}
&lt;/ref&gt;
As a result, even very large file repositories to be efficiently analyzed, as only the required data is processed in the database. The data can be accessed through either the MonetDB SQL or SciQL interfaces. The Data Vault technology was used in the [[European Union]]'s [http://www.earthobservatory.eu/ TELEIOS] project, which was aimed at building a [[virtual observatory]] for Earth observation data.&lt;ref name=dv-2 /&gt;

==== SAM/BAM ====
MonetDB has a [[BAM format|SAM/BAM]] module for efficient processing of [[sequence alignment]] data. Aimed at the [[bioinformatics]] research, the module has a SAM/BAM data loader and a set of SQL UDFs for working with [[DNA]] data.&lt;ref name=sambam /&gt; The module uses the popular [[SAMtools]] library.&lt;ref&gt;{{cite web | url = https://www.monetdb.org/Documentation/Extensions/LifeScience/install | title = SAM/BAM installation | date = 24 November 2014}}&lt;/ref&gt;

=== RDF/SPARQL ===
MonetDB/RDF is a [[SPARQL]]-based extension for working with linked data, which adds support for [[Resource Description Framework|RDF]] and allowing MonetDB to function as a [[triplestore]]. Under development for the [[Linked Open Data#European Union Projects|Linked Open Data 2]] project.&lt;ref name=rdf /&gt;

=== R integration ===
'''MonetDB/R''' module allows for [[User Defined Function|UDFs]] written in [[R (programming language)|R]] to be executed in the SQL layer of the system. This is done using the native R support for running embedded in another application, inside the RDBMS in this case. Previously the '''MonetDB.R''' connector allowed the using MonetDB data sources and process them in an R session. The newer R integration feature of MonetDB does not require data to be transferred between the RDBMS and the R session, reducing overhead and improving performance. The feature is intended to give users access to functions of the R statistical software for in-line analysis of data stored in the RDBMS. It complements the existing support for [[C (programming language)|C]] UDFs and is intended to be used for [[in-database processing]].&lt;ref name= Rintegration &gt;{{cite web | url = https://www.monetdb.org/content/embedded-r-monetdb | title = Embedded R in MonetDB | date = 13 November 2014}}&lt;/ref&gt;

===Former extensions===
MonetDB has had a number of extensions, that were deprecated and removed stable code base over time. Some notable examples include an [[XQuery]] extensions removed in MonetDB version 5; a [[Jaql|JAQL]] extension and a [[streaming data]] extension called ''Data Cell''.&lt;ref&gt;{{cite web| url=https://www.monetdb.org/XQuery/| title=Xquery (obsolete)| publisher=MonetDB| access-date=2015-05-26}}&lt;/ref&gt;
&lt;ref&gt;{{cite web| url=https://www.monetdb.org/pipermail/announce-list/2014-October/000085.html| title=Announcement: New Oct2014 Feature release of MonetDB suite| publisher=MonetDB| access-date=2015-05-26}}&lt;/ref&gt;&lt;ref name=monetdb-20 /&gt;

== See also ==
* [[List of relational database management systems]]
* [[Database management system]]
* [[Column-oriented DBMS]]
* [[Array DBMS]]

== References ==
{{Reflist|2}}

== Bibliography ==
{{refbegin|2}}
* {{cite conference 
| title=Database architecture optimized for the new bottleneck: Memory access
| author=Boncz, Peter and Manegold, Stefan and Kersten, Martin
| journal=Proceedings of International Conference on Very Large Data Bases
| pages=54–65
| year=1999
}}
* {{cite journal
| title=Efficient relational storage and retrieval of XML documents
| author=Schmidt, Albrecht and Kersten, Martin and Windhouwer, Menzo and Waas, Florian
| journal=The World Wide Web and Databases
| pages=137–150
| year=2001
| publisher=Springer
}}
* {{cite conference
| title=Database cracking
| author=Idreos, Stratos and Kersten, Martin L and Manegold, Stefan
| journal=Proceedings of CIDR
| year=2007
}}
* {{cite journal
| title=Breaking the memory wall in MonetDB
| author=Boncz, Peter A and Kersten, Martin L and Manegold, Stefan
| journal=Communications of the ACM
| volume=51
| number=12
| pages=77–85
| year=2008
| publisher=ACM
| doi=10.1145/1409360.1409380
}}
* {{cite journal
| title=Column-store support for RDF data management: not all swans are white
| author=Sidirourgos, Lefteris and Goncalves, Romulo and Kersten, Martin and Nes, Niels and Manegold, Stefan
| journal=Proceedings of the VLDB Endowment
| volume=1
| number=2
| pages=1553–1563
| year=2008
| publisher=VLDB Endowment
| doi=10.14778/1454159.1454227
}}
* {{cite conference
| author = Ivanova, Milena G. and Kersten, Martin L. and Nes, Niels J. and Goncalves, Romulo A.P.
| title = An Architecture for Recycling Intermediates in a Column-store
| booktitle = Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data
| series = SIGMOD '09
| year = 2009
| isbn = 978-1-60558-551-2
| pages = 309–320
| url = http://doi.acm.org/10.1145/1559845.1559879
| doi = 10.1145/1559845.1559879
| publisher = ACM
}}
* {{cite journal
| author = Manegold, Stefan and Boncz, Peter A. and Kersten, Martin L.
| title = Optimizing Database Architecture for the New Bottleneck: Memory Access
| journal = The VLDB Journal
| date = December 2000
| volume = 9
| number = 3
|date=Dec 2000
| issn = 1066-8888
| pages = 231–246
| url = http://dx.doi.org/10.1007/s007780000031
| doi = 10.1007/s007780000031
| publisher = Springer-Verlag New York, Inc.
}}
* {{cite journal
| title=An architecture for recycling intermediates in a column-store
| author=Ivanova, Milena G and Kersten, Martin L and Nes, Niels J and Goncalves, Romulo AP
| journal=ACM Transactions on Database Systems (TODS)
| volume=35
| number=4
| pages=24
| year=2010
| publisher=ACM
}}
* {{cite journal
| title=The data cyclotron query processing scheme
| author=Goncalves, Romulo and Kersten, Martin
| journal=ACM Transactions on Database Systems (TODS)
| volume=36
| number=4
| pages=27
| year=2011
| publisher=ACM
}}
* {{cite journal
| title=The researcher’s guide to the data deluge: Querying a scientific database in just a few seconds
| author=Kersten, Martin L and Idreos, Stratos and Manegold, Stefan and Liarou, Erietta
| journal=PVLDB Challenges and Visions
| year=2011
}}
* {{cite book
| title=SciQL, a query language for science applications
| author=Kersten, M and Zhang, Ying and Ivanova, Milena and Nes, Niels
| booktitle=Proceedings of the EDBT/ICDT 2011 Workshop on Array Databases
| pages=1–12
| year=2011
| publisher=ACM}
}}
* {{cite paper
| title=SciBORQ: Scientific data management with Bounds On Runtime and Quality
| author=Sidirourgos, Lefteris and Kersten, Martin and Boncz, Peter
| year=2011
| publisher=Creative Commons
}}
* {{cite journal
| title=MonetDB/DataCell: online analytics in a streaming column-store
| author=Liarou, Erietta and Idreos, Stratos and Manegold, Stefan and Kersten, Martin
| journal=Proceedings of the VLDB Endowment
| volume=5
| number=12
| pages=1910–1913
| year=2012
| publisher=VLDB Endowment
| doi=10.14778/2367502.2367535
}}
* {{cite book
| title=Data vaults: a symbiosis between database technology and scientific file repositories
| author=Ivanova, Milena and Kersten, Martin and Manegold, Stefan
| booktitle=Scientific and Statistical Database Management
| pages=485–494
| year=2012
| publisher=Springer Berlin Heidelberg
}}
* {{cite news
| author = Kargin, Yagiz and Ivanova, Milena and Zhang, Ying and Manegold, Stefan and Kersten, Martin
| title = Lazy ETL in Action: ETL Technology Dates Scientific Data
| journal = Proceedings VLDB Endowment
| volume = 6
| number = 12
|date=August 2013
| issn = 2150-8097
| pages = 1286–1289
| doi = 10.14778/2536274.2536297
| publisher = VLDB Endowment
}} 
* {{cite conference
| title=Column imprints: a secondary index structure
| author=Sidirourgos, Lefteris and Kersten, Martin
| booktitle=Proceedings of the 2013 international conference on Management of data
| pages=893–904
| year=2013
| publisher=ACM
}}
* {{cite news
| author = Ivanova, Milena and Kargin, Yagiz and Kersten, Martin and Manegold, Stefan and Zhang, Ying and Datcu, Mihai and Molina, Daniela Espinoza
| title = Data Vaults: A Database Welcome to Scientific File Repositories
| booktitle = Proceedings of the 25th International Conference on Scientific and Statistical Database Management
| series = SSDBM
| year = 2013
| isbn = 978-1-4503-1921-8
| doi = 10.1145/2484838.2484876
| publisher = ACM
}}
{{refend}}

== External links ==
* [https://www.monetdb.org/Home Official homepage of MonetDB]
* [https://www.cwi.nl/research-groups/database-architectures Database Architectures group at CWI - the primary developers of MonetDB]
* [https://www.monetdb.org/Home/ProjectGallery List of scientific projects using MonetDB]
* [http://monetr.r-forge.r-project.org MonetDB.R - MonetDB to R Connector]
* [https://www.monetdbsolutions.com/ MonetDB Solutions - MonetDB's professional services company]

{{FOLDOC}}

{{Data warehouse}}

{{DEFAULTSORT:Monetdb}}
[[Category:Big data]]
[[Category:Client-server database management systems]]
[[Category:Column-oriented DBMS software for Linux]]
[[Category:Cross-platform free software]]
[[Category:Cross-platform software]]
[[Category:Data warehousing products]]
[[Category:Database engines]]
[[Category:Dutch inventions]]
[[Category:Free database management systems]]
[[Category:Free software programmed in C]]
[[Category:Products introduced in 2004]]
[[Category:Relational database management systems]]
[[Category:Structured storage]]</text>
      <sha1>7o8ysraymtgzp9c23aps5cge5wcvkxe</sha1>
    </revision>
  </page>
  <page>
    <title>CVidya</title>
    <ns>0</ns>
    <id>31890204</id>
    <revision>
      <id>651906419</id>
      <parentid>636575078</parentid>
      <timestamp>2015-03-18T09:56:40Z</timestamp>
      <contributor>
        <ip>62.219.146.159</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3356">{{buzzword|date=May 2014}}
{{Use dmy dates|date=November 2011}}
{{DISPLAYTITLE:cVidya}}
{{Infobox company
| name = cVidya Networks
| logo =Image:Cvidya_Logo.png
| type = Private
| industry = Telecommunication, [[Service (economics)|Services]], [[Consultant|Consulting]], [[Revenue Management]], [[Revenue assurance|Revenue Assurance]], [[Fraud|Fraud Management]], [[Dealership Management System|Dealer management]]
| foundation = 2001
| founder = [http://www.cvidya.com/Management.html Alon Aginsky]
| employment = 300 (2010)

| homepage = [http://www.cvidya.com/ www.cvidya.com]}}

'''cVidya Networks''' is a provider of Revenue Analytics [[solution]]s for [[communications]] and [[Telecommunications service provider|digital service providers]]. cVidya’s portfolio includes solutions for business protection and business growth, including [[Revenue assurance|Revenue Assurance]], [[Fraud]] Management, Marketing Analytics and [[Sales performance management|Sales Performance Management]] solutions.
The company has 300 employees in 18 countries and has over 150 customers worldwide.
cVidya's investors include [[Battery Ventures]], [[Carmel Ventures]], Hyperion, StageOne, Saints Capital and Plenus.

==History==

cVidya was founded in 2001 by Alon Aginsky in the US and saw successful projects at [[Telecom Italia]] and [[Bezeq]] within its first couple of years.

Between 2004–2010 cVidya became one of the leading vendors in the [[Revenue assurance|Revenue Assurance]] domain with the MoneyMap® product suite, and gained global market penetration which began with their entrance into Europe followed closely by LATAM, APAC, North America and Africa.

cVidya’s acquisition of ECtel in 2010 &lt;ref&gt;[http://www.billingworld.com/news/2010/01/cvidya-wraps-ectel-acquisition.aspx cVidya Wraps ECtel Acquisition]. Billingworld.com&lt;/ref&gt; was a major milestone for the company. By acquiring the larger, publicly traded company and through consolidating their product portfolios cVidya added the FraudView® [[Fraud]] Management product and integrated it to its Revenue Analytics solutions portfolio.

In 2011 cVidya adds [[Risk Management]] product to its portfolio and launches the Education Center service, offering Revenue Assurance and Fraud Management [[eLearning]] courses to industry professionals.&lt;ref&gt;[http://www.telecomasia.net/content/cvidya-launches-elearning-education-center?src=related cVidya launches eLearning education center]. TelecomAsia.net&lt;/ref&gt; In that same year, [[Gartner]] Ranks cVidya as the global market leader.&lt;ref&gt;[http://www.billingworld.com/news/2011/04/gartner-puts-cvidya-at-top-of-revenue-assurance-m.aspx Gartner Puts cVidya at Top of Revenue Assurance Market]. Billingworld.com&lt;/ref&gt;

In 2013 cVidya expands its portfolio, adding [[Big Data]] Analytics capabilities and Marketing [[Analytics]] solution suite.&lt;ref&gt;[http://www.telcoprofessionals.com/pressreleases/611/c-vidya-launches-powerful-marketi cVidya Launches Powerful Marketing Analytics Suite]. TelcoProfessionals.com&lt;/ref&gt;

==Pictures==
&lt;gallery&gt;
:Image:Cvidya_Logo.gif|cVidya Logo
&lt;/gallery&gt;

==External links==
* [http://www.cvidya.com cVidya website]

== References ==
{{reflist|colwidth=30em}}

[[Category:Revenue assurance]]
[[Category:Big data]]

[[Category:Business intelligence companies]]
[[Category:Revenue services]]
[[Category:Consulting firms]]</text>
      <sha1>q53t8qduy77tcosg1riqy7veydwjkur</sha1>
    </revision>
  </page>
  <page>
    <title>Alpine Data Labs</title>
    <ns>0</ns>
    <id>42149032</id>
    <revision>
      <id>649753027</id>
      <parentid>634597091</parentid>
      <timestamp>2015-03-03T22:27:47Z</timestamp>
      <contributor>
        <ip>192.35.35.35</ip>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10054">{{Infobox company
| name             = Alpine Data Labs
| logo             = File:Alpine_only_logo.jpg
| logo_caption     = 
| image            = 
| image_caption    = 
| type             = Private
| industry         = 
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = [[San Mateo, California|San Mateo]], [[California]] 2011
| co-founders          = Anderson Wong &amp; Yi-Ling Chen
| defunct          = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| location_city    = San Francisco, California
| location_country = USA
| locations        = &lt;!-- Number of locations, stores, offices, etc. --&gt;
| area_served      = 
| key_people       = Joe Otto, President &amp; CEO
| services         = Advanced Analytics on Hadoop and Big Data
| num_employees    = 45 (As of October 2013)
| parent           = 
| divisions        = 
| subsid           = 
| homepage         = {{URL|http://www.alpinenow.com}}
}}

'''Alpine Data Labs''' is an advanced analytics interface working with [[Apache Hadoop]] and [[big data]].&lt;ref&gt;{{cite web|url=http://sandhill.com/article/sand-hill-50-swift-and-strong-in-big-data/|title=Sand Hill 50 &quot;Swift and Strong&quot; in Big Data|publisher=Sand Hill|date=1/8/14|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref name=expand1&gt;{{cite web|url=http://insideanalysis.com/2014/01/10-companies-and-technologies-to-watch-in-2014/|title=10 Companies and Technologies to Watch in 2014|publisher=Inside Analysis|author=Robin Bloor|date=1/6/14|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://datadoodle.com/2013/12/04/alpine-data-and-goliath/|title=Alpine Data and Goliath|publisher=Data Doodle|author=Ted Cuzzillo|date=12/4/13|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://venturebeat.com/2013/10/30/big-data-little-companies-these-six-startups-want-to-disrupt-the-data-world/|title=Big data, little companies: These six startups want to disrupt the data world|publisher=Venture Beat|author=Eric Blattberg|date=2013-10-30|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.allanalytics.com/author.asp?section_id=2220&amp;doc_id=268388|title=Big-Data Draws Attention at Interop New York|publisher=All Analytics|author=Noreen Seebacher|date=10/2/13|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.forbes.com/sites/emc/2013/12/03/making-big-data-work-for-your-business/|title=Making Big Data Work For Your Business|publisher=Forbes|author=Scott Koegler|date=12/3/13|accessdate=3/8/14}}&lt;/ref&gt; It provides a collaborative, visual environment to create and deploy analytics workflow and predictive models.&lt;ref&gt;{{cite web|url=http://www.analystone.com/the-analyst-one-top-technologies-list/|title=The breakthrough technologies every analyst should know about|publisher=Analyst One|author=Bob Gourley|date=2013-10-17|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://siliconangle.com/blog/2013/10/29/2014-the-year-of-big-data-applications-bigdatanyc-2013/|title=2014, the year of Big Data applications|publisher=Silicon Angle|author=Valentina Craft|date=2013-10-29|accessdate=3/8/14}}&lt;/ref&gt; This aims to make analytics more suitable for business analyst level staff, like sales and other departments using the data, rather than requiring a &quot;data scientist&quot; who understands languages like [[MapReduce]] or [[Pig (programming tool)|Pig]].&lt;ref name=expand1/&gt;&lt;ref name=expand2&gt;{{cite web|url=http://venturebeat.com/2013/11/22/alpine-data-labs-gets-16m-to-ensure-companies-wont-fail-with-hadoop/|title=Alpine Data Labs gets $16M to ensure companies ‘won’t fail’ with big data analytics|publisher=Venture Beat|author=Christina Farr|date=2013-11-22|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref name=expand3&gt;{{cite web|url=http://gigaom.com/2013/11/22/alpine-data-labs-raises-16-million-for-its-visual-approach-to-data-science/|title=Alpine Data Labs raises $16 million for its visual approach to data science|publisher=Gigaom|author=Derrick Harris|date=2013-11-22|accessdate=3/8/14}}&lt;/ref&gt;

Co-founded by Anderson Wong and Yi-Ling Chen, Joe Otto serves as president and [[CEO]] of Alpine Data Labs.&lt;ref&gt;{{cite web|url=http://www.forbes.com/sites/gilpress/2013/09/24/whats-a-cmo-to-do-alpine-data-labs-otto-and-aziza-on-the-digital-marketing-landscape/|title=What's A CMO To Do? Alpine Data Labs' Otto And Aziza On The Digital Marketing Landscape|publisher=Forbes|author=Gil Press|date=2013-09-24|accessdate=3/8/14}}&lt;/ref&gt;

==History==

[[Greenplum]] commissioned its then employees Anderson Wong and Yi-Ling Chen to develop an [[application software|app]] that could work with [[database]]s.&lt;ref name=expand6&gt;{{cite web|url=http://gigaom.com/2011/07/20/greenplum-protege-brings-predictive-muscle-to-exadata/|title=Greenplum protégé brings predictive muscle to Exadata|publisher=Gigaom|author=Derrick Harris|date=7/20/11|accessdate=3/8/14}}&lt;/ref&gt; Greenplum was acquired by [[EMC Corporation]] and Alpine Data Labs was co-founded by Wong and Chen in 2010.&lt;ref name=expand4&gt;{{cite web|url=http://www.datanami.com/datanami/2013-10-29/alpine_demos_big_data_analytics_from_an_ipad.html|title=Alpine Demos Big Data Analytics from an iPad|publisher=Datanami|author=Alex Woodie|date=2013-10-29|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://techcrunch.com/2011/05/11/alpine-data-labs-scores-7-5-million-to-help-companies-analyze-troves-of-data/|title=Alpine Data Labs Scores $7.5 Million To Help Companies Analyze Troves Of Data|publisher=TechCrunch|author=Robin Wauters|date=5/11/11|accessdate=3/8/14}}&lt;/ref&gt; The site launched in May 2011 with Wong serving as CEO.&lt;ref name=expand5&gt;{{cite web|url=http://www.bizjournals.com/sanfrancisco/news/2011/05/11/alpine-labs-gets-75m-to-mine-big-data.html|title=Alpine Labs gets $7.5M to mine Big Data|publisher=San Francisco Business Times|author=Patrick Hoge|date=5/11/11|accessdate=3/8/14}}&lt;/ref&gt; That month, Alpine raised 7.5 million in [[Series A round]] funding from EMC Greenplum, Sierra Ventures, Mission Ventures, and Sumitomo Corp. Equity Asia.&lt;ref name=expand5/&gt; The funding was used in part to move Alpine’s headquarters from [[Beijing]] to [[San Mateo, California]].&lt;ref&gt;{{cite web|url=http://www.dbta.com/Editorial/News-Flashes/Alpine-Data-Labs-Raises-75-Million-in-Series-A-Funding-and-Formally-Launches-in-the-US-75490.aspx|title=Alpine Data Labs Raises $7.5 Million in Series A Funding and Formally Launches in the U.S.|publisher=Database Trends and Applications|date=5/13/11|accessdate=3/8/14}}&lt;/ref&gt; It’s core product then, Alpine Miner, allowed for non-data scientists to create predictive analytics data models without using code and used an &quot;In-Database&quot; model.&lt;ref&gt;{{cite web|url=http://siliconangle.com/blog/2011/05/23/alpine-data-labs-offers-visualization-tools-to-create-in-database-analytics-models/|title=Alpine Data Labs Offers Visualization Tools to Create In-Database Analytics Models|publisher=Silicon Angle|author=Jeffrey Kelly|date=5/23/11|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.datanami.com/datanami/2012-02-16/alpine_data_climbs_analytics_mountain.html|title=Alpine Data Climbs Analytics Mountain|publisher=Datanami|author=Robert Gelber|date=2/16/12|accessdate=3/8/14}}&lt;/ref&gt; In June 2011, Alpine Miner 2.0 for [[Oracle Database]] was released.&lt;ref name=expand6/&gt;

Tom Ryan was appointed CEO and president of Alpine Data Labs in January 2012 and served until April 2013. The following month, Joe Otto was appointed to serve as CEO and president.&lt;ref&gt;{{cite web|url=http://www.pehub.com/2013/03/alpine-data-labs-names-joe-otto-ceo/|title=Alpine Data Labs Names Joe Otto CEO|publisher=Reuters PE HUB|date=5/3/13|accessdate=3/8/14}}&lt;/ref&gt; In November 2013, Alpine Data Labs raised $16 million in Series B venture funding.&lt;ref name=expand2/&gt;&lt;ref name=expand3/&gt; Investors included Sierra Ventures, Mission Ventures, UMC Capital, and [[Robert Bosch GmbH|Robert Bosch Venture Capital GmbH]].&lt;ref&gt;{{cite web|url=http://www.forbes.com/sites/alexkonrad/2013/11/22/alpine-data-labs-raises-series-b/|title=This Startup Just Raised $16M To Help Barclays, Nike And Havas Play With Big Data|publisher=Forbes|author=Alex Konrad|date=2013-11-22|accessdate=3/8/14}}&lt;/ref&gt; That same month, it also released Alpine 3.0, which introduced a [[drag and drop]] interface and access to data from any device that with internet capabilities, including [[tablet computer|tablets]] and phones.&lt;ref name=expand4/&gt;&lt;ref&gt;{{cite web|url=http://www.networkworld.com/slideshow/127869/products-of-the-week-111113.html#slide16|title=Products of the week 11.11.13|publisher=Network World|author=Brandon Butler|date=11/11/13|accessdate=3/8/14}}&lt;/ref&gt; This makes it possible for analysts to access data on Hadoop, and other databases and data warehouses, without IT having to move the data into another interface.&lt;ref&gt;{{cite web|url=http://www.itbusinessedge.com/blogs/it-unmasked/alpine-data-analytics-app-works-directly-against-hadoop.html|title=Alpine Data Analytics App Works Directly Against Hadoop|publisher=IT Business Edge|author=Mike Vizard|date=11/7/13|accessdate=3/8/14}}&lt;/ref&gt; Alpine also moved its headquarters from San Mateo to [[San Francisco]] in November 2013. In February 2014, Alpine Data Labs was added to the [[Gartner Magic Quadrant]] as a &quot;Niche Player.&quot;&lt;ref&gt;{{cite web|url=http://www.kdnuggets.com/2014/02/gartner-mq-for-advanced-analytics-platforms.html|title=SAS, IBM, RapidMiner, Knime leaders in Gartner MQ for Advanced Analytics Platforms|publisher=KDnuggets|author=Gregory Piatetsky|date=2014-02-24|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.gartner.com/doc/2667527/magic-quadrant-advanced-analytics-platforms|title=Magic Quadrant for Advanced Analytics Platforms|publisher=Gartner|date=2014-02-19|accessdate=3/8/14}}&lt;/ref&gt;

In March 2014, Alpine Data Labs was certified by [[Databricks]] on [[Apache Spark]].&lt;ref&gt;{{cite web|url=http://inside-bigdata.com/2014/03/18/databricks-certifies-alpine-data-labs-spark/|title=Databricks Certifies Alpine Data Labs on Spark|publisher=Inside Big Data|author=Daniel Gutierrez|date=2014-03-18|accessdate=2014-03-30}}&lt;/ref&gt;

==References==
{{reflist|2}}

[[Category:Big data]]
[[Category:Companies established in 2010]]</text>
      <sha1>sec7gjrevi7kxdh4punzkmz69gifszh</sha1>
    </revision>
  </page>
  <page>
    <title>Platfora</title>
    <ns>0</ns>
    <id>41715854</id>
    <revision>
      <id>667538237</id>
      <parentid>665036272</parentid>
      <timestamp>2015-06-18T20:03:20Z</timestamp>
      <contributor>
        <username>Dewritech</username>
        <id>11498870</id>
      </contributor>
      <comment>/* Customers */clean up, [[WP:AWB/T|typo(s) fixed]]: Paypal → PayPal using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6307">{{Infobox company
|name             = Platfora
| logo     =

|industry         = [[Data analytics|Big Data Analytics]]
|founder          = Ben Werther ([[CEO]])
|location_city    = [[San Mateo, California]],
|num_employees    = 125
|homepage         = {{URL|http://www.platfora.com|Platfora.com}}
}}

'''Platfora, Inc.''' is a [[big data analytics]] company based in [[San Mateo, California]]. The firm’s software works in tandem with the [[open-source software]] framework [[Apache Hadoop]] to assist companies and government organizations with rapid data analysis, [[data visualization]], and sharing.&lt;ref name=&quot;1 Business Insider&quot;&gt;{{cite web|last=Bort|first=Julie|title=Larry Page's College Friend Has Launched a Cool New Startup|url=http://www.businessinsider.com/platfora-ben-werther-2012-10|publisher=Business Insider|accessdate=22 January 2014}}&lt;/ref&gt;&lt;ref name=&quot;2 WSJ&quot;&gt;{{cite web|last=Gage|first=Deborah|title=Platfora Founder Goes in Search of Big-Data Answers|url=http://blogs.wsj.com/venturecapital/2013/04/15/platfora-founder-goes-in-search-of-big-data-answers/|publisher=Wall Street Journal|accessdate=22 January 2014}}&lt;/ref&gt;&lt;ref name=&quot;4 Bloomberg&quot;&gt;{{cite web|last=Vance|first=Ashlee|title=Big Data for Dummies or at Least Product Managers|url=http://www.businessweek.com/articles/2013-09-12/big-data-for-dummies-or-at-least-product-managers|publisher=Bloomberg Businessweek|accessdate=22 January 2014}}&lt;/ref&gt;

== History ==
Platfora was founded in 2011 by Ben Werther. Werther studied [[Computer Science]] at [[Stanford University]].&lt;ref name=&quot;1 Business Insider&quot; /&gt; Prior to founding Platfora, he worked at There Inc., [[Siebel Systems]], [[Microsoft]], and [[Greenplum]].&lt;ref name=&quot;1 Business Insider&quot; /&gt;&lt;ref name=&quot;5 Information Week&quot;&gt;{{cite web|last=Henschen|first=Doug|title=Big Data's Big Picture: An Insider's View|url=http://www.informationweek.com/big-data/big-data-analytics/big-datas-big-picture-an-insiders-view/d/d-id/899326|publisher=Information Week|accessdate=22 January 2014}}&lt;/ref&gt;
In 2011, Werther met regularly with former coworkers John Eshleman and SriSatish Ambati in a café in downtown San Mateo, California.&lt;ref name=&quot;2 WSJ&quot; /&gt; At one of these meetings, while discussing the technical process of big data analysis, Werther realized that he could develop software that paired with Hadoop to greatly speed up data analysis and visualization. Werther sought financing to start Platfora; Eshleman was initially an adviser, and later joined Platfora as founding vice president of technology. Ambati formed his own company, [[Oxdata]] (now [[H2O (software)|H2O.ai]]).&lt;ref name=&quot;2 WSJ&quot; /&gt; In 2012, Platfora acquired startup Plot.io for its browser-based data visualization technology.

Platfora received funding from [[Andreessen Horowitz]], [[Battery Ventures]], [[Sutter Hill Ventures]], [[Tenaya Capital]], [[Citi Ventures]], [[Cisco]], [[Allegis Capital]] and [[In-Q-Tel]], the venture fund of the [[Central Intelligence Agency]]. Series C funding was $38 million. As of April 2014, total funding stood at $65 million.&lt;ref name=&quot;1 Business Insider&quot; /&gt;&lt;ref name=&quot;2 WSJ&quot; /&gt;&lt;ref name=&quot;4 Bloomberg&quot; /&gt;

Platfora is one of several new big data analytics companies that industry analysts expect to compete with established firms including [[SAP AG|SAP]], [[IBM]], [[SAS Institute|SAS]], and [[Oracle Corporation|Oracle]], whose older methods of data analysis and visualization are currently more time consuming.&lt;ref name=&quot;4 Bloomberg&quot; /&gt;&lt;ref&gt;{{cite web|last=Bort|first=Julie|title=How Big Data Startups Could Kill a $30 Billion Industry|url=http://www.businessinsider.com/how-new-big-data-startups-will-kill-this-30-billion-industry-2012-8|publisher=Business Insider|accessdate=22 January 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Harris|first=Derrick|title=Visualization is the future: 6 startups re-imagining how we consume data|url=http://gigaom.com/2013/05/13/visualization-is-the-future-6-startups-re-imagining-how-we-consume-data/|publisher=Gigaom|accessdate=22 January 2014}}&lt;/ref&gt;

== Product ==
Platfora’s software works with the open-source software framework Apache Hadoop; when a user queries a database, the product delivers answers in [[Real-time computing|real time]] via a [[graphical user interface]]. [[Bloomberg Businessweek]] called it “Big Data for Dummies.”&lt;ref name=&quot;4 Bloomberg&quot; /&gt; A corporate or government data analyst can use the interface to filter results, or drag and drop fields to create graphs, overlays, and other visualizations of the data. The analyst can then share those data visualizations and answers with others.&lt;ref name=&quot;2 WSJ&quot; /&gt;&lt;ref name=&quot;4 Bloomberg&quot; /&gt;

==Customers==
FedEx, JP Morgan Chase, PayPal, Bank of New York, Cox Automotive, Opower, Disney, Riot Games, Edmunds.com, Sears, Vivint, The Washington Post, Opower, Citi, Unisys, Groupon, AutoTrader, TUI Travel among many others.

== Awards and recognition ==
[[CRN Magazine]] named Platfora’s software one of “The 10 Coolest Big Data Products of 2013.”&lt;ref&gt;{{cite web|last=Whiting|first=Rick|title=The 10 Coolest Big Data Products Of 2013|url=http://www.crn.com/slide-shows/applications-os/240164423/the-10-coolest-big-data-products-of-2013.htm|publisher=CRN Magazine|accessdate=22 January 2014}}&lt;/ref&gt; 
Platfora’s software was also voted a winner at the Big Data Festival in Kansas City, where it was used to analyze the city’s crime data.&lt;ref&gt;{{cite web|title=Platfora and DST Win Big at Big Data Kansas City Festival|url=http://finance.yahoo.com/news/platfora-dst-win-big-big-140000848.html|publisher=Yahoo Finance|accessdate=22 January 2014}}&lt;/ref&gt;  [[CIO Magazine]] ranked Platfora number 1 on the &quot;10 Hot Hadoop Startups to Watch&quot; list.&lt;ref&gt;{{cite web|last=Vance|first=Jeff|title=10 Hot Hadoop Startups to Watch|url=http://www.cio.com/article/751572/10_Hot_Hadoop_Startups_to_Watch_|publisher=CIO Magazine|accessdate=16 April 2014}}&lt;/ref&gt;

== See also ==
* [[Big data]]
* [[Data science]]
* [[Apache Hadoop]]
* [[Map Reduce]]
* [[In-Q-Tel]]

== References ==
{{reflist|33em}}

== External links ==
* [http://www.platfora.com Official website]
* [http://patents.justia.com/inventor/john-glenn-eshleman Patents by John Eshleman]
* [http://www.bigdatafestival.co Big Data Festival website]

[[Category:Big data]]
[[Category:Software companies based in California]]</text>
      <sha1>doz67l7u507tzz78bswolxwu69p48ek</sha1>
    </revision>
  </page>
  <page>
    <title>Sojern</title>
    <ns>0</ns>
    <id>42370871</id>
    <revision>
      <id>652845317</id>
      <parentid>630367318</parentid>
      <timestamp>2015-03-21T06:58:38Z</timestamp>
      <contributor>
        <username>Coffee</username>
        <id>4015543</id>
      </contributor>
      <comment>Removing link(s) to &quot;Programmatic marketing&quot;: Article deleted. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15651">{{Orphan|date=April 2014}}

{{buzzword|date=May 2014}}
{{ad|date=May 2014}}

{{Infobox company
| name             = Sojern 
| logo             =

| logo_caption     = 
| image            = 
| image_caption    = 
| trading_name     = &lt;!-- d/b/a/, doing business as - if different from legal name above --&gt;
| native_name      = &lt;!-- Company's name in home country language --&gt;
| native_name_lang = &lt;!-- Use ISO 639-1 code, e.g. &quot;fr&quot; for French. If there is more than one native name, in different languages, enter those names using {{tl|lang}}, instead. --&gt;
| romanized_name   = 
| former_name      = 
| former type      = 
| type             = Private 
| traded_as        = 
| industry         = Travel/Data advertising 
| genre            = &lt;!-- Only used with media and publishing companies --&gt;
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = Omaha, NE, {{Start date|2007|09|}}
| founder          = Gordon Whitten
| defunct          = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| location_city    = San Francisco, CA
| location_country = 
| locations        = &lt;!-- Number of locations, stores, offices, etc. --&gt;
| area_served      = Worldwide 
| key_people       = Mark Rabe (CEO)&lt;br /&gt;Curtis Atkisson (CFO) &lt;br /&gt; Brad King (VP of Sales) &lt;br /&gt; Stephen Taylor (International VP &amp; Managing Director) 
| products         = 
| production       = 
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| aum              = &lt;!-- Only used with financial services companies --&gt;
| assets           = 
| equity           = 
| owner            = 
| num_employees    = 100+
| parent           = 
| divisions        = 
| subsid           = 
| homepage         =  {{URL|www.sojern.com}} 
| footnotes        = 
| intl             = 
| bodystyle        = 
}}

'''Sojern''' is a provider of a data-driven traveler marketing platform that utilizes programmatic buying and machine learning technology.&lt;ref name=&quot;crunchbase&quot;&gt;{{cite web|title=Sojern|url=http://www.crunchbase.com/company/sojern|publisher=CrunchBase|accessdate=10 March 2014}}&lt;/ref&gt;&lt;ref name=&quot;eyefortravel&quot;&gt;{{cite web|title=Sojern confirms $7.5 million funding|url=http://www.eyefortravel.com/mobile-and-technology/sojern-confirms-75-million-funding|publisher=EyeforTravel Limited|accessdate=11 March 2014}}&lt;/ref&gt;  Sojern partners with travel companies including airlines, [[Travel website#Online travel agencies|OTAs]], hotels, and rental car companies to collect [[Data anonymization|anonymized]] (non-personally identifiable) traveler profiles.&lt;ref name=&quot;eyefortravel&quot; /&gt;&lt;ref name=&quot;travolution&quot;&gt;{{cite web|title=Sojern brings its Big Data travel technology to Europe|url=http://www.travolution.co.uk/articles/2013/03/07/6537/sojern-brings-its-big-data-travel-technology-to-europe.html|publisher=Travolution|accessdate=13 March 2014}}&lt;/ref&gt;  The company utilizes this data to target travelers and deliver marketing messages across media channels.&lt;ref name=&quot;crunchbase&quot; /&gt;&lt;ref name=&quot;travolution&quot; /&gt; Sojern is currently headquartered in San Francisco, CA with key offices in New York, Omaha and London.&lt;ref name=&quot;crunchbase&quot; /&gt;

==History==

Originally based in Omaha, Nebraska, Sojern was founded in 2007 by Gordon Whitten and has since acquired patents for its products.&lt;ref name=&quot;silicon&quot;&gt;{{cite web|last=Stacy|first=Michael|title=Sojern brings Yahoo! veteran Mark Rabe aboard as CEO|url=http://www.siliconprairienews.com/2011/04/sojern-brings-yahoo-veteran-mark-rabe-aboard-as-ceo|publisher=Silicon Prairie News|accessdate=13 March 2014}}&lt;/ref&gt;  In April 2011, Yahoo! veteran Mark Rabe succeeded Whitten as CEO, though Whitten has stayed on as a member of the board of directors.&lt;ref name=&quot;silicon&quot; /&gt; According to Rabe, the company’s mission has been to build the world’s leading data-driven travel engagement platform.&lt;ref name=&quot;flight&quot;&gt;{{cite web|title=Sojern Takes Flight: Big Data Travel Company Reports More Than Double Year-on-Year Growth|url=http://www.prnewswire.com/news-releases/sojern-takes-flight-big-data-travel-company-reports-more-than-double-year-on-year-growth-195813661.html|accessdate=12 March 2014}}&lt;/ref&gt;

Sojern began as a pioneer in the [[In-flight advertising#Boarding passes advertising|boarding pass advertising]] space; it was the first company in the U.S. with exclusive patents for Printed and Online Boarding Pass customization technology.&lt;ref name=&quot;techcrunch&quot;&gt;{{cite web|last=Lunden|first=Ingrid|title=Sojern Raises $7.5 Million For Its Push Into Travel Tech And Targeted Travel Ads|url=http://techcrunch.com/2012/03/27/sojern-raises-7-5-million-for-its-push-into-travel-tech-and-targeted-travel-ads/|publisher=TechCrunch|accessdate=12 March 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=United States Patent 8,131,592|url=http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=2&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=Sojern&amp;s2=%22Boarding+pass%22&amp;OS=Sojern+AND+%22Boarding+pass%22&amp;RS=Sojern+AND+%22Boarding+pass%22|publisher=USPTO|accessdate=31 March 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=United States Patent 8,150,731|url=http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=Sojern&amp;s2=%22Boarding+pass%22&amp;OS=Sojern+AND+%22Boarding+pass%22&amp;RS=Sojern+AND+%22Boarding+pass%22|publisher=USPTO|accessdate=31 March 2014}}&lt;/ref&gt;  In 2008, the company’s airline partners, including [[Delta Air Lines]] and Continental Airlines, began utilizing Sojern’s technology to offer targeted ads on boarding passes.&lt;ref name=delta&gt;{{cite web|last=Prior|first=Anna|title=Delta Launches Online Boarding-Pass Ads|url=http://online.wsj.com/news/articles/SB121608829404253341|publisher=The Wall Street Journal|accessdate=18 March 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Yu|first=Roger|title=Airlines turn boarding passes into ad space|url=http://usatoday30.usatoday.com/travel/flights/2008-07-14-boarding-pass-ads_N.htm?csp=15|publisher=USA TODAY|accessdate=18 March 2014}}&lt;/ref&gt; Sojern released a new media platform, the Sojern Traveler Platform (STP), in 2011 that focused on improving its technology's targeting capabilities.&lt;ref name=&quot;techcrunch&quot; /&gt;&lt;ref name=skift&gt;{{cite web|last=Schaal|first=Dennis|title=Sojern Raises $10 Million in Series C Funding Round|url=http://skift.com/2013/12/17/sojern-raises-10-million-in-series-c-funding-round/|publisher=Skift|accessdate=12 March 2014}}&lt;/ref&gt;

To protect its platform technology, the company received a [[Patent|U.S. Patent]] for “providing customized or personalized content, relevant and timely messages and targeted advertising to travelers based on their destination and dates of travel” in March 2012.&lt;ref&gt;{{cite web|title=Sojern Receives U.S. Patent for Method to Provide Customized Messages to Travelers|url=http://www.prnewswire.com/news-releases/sojern-receives-us-patent-for-method-to-provide-customized-messages-to-travelers-142611226.html|accessdate=13 March 2014}}&lt;/ref&gt;  

Since its founding, Sojern has been expanding with a 3,811% fiscal year revenue growth between 2008 and 2012.&lt;ref name=&quot;deloitte&quot;&gt;{{cite web|title=Technology Fast 500 2013 Ranking|url=http://www.deloitte.com/assets/Dcom-UnitedStates/Local%20Assets/Documents/TMT_us_tmt/us_tmt_fast500_rankings_110713.pdf|publisher=Deloitte|accessdate=10 March 2014}}&lt;/ref&gt;  The company has also made moves towards international expansion, establishing a London office in 2013.&lt;ref name=&quot;travolution&quot; /&gt; Sojern has now worked with over 600 brands, such as [[Hyatt]], [[Avis Budget Group|Avis]], [[Microsoft]], and [[US Airways]].&lt;ref&gt;{{cite web|title=Sojern Named Top Travel-Focused Technology Company on Deloitte's Annual Fast 500 List|url=http://www.prnewswire.com/news-releases/sojern-named-top-travel-focused-technology-company-on-deloittes-annual-fast-500-list-231723701.html}}&lt;/ref&gt;

==Product==

In addition to online and print boarding pass products, the Sojern Traveler Platform uses data collected by the platform to target audiences and provide access to [[touchpoint|customer touch-points]].&lt;ref name=&quot;eyefortravel&quot; /&gt;&lt;ref name=&quot;techcrunch&quot; /&gt;&lt;ref name=&quot;skift&quot; /&gt; Specifically the company offers solutions in such areas as [[display advertising]], mobile, video, Facebook (FBX) campaigns, prospecting, and [[retargeting]]. &lt;ref name=&quot;techcrunch&quot; /&gt;&lt;ref&gt;{{cite web|title=Company Overview of Sojern Inc.|url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=46740682|publisher=Bloomberg}}&lt;/ref&gt; Sojern uses data on traveler behavior in addition to targeting algorithms and programmatic bidding to provide these solutions.&lt;ref name=&quot;eyefortravel&quot; /&gt;&lt;ref name=&quot;flight&quot; /&gt;

==Funding==

With over $40 million raised, Sojern ranked #14 on travel news website Skift’s “Top 31 Most Heavily Venture-Funded Startups in Travel.”&lt;ref&gt;{{cite web|last=Ali|first=Rafat|title=The Top 31 Most Heavily Venture-Funded Startups In Travel|url=http://skift.com/2014/01/31/the-top-29-most-heavily-venture-funded-startups-in-travel/|publisher=Skift|accessdate=12 March 2014}}&lt;/ref&gt; Sojern received $16 million in [[Series A round|Series A]] funding in 2008, $10 million and then $7.5 million in Series B in 2013, and recently secured a Series C funding round of $10 million.&lt;ref name=tnooz&gt;{{cite web|last=May|first=Kevin|title=Sojern wins $10M funding round as travel marketing goes hi-tech |url=http://www.tnooz.com/article/travel-marketing-sojern-series-c|publisher=Tnooz}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Hoge|first=Patrick|title=Sojern raises $10 million more for travel data, ads|url=http://www.bizjournals.com/sanfrancisco/blog/2013/12/sojern-raises-10-million-more-for.html|publisher=San Francisco Business Times|accessdate=18 March 2014}}&lt;/ref&gt;  The company’s investors include Industry Ventures, Focus Ventures, [[Norwest Venture Partners]], Trident Capital, and Triangle Peak Partners.&lt;ref name=&quot;skift&quot; /&gt;&lt;ref name=&quot;tnooz&quot; /&gt;

==Privacy==

Sojern collects and uses anonymous, non-personally identifiable information.&lt;ref name=&quot;travolution&quot; /&gt;&lt;ref name=&quot;techcrunch&quot; /&gt;&lt;ref&gt;{{cite web|title=Travelport and Sojern Forge Partnership to Provide Destination Targeted Content on Travel Itineraries|url=http://www.businesswire.com/news/home/20090922005842/en/Travelport-Sojern-Forge-Partnership-Provide-Destination-Targeted#.UydH3q1dVe5}}&lt;/ref&gt;&lt;ref name=privacy&gt;{{cite web|title=Website Privacy Policy|url=http://www.sojern.com/privacy.php}}&lt;/ref&gt;  Traveler profiles are anonymized to ensure that [[personally identifiable information]], such as name and contact information, is hidden.&lt;ref name=&quot;techcrunch&quot; /&gt;&lt;ref name=&quot;privacy&quot; /&gt; In order to identify what information and offers a site visitor might be interested in, anonymous [[HTTP cookie|cookies]] record which pages a site visitor has browsed or which products were purchased on partners’ websites.&lt;ref name=&quot;privacy&quot; /&gt; This data is anonymous and cannot be used to identify visitors.&lt;ref name=&quot;privacy&quot; /&gt;

==Travel Trends and Online Presence==

Sojern’s travel trends reports have been cited by several sites including [[Zagat]], Skift, Tnooz, and [[The Boston Globe]].&lt;ref&gt;{{cite web|last=Clark|first=Colleen|title=Most U.S. Travelers Go It Alone|url=http://blog.zagat.com/2013/07/most-us-travelers-go-it-alone.html|publisher=Zagat|accessdate=12 March 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Shankman|first=Samantha|title=Travelers More Likely to Book Hotels on Day of Arrival Than Any Day Prior|url=http://skift.com/2014/01/31/travelers-more-likely-to-book-hotels-on-day-of-arrival-than-any-day-prior/|publisher=Skift|accessdate=12 March 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Vivion|first=Nick|title=Latest analysis of US consumer travel show marketable trends|url=http://www.tnooz.com/article/latest-analysis-of-us-consumer-travel-show-marketable-trends-infographic/|publisher=Tnooz|accessdate=12 March 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Kandarian|first=Paul|title=Sojern releases quarterly travel data|url=http://www.boston.com/travel/blog/2013/06/sojern_releases.html|publisher=The Boston Globe|accessdate=13 March 2014}}&lt;/ref&gt;  The company’s holiday and travel trends infographics are also found on sites like [[Reddit]], Tnooz, and [[Visual.ly]].&lt;ref&gt;{{cite web|title=Infographics|url=http://www.reddit.com/r/Infographics/search?q=travel+trends+&amp;sort=relevance&amp;restrict_sr=on&amp;t=all|publisher=Reddit}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Most recent data breaks down summer travel trends|url=http://www.tnooz.com/article/most-recent-data-breaks-down-summer-travel-trends-infographic/|publisher=Tnooz}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=2013 U.S. Summer Travel Trends|url=http://visual.ly/sojerns-2013-summer-travel-trends|publisher=Visual.ly}}&lt;/ref&gt;  Following the release of the Q4 2013 report, [[The New York Times]] featured CEO Mark Rabe in an article on current and future travel trends.&lt;ref&gt;{{cite web|last=Christiansen|first=Kenan|title=Travel Trends and the Year Ahead|url=http://www.nytimes.com/2014/01/12/travel/travel-trends-and-the-year-ahead.html?_r=0|publisher=The New York Times|accessdate=11 March 2014}}&lt;/ref&gt;

Other news organizations have also covered the company's VPs; in 2013, the company’s VP of product Chris Smutny wrote an article for [[Advertising Age|Ad Age]] on monetizing data assets.&lt;ref&gt;{{cite web|last=Smutny|first=Chris|title=The Bottom Line: Turn Your Data Into Dollars|url=http://adage.com/article/datadriven-marketing/bottom-line-turn-data-dollars/243931/|publisher=AdAge|accessdate=11 March 2014}}&lt;/ref&gt;  Stephen Taylor, VP &amp; Managing Director of Sojern’s international operations, has also made appearances on news sites, including British travel site Travel Bulletin, for which he wrote the articles “Knowing Your Traveller Through Big Data” and “Sojern - Industry Predictions 2014.”&lt;ref&gt;{{cite web|last=Taylor|first=Stephen|title=Knowing Your Traveller Through Big Data|url=http://www.travelbulletin.co.uk/travel-opinions/knowing-your-traveller-through-big-data-by-stephen-taylor-vp-md-international-of-sojern|publisher=Travel Bulletin|accessdate=12 March 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Taylor|first=Stephen|title=Sojern - Industry Predictions 2014|url=http://www.travelbulletin.co.uk/travel-opinions/sojern-industry-predictions-2014|publisher=Travel Bulletin|accessdate=12 March 2014}}&lt;/ref&gt;

==Achievements==

2013 [[Deloitte Fast 500|Deloitte Technology Fast 500]] List (#60)&lt;ref name=&quot;deloitte&quot; /&gt;

2013 Mediapost.com Digital Out-of-Home Awards: Finalist - Best Venue/Location Based Execution&lt;ref&gt;{{cite web|title=2013 Finalists|url=http://www.mediapost.com/digitaloutofhomeawards/finalists/|publisher=MediaPost}}&lt;/ref&gt;

2011 Technology Company of the Year – AIM Institute&lt;ref name=bizwire&gt;{{cite web|title=Sojern Named Technology Company of the Year by AIM Institute|url=http://www.businesswire.com/news/home/20110324005178/en/Sojern-Named-Technology-Company-Year-AIM-Institute#.Ux4wc-ddUhJ|publisher=Business Wire}}&lt;/ref&gt; 
 
2010 Edison Awards Winner – Media &amp; Visual Communications&lt;ref&gt;{{cite web|title=2010 Edison Award Winners|url=http://www.edisonawards.com/BestNewProduct_2010.php|publisher=Edison Awards}}&lt;/ref&gt;

2009 OnMedia Top 100 for Technology Innovation&lt;ref name=&quot;bizwire&quot; /&gt;

2009 HSMAI Advertising Award&lt;ref name=&quot;bizwire&quot; /&gt;

2008 OMMA Award for Online Advertising Creativity&lt;ref&gt;{{cite web|title=Sojern Wins 2008 OMMA Award for Online Advertising Creativity in Content Integration|url=http://www.reuters.com/article/2008/09/19/idUS175843+19-Sep-2008+BW20080919|publisher=Reuters}}&lt;/ref&gt;

==References==
&lt;references /&gt;


[[Category:Internet marketing]]
[[Category:Big data]]
[[Category:Companies based in San Francisco, California]]</text>
      <sha1>odaw9sl8wgl2msxhslm966xrfcht1bg</sha1>
    </revision>
  </page>
  <page>
    <title>VoloMetrix</title>
    <ns>0</ns>
    <id>42460055</id>
    <revision>
      <id>665262972</id>
      <parentid>657130603</parentid>
      <timestamp>2015-06-03T01:52:36Z</timestamp>
      <contributor>
        <ip>50.181.140.73</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6829">{{Orphan|date=April 2014}}

{{Infobox dot-com company
| name     = VoloMetrix, Inc.
| logo = [[File:VOLO logo.jpg|220px]]
| foundation       = 2011
| chairman         = Chris Brahm
| CEO              = Ryan Fuller
| location = [[Seattle, Washington]], US
| products         = [[Software as a service]]
| homepage         = [http://www.volometrix.com www.VoloMetrix.com]
}}

'''VoloMetrix, Inc.''' is an American enterprise software start-up based in [[Seattle, Washington]]. VoloMetrix offers a [[people analytics]] solution that combines data from collaboration platforms to create data visualizations and dashboards. At the end of April 2013, the company raised $3.3M in series A funding from [[Shasta Ventures]].&lt;ref&gt;{{cite web|author= |url=http://techcrunch.com/2013/04/24/enterprise-performance-analytics-startup-volometrix-raises-3-3m-from-shasta-ventures// |title=Enterprise Performance Analytics Startup VoloMetrix Raises $3.3M From Shasta Ventures|publisher=TechCrunch |date=2013-04-24}}&lt;/ref&gt; In October 2014, VoloMetrix announced a series B funding round with Shasta Ventures and Split Rock Partners that raised $12M.&lt;ref&gt;{{cite web|author= |url=http://money.cnn.com/news/newsfeeds/articles/marketwire/1150900.htm |title=VoloMetrix Raises $12 Million in Series B Funding to Accelerate Growth|publisher=CNN Money|date=2014-10-13}}&lt;/ref&gt;

== Company Team ==
The company was founded in 2011 by Ryan Fuller, the company's CEO, and Chris Brahm, a Senior Partner and Director at [[Bain &amp; Company]]. The VoloMetrix leadership team team includes Natalie McCullough (Chief Revenue Officer), Nimrod Vered (VP of Engineering), Chantrelle Nielsen (VP of Product), Ahmed Quadri (VP of Customer Solutions), Eileen Conway (VP of Marketing), and Peter Cullen (Chief Privacy Officer). Jim Simons, Managing Director of Split Rock Partners, joined the VoloMetrix Board of Directors following the series B funding round.

== Technology ==
VoloMetrix extracts and analyzes anonymous data from company email, calendar, social platforms and line-of-business applications. They enable executives, managers and individuals in any organization to clearly understand where their teams are focused and how they can work together most effectively through easy-to-use, interactive dashboards.&lt;ref&gt;{{cite web|author=Alan Pelz-Sharpe |url=https://451research.com/report-short?entityId=79539// |title=Startup VoloMetrix exposes social inefficiencies within the enterprise|publisher=451 Research |date=2013-12-05}}&lt;/ref&gt; The application and integrated professional services work together to diagnose and resolve company-wide issues such as tracking bottlenecks,&lt;ref&gt;{{cite web|author= Mike Vizard |url=http://www.saasintheenterprise.com/author.asp?section_id=3175&amp;doc_id=266974&amp;/// |title=An App to Track Bottlenecks |publisher=SaaS in the Enterprise|date=2013-08-23}}&lt;/ref&gt; and providing a more dynamic view of the organization than a classic org chart can provide.&lt;ref&gt;{{cite web|author=Judith Lamont|url=http://www.kmworld.com/Articles/Editorial/Features/On-the-cutting-edge-of-social-networking-89796.aspx?PageNum=2// |title=On the cutting edge of social networking|publisher=KM World|date=2013-05-28}}&lt;/ref&gt; VoloMetrix solutions can be used to change the way functions across the organization manage their business, including HR, Finance,&lt;ref&gt;{{cite web|author= Ritobaan Roy |url=http://www.cfo-insight.com/risk-management-it/it/do-your-employees-talk-to-each-other/// |title=Do Your Employees Talk to Each Other? |publisher=CFO Insight|date=2013-02-08}}&lt;/ref&gt; and IT.&lt;ref&gt;{{cite web|author= |url=http://www.itproportal.com/2014/02/06/elevating-the-cio-how-analytics-will-boost-enterprise-it/// |title=Elevating the CIO: How analytics will boost enterprise IT  |publisher=TechCrunch |date=2014-02-06}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Sharon Florentine  |url=http://www.cio.com/article/738987/How_Social_Analytics_Can_Improve_Enterprise_IT_Efficiency/// |title=How Social Analytics Can Improve Enterprise IT Efficiency |publisher=CIO.com|date=2013-08-30}}&lt;/ref&gt; Companies can fuel innovation and accelerate profitable growth by treating time as a scarce resource and being mindful about how they invest it.&lt;ref&gt;{{cite web|author= Michael Mankins, Chris Brahm, Gregory Caimi  |url=http://hbr.org/2014/05/your-scarcest-resource/// |title=Your Scarcest Resource |publisher=HBR|date=2014-05-01}}&lt;/ref&gt;

== Patent ==
In August 2014, VoloMetrix publicly announced that it had filed a patent for its proprietary technology and associated metrics.&lt;ref&gt;{{cite web|author= |url=http://www.marketwired.com/press-release/volometrix-files-patent-for-ground-breaking-predictive-people-analytics-software-1941853 |title=VoloMetrix Files Patent for Ground-Breaking Predictive People Analytics Software|publisher=Market Wired|date=2014-08-27}}&lt;/ref&gt; VoloMetrix's key organizational metrics include Organizational Load Index (OLI), Fragmentation, Network Efficiency Index, and other measures aimed at improving employee performance.

== Organizational Insights ==
VoloMetrix has gained attention from academic and corporate research projects focused on the significance of time use and internal networks within organizations. Features in [[The Economist]],&lt;ref&gt;{{cite web|author= |url=http://www.economist.com/news/business/21610237-businesses-must-fight-relentless-battle-against-bureaucracy-decluttering-company |title=Decluttering the company|publisher=The Economist |date=2014-08-14}}&lt;/ref&gt; [[Harvard Business Review]],&lt;ref&gt;{{cite web|author=Ryan Fuller |url=http://blogs.hbr.org/2014/08/3-behaviors-that-drive-successful-salespeople/ |title=3 Behaviors That Drive Successful Salespeople|publisher=Harvard Business Review |date=2014-08-20}}&lt;/ref&gt; and [[The Wall Street Journal]]&lt;ref&gt;{{cite web|author=Sue Shellenbarger |url=http://online.wsj.com/articles/new-office-flashpoint-who-gets-the-conference-room-1413307377 |title=New Office Flashpoint: Who Gets the Conference Room?|publisher=The Wall Street Journal |date=2014-10-15}}&lt;/ref&gt; have highlighted VoloMetrix's use of big data and people analytics applications to identify meaningful trends within large firms. Widely referenced metrics provided by VoloMetrix include:

*'''Organizational Load Index''': number of hours a person consumes from the rest of the organization based on meetings they organize and emails they send

*'''1:1 Manager Interactions''': number of hours per week spent in meetings involving only an individual and the individual's direct supervisor during a given time period

*'''Time in Meetings''': number of hours per week spent in meetings during a given time period

==References==
{{Reflist}}

==External links==
*[http://www.volometrix.com Official website]

[[Category:Big data]]
[[Category:Data management software]]
[[Category:Enterprise software]]
[[Category:Business software]]</text>
      <sha1>haavbt0t7jw0y7jqshdkjqn3abg85kq</sha1>
    </revision>
  </page>
  <page>
    <title>Ninja Metrics</title>
    <ns>0</ns>
    <id>42250580</id>
    <revision>
      <id>648017610</id>
      <parentid>617685065</parentid>
      <timestamp>2015-02-20T09:41:21Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>cleanup / infobox standardisation using [[Project:AWB|AWB]] (10833)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3077">{{Orphan|date=July 2014}}

{{Infobox dot-com company
| name     = Ninja Metrics
| logo     = [[File:Ninja Metrics Logo 2014.png]]
| company_type     = [[Privately held company|Private]]
| location         = [[Manhattan Beach, California]]
| key_people       = Dr. Dmitri Williams, CEO&lt;br /&gt;Dr. Jaideep Srivastava, CTO&lt;br /&gt;Fabian Schonholz, COO
| industry         = [[Big data]]
| products         = [[Web analytics]]&lt;br /&gt;[[Predictive analytics]]&lt;br /&gt;[[Mobile web analytics]]&lt;br /&gt;[[Social analytics]]
| num_employees    = 20
| url              = {{URL|http://www.ninjametrics.com/}} 
| launch_date      = 2009
}}

'''Ninja Metrics, Inc.''' is a [[Social analytics]] and [[Big data]] company based in [[Manhattan Beach, California]]. Its primary service measures social influence and provides predictive analytics for web and [[Mobile application software|mobile applications]].

==History==

Ninja Metrics was founded by Dr. Dmitri Williams (PhD, [[University of Michigan]]) and Dr. Jaideep Srivastava (PhD, [[University of Minnesota]]) in 2009 after being in development at University of Michigan labs for 2 years.&lt;ref&gt;Grayson, Katharine. [http://www.bizjournals.com/twincities/blog/in_private/2013/11/u-of-m-spin-off-ninja-metrics-raises.html &quot;U of M spin-off Ninja Metrics raises $2.8M&quot;], ''Minneapolis/St. Paul Business Journal'', Minneapolis, 12 November 2013. Retrieved on 19 March 2014.&lt;/ref&gt;

On July 17, 2013, Ninja Metrics raised $2.8 million in funding primarily from angels, super angels, the Harvard Business School Angels of Los Angeles, and the [[Tech Coast Angels]], including the latter's ACE fund.&lt;ref&gt;[http://www.finsmes.com/2013/11/ninja-metrics-closes-2-8m-funding.html &quot;NINJA METRICS CLOSES $2.8M IN FUNDING&quot;], ''FinSMEs'', 11 November 2013. Retrieved on 19 March 2014.&lt;/ref&gt;

The company’s current COO is Fabian Schonholz, and its board of directors include Mark Kolokotrones, Bob Hawk, Robin Kaminsky and Yuri Pikover.

==Katana® Social Analytics Engine==

Ninja Metrics launched its primary service called the Katana® Social Analytics Engine in 2013, a [[Cloud computing]] [[Social analytics]] platform for [[video game]]s.&lt;ref&gt;Hasson, Jessica. [http://www.marketwatch.com/story/ninja-metrics-launches-katana-analytics-engine-delivering-social-value-based-analytics-for-gaming-industry-2013-11-11 &quot;Ninja Metrics Launches Katana Analytics Engine, Delivering Social Value Based Analytics For Gaming Industry&quot;], ''[[MarketWatch]]'', Los Angeles, 11 November 2013. Retrieved on 19 March 2014.&lt;/ref&gt;

The platform measures the value of social contributions in [[Mobile application software|applications]] and games, provides projections for the outcomes to developers and offers tools to determine how effectively features facilitate engagement and social interaction.&lt;ref&gt;[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=225651493 &quot;Company Overview of Ninja Metrics, Inc.&quot;], ''Blomberg Businessweek''. Retrieved on 19 March 2014.&lt;/ref&gt;

==References==
&lt;references /&gt;

[[Category:Big data]]
[[Category:Web analytics]]</text>
      <sha1>h8akgqlctbp134ehziam38pkdlbza5a</sha1>
    </revision>
  </page>
  <page>
    <title>Groundhog Technologies</title>
    <ns>0</ns>
    <id>42873698</id>
    <revision>
      <id>660523198</id>
      <parentid>660523040</parentid>
      <timestamp>2015-05-03T01:33:47Z</timestamp>
      <contributor>
        <username>Stesmo</username>
        <id>98915</id>
      </contributor>
      <comment>Removed wikilink to dab page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5800">{{Infobox company
| name     = Groundhog Technologies, Inc.
| logo     = [[File:Groundhog Technologies Inc Logo, 2014.gif|150px]]
| caption= 
| type     = [[Privately held company|Private]]
| genre            =
| foundation       = 
2001| founder          = David Chiou
| location         = Cambridge, Massachusetts, U.S.
| locations        =
| incorporated     = 
| area_served      = 
| key_people       = 
| industry         = [[Telecommunication]]
| products         =
| services         = [[Geolocation]], Mobility Intelligence, Network Optimization
| revenue          = 
| operating_income =
| net_income       = 
| owner            =
| parent           =
| divisions        =
| subsid           =
| company_slogan   =
| website              = [http://www.ghtinc.com http://www.ghtinc.com]
}}
'''Groundhog Technologies''' is a privately held company founded in 2001 and is headquartered in Cambridge, Massachusetts, USA.  As a spin-off of [[MIT Media Lab]],&lt;ref&gt;[[MIT Media Lab#Spin-offs]]&lt;/ref&gt;&lt;ref&gt;[http://www.media.mit.edu/sponsorship/spin-offs] MIT Media Lab Spin-Offs&lt;/ref&gt; it was a semi-finalist in MIT's $50k Entrepreneurship Competition in 2000 and was incorporated the following year.&lt;ref&gt;[http://mitsloan.mit.edu/newsroom/50k/50kalumco.php 15 of the Top Companies Launched— More than 80 companies, 1600 employees, $4 billion in value], MIT Sloan Management&lt;/ref&gt;&lt;ref&gt;[http://www.bizjournals.com/boston/stories/2005/02/07/daily64.html Groundhog's Day], Boston Business Journal, Feb 2005&lt;/ref&gt; The company received the first round of financing from major Japanese corporations and their venture capital arms in November 2002: [[Marubeni]], Yasuda Enterprise Development and Japan Asia Investment Co.&lt;ref&gt;{{cite web|last=Miller|first=Jeff|title=A Grounded Enterprise|url=http://www.bizjournals.com/boston/blog/mass-high-tech/2004/07/a-grounded-enterprise-2004-07-19.html|publisher=Boston Business Journal|accessdate=July 19, 2004}}&lt;/ref&gt;&lt;ref&gt;[http://www.chubbybrain.com/companies/groundhog-technologies/investors-funding-history Investors and Funding History] ChubbyBrain&lt;/ref&gt; It received second round of financing in 2004 and since then has become self-sustainable.&lt;ref&gt;[http://www.bizjournals.com/boston/stories/2005/02/07/daily64.html Groundhog's Day], Boston Business Journal, Feb 2005&lt;/ref&gt;

The company’s products are built on top of its Mobility Intelligence Platform, which analyzes the locations, [[Quality of Experience]],&lt;ref&gt;[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=20462720] Bloomberg Businessweek, May 2014&lt;/ref&gt; context, and lifestyles of subscribers in mobile operator’s network.&lt;ref&gt;[http://universityspinoff.wordpress.com/2010/05/22/groundhog-technologies-mit/ Groundhog Technologies (MIT) selected by China Unicom]&lt;/ref&gt; The intelligence about [[geolocation]] is then applied to improve subscribers’ experience &lt;ref&gt;[http://www.lightreading.com/mobile/4g-lte/groundhog-boasts-3g-4g-oss-deal/d/d-id/707137 Groundhog Boasts 3G/4G OSS Deal] LightReading&lt;/ref&gt; and enable applications such as [[geomarketing]] and [[geotargeting]].&lt;ref&gt;[http://digitalshingle.mit.edu/node/401] MIT Digital Shingle Project&lt;/ref&gt;

==Core Technologies==
Groundhog Technologies launched its Mobility Intelligence platform based on [[Chaos Theory]] and multi-dimensional modeling.  The application of Chaos Theory gave rise to the company’s mathematical models of subscribers' mobility and usage behavior,&lt;ref&gt;{{cite web|title=Management system and method for wireless communication network and associated user interface|url=http://patents.justia.com/patent/7747254|website=Justia Patents|publisher=Justia Patents|accessdate=30 June 2014}}&lt;/ref&gt; which can be used for different applications such as by mobile operators to optimize networks according to the user demands.&lt;ref&gt;{{cite web|title=System with user interface for network planning and mobility management optimization in a mobile communication network and method thereof|url=http://patents.justia.com/patent/7561876|website=Justia Patents|publisher=Justia Patents|accessdate=30 June 2014}}&lt;/ref&gt;

[[File:Groundhog Technologies Chaos Theory.png|thumb|right|Human moving pattern in phase space]]
According to Chaos Theory, some seemly random or chaotic signals may be converted to analyze in [[phase space]] which can reveal the patterns behind it. The cases of most interest arise when the chaotic behavior shows patterns around an [[attractor]] in the phase space. Based on the attractor in the phase space, data can be utilized from different space, time, and individuals for modeling and indoor geolocation.

It is also found that the dimensional structure and characteristics of phase space can naturally neutralize the bias of positioning (based on techniques such as [[triangulation]] or trilateration) caused by reasons such as multipath. That is, although each input is biased in some way, the observation from different dimensions and angles are biased in different ways. Combining multi-dimensional input in the phase space, based on the [[Law of Large Numbers]] it can average out the bias with different samples through dimensions, time, and individuals.&lt;ref&gt;{{cite web|title=System for constructing a mobility model for use in mobility management in a wireless communication system and method thereof|url=http://patents.justia.com/patent/8031676|website=Justia Patents|publisher=Justia Patents|accessdate=30 June 2014}}&lt;/ref&gt;

==See also==
* [[Location-Based Services]]
* [[Geolocation software]]

==References==
&lt;references/&gt;

[[Category:Advertising]]
[[Category:Big data]]
[[Category:Geolocation]]
[[Category:Geomarketing research]]
[[Category:Mobile technology| ]]
[[Category:Mobile telecommunications| ]]
[[Category:Telecommunications]]
[[Category:Wireless locating]]</text>
      <sha1>tijp9743unbtmu4ssxg6z8pk7sho25a</sha1>
    </revision>
  </page>
  <page>
    <title>Virtuoso Universal Server</title>
    <ns>0</ns>
    <id>5758975</id>
    <revision>
      <id>665340862</id>
      <parentid>662918887</parentid>
      <timestamp>2015-06-03T16:02:45Z</timestamp>
      <contributor>
        <username>Janvlug</username>
        <id>392879</id>
      </contributor>
      <comment>New release (7.2.0) in February 2015</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17510">{{Multiple issues|
{{refimprove|date=October 2009}}
{{advert|date=March 2009}}
}}

{{Infobox software
| name                   = Virtuoso Universal Server
| logo                   = [[File:virtuoso-logo-sm.png]]
| screenshot             = [[File:conductor-sm.png|250px]]
| caption                = Virtuoso Conductor (Database Administration User Interface)
| developer              = [[OpenLink Software]]
| released               =
| latest release version = 7.2.0
| latest release date    = February 2015
| frequently updated     =
| programming language   = [[C (programming language)|C]]
| operating system       = [[Cross-platform]]
| size                   =
| genre                  = [[Triplestore]], [[Relational database management system|RDBMS]], [[Application server]], [[Web server]]
| license                = [[GPLv2]]&lt;ref&gt;{{cite web |url=http://virtuoso.openlinksw.com/dataspace/dav/wiki/Main/VOSLicense |title=Virtuoso Open-Source License Terms |author=OpenLink Software |accessdate=2011-03-30 }}&lt;/ref&gt; and proprietary
| website                = [http://virtuoso.openlinksw.com/ virtuoso.openlinksw.com]
}}

'''Virtuoso Universal Server''' is a [[middleware]] and [[database engine]] hybrid that combines the functionality of a traditional [[Relational database management system|RDBMS]], [[Object-relational database|ORDBMS]], [[Federated database system|virtual database]], [[Resource Description Framework|RDF]], [[XML]], [[free-text]], [[application server|web application server]] and [[file server]] functionality in a single system. Rather than have dedicated servers for each of the aforementioned functionality realms, Virtuoso is a &quot;universal server&quot;; it enables a single [[Thread (computer science)|multithreaded]] server [[process (computing)|process]] that implements multiple protocols. The [[open source]] edition of Virtuoso Universal Server is also known as '''OpenLink Virtuoso'''. The software has been developed by [[OpenLink Software]] with [[Kingsley Uyi Idehen]] and [[Orri Erling]] as the chief [[software architect]]s.

== Database structure ==

=== Core database engine ===
Virtuoso provides an extended object-relational model, which combines the flexibility of relational access with inheritance, run time data typing, late binding, and [[Identity resolution|identity based access]]. Virtuoso Universal Server database includes physical file and in memory storage and operating system processes that interact with the storage. There is one main process, which has listeners on a specified port for [[Hypertext Transfer Protocol|HTTP]], [[SOAP]], and other protocols.

====Architecture====
Virtuoso is designed to take advantage of operating system threading support and multiple CPUs. It consists of a single process with an adjustable pool of threads shared between clients. Multiple threads may work on a single index tree with minimal interference with each other. One cache of database pages is shared among all threads and old dirty pages are written back to disk as a background process.

The database has at all times a clean checkpoint state and a delta of committed or uncommitted changes to this checkpointed state. This makes it possible to do a clean backup of the checkpoint state while transactions proceed on the commit state.

A transaction log file records all transactions since the last checkpoint. Transaction log files may be preserved and archived for an indefinite time, providing a full, recoverable history of the database.

A single set of files is used for storing all tables. A separate set of files is used for all temporary data. The maximum size of a file set is 32 terabytes, for 4G × 8K pages.

====Locking====
Virtuoso provides dynamic locking, starting with row level locks and escalating to page level locks when a cursor holds a large percentage of a page's rows or when it has a history of locking entire pages. Lock escalation only happens when no other transactions hold locks on the same page, hence it never deadlocks. Virtuoso SQL provides means for exclusive read and for setting transaction isolation.

====Transactions====
All four levels of isolation are supported: Dirty read, read committed, repeatable read and serializable. The level of isolation may be specified operation by operation within a single transaction. Virtuoso can also act as a [[resource manager]] and/or transaction coordinator under Microsoft's Distributed Transaction Coordinator ([[Distributed Transaction Coordinator|MS DTC]]) or the [[X/Open|XA]] [[standardization|standard]].

====Data integrity====
Virtuoso [[Object-relational database|ORDBMS]] database supports [[entity integrity]] and [[referential integrity]]. Virtuoso ensures that relationships between records in related tables are valid by enforcing [[referential integrity]].  Integrity constraints include:
* [[Null (SQL)|NOT NULL]] - Within the definition of a table, Virtuoso allows data to contain a NULL value. This NULL value is not really a value at all and is considered an absence of value. The constraint of NOT NULL forces a value to be given to a column.
* [[Unique key|Unique Key]] - Uniqueness for a column or set of columns means that the values in that column or set of columns must be different from all other columns or set of columns in that table. A unique key may contain NULL values since they are by definition a unique non-valued value.
* [[Unique key|Primary Key]] - Primary key are much like unique keys except that they are designed to uniquely identify a row in a table. They can consist of a single column or multiple columns. The primary key cannot contain a NULL value.
* [[Check Constraint|CHECK Constraint]] - Virtuoso provides on a column an integrity constraint that requires certain conditions to be met before the data is inserted or modified. If the checks are not satisfied then the transaction cannot be completed.

====Data dictionary====
Virtuoso stores all its information about all user objects in the database in the system catalog tables designated by db.dba*.

=== Components and files ===

==== Components ====
Virtuoso is made up of client and server components. These components typically communicate with a local or remote Virtuoso server, which include:
* Virtuoso Drivers for [[Open DataBase Connectivity|ODBC]], [[Java Database Connectivity|JDBC]], [[ADO.NET]] and [[OLE DB]]
* Conductor, a web-based database administration user interface
* ISQL (Interactive SQL) and ISQO Utilities
* Documentation and Tutorials
* Samples

Installations come with two databases: a default and a demo database.

== History ==
The Virtuoso project was born in 1998 from a merger of the '''OpenLink''' data access [[middleware]] and '''Kubl''' RDBMS.

=== Kubl RDBMS ===
The Kubl [[Object-relational database|ORDBMS]] was one of a list of [[Relational database management system|relational database systems]] with roots in [[Finland]]. This list also includes [[MySQL]], [[InnoDB]], and [[solidDB|Solid RDBMS]]/[[Solid Technologies]].

As is the case with most technology products, key personnel behind OpenLink Virtuoso, [[InnoDB]], and [[Solid Technologies|Solid]] share periods of professional overlap that provide noteworthy insight into the history of database technology development in Finland.  [[Heikki Tuuri]] (creator of [[InnoDB]]), [[Ora Lassila]] (W3C and Nokia Research, a technology lead and visionary in the areas [[Resource Description Framework|RDF]] and [[Semantic Web]] in general alongside [[Tim Berners-Lee]]), and [[Orri Erling]] (Virtuoso Program Manager at OpenLink Software) all worked together in a startup company called [[Entity Systems]] in [[Finland]] - where they were developing [[Common Lisp]] and [[Prolog]] development environments for the early generation of [[Personal computer|PC]]'s circa. 1986–88.

Later, Orri Erling worked with [[VIA International]], the developer of [[VIA/DRE]] in designing a [[Lisp (programming language)|LISP]] based [[Object-oriented programming|object-oriented]] [[data access]] layer atop the company's [[Database management system|DBMS]] product. The core development team of VIA, following the company's demise in 1992, went on to found [[Solid Technologies]] under the direction of [[Artturi Tarjanne]].

[[Heikki Tuuri]] worked at [[Solid Technologies|Solid]] for a while before starting his own [[database]] development project which became [[InnoDB]] (acquired by [[Oracle Corporation|Oracle]] in 2005).

Orri Erling started his own DBMS development work in 1994, which was to become [[Kubl]]. Development of Kubl was initially financed by [[Infosto Group]], publisher of Finland's largest [[free ads paper]], as part of their in-house software development project for their [[Consumer-to-consumer|on-line services]]. The on-line version of ''[[Keltainen Pörssi]]'' was at one time said to be Finland's most popular web site with 500,000 registered users. The Kubl database was prominently displayed in a ''&quot;Powered by Kubl&quot;'' logo on the search results.

A free trial version of Kubl was made available for download on November 7, 1996.&lt;ref&gt;{{cite web |url=http://groups.google.com/group/comp.os.linux.announce/browse_thread/thread/1f16aee3485e6ec4/584d1a35614d67e1#584d1a35614d67e1 |title=Kubl RDBMS Free Trial Downloading Now |date=7 November 1996 |accessdate=2010-02-03 }}{{cite newsgroup|newsgroup=comp.os.linux.misc|message-id=pgpmoose.199611072316.10073@liw.clinet.fi|title=Kubl RDBMS Free Trial Downloading Now |date=7 November 1996 }}&lt;/ref&gt;

Kubl was marketed as a high performance lightweight database for [[embedded software|embedded]] use; the development aim was to achieve top scores in [[Transactions Per Second]] tests.&lt;ref&gt;[http://sal.linet.gr.jp/H/1/KUBL.html]&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://groups.google.com/group/comp.databases/browse_thread/thread/2460a5cf7b13e6b7/31b04e5457c76e66#31b04e5457c76e66 |title=
DBMS Benchmark code? Who's fastest? |date=11 April 1996 |accessdate=2010-02-03 }}&lt;/ref&gt; Pricing of the product was especially favorable to [[Linux]] users with a Linux license priced at $199.&lt;ref&gt;{{cite web |url=http://groups.google.com/group/hun.lists.mlf.linux/browse_thread/thread/c3c8c5a8f23c9ace/de60d5e2abbfa638?q=&amp;rnum=32#de60d5e2abbfa638 |title=Kubl RDBMS for Linux |date=12 November 1997 |accessdate=2010-02-03 }}&lt;/ref&gt;

Kubl became the cornerstone of OpenLink Virtuoso, after the technology paths of [[Kingsley Uyi Idehen]] and [[Orri Erling]] crossed in 1998, leading to the acquisition of Kubl by [[OpenLink Software]].

== Functionality realms ==
Virtuoso's functionality covers a broad range of traditionally distinct realms in a single product offering. These functional realms include:
* [[Object-relational database]] engine for ([[SQL]], [[XML]], [[Resource Description Framework|RDF]] and [[text file|plain text]])
* [[Web service]]s [[computing platform]]
* [[World Wide Web|Web]] [[application server]]
* [[Web content management system]] (WCMS)
* [[Network News Transfer Protocol|NNTP]]-based Discussion Management
* [[Replication (computer science)|Replication]] of Homogeneous and Heterogeneous Data
* [[Mail Storage]] Sink and ([[Post Office Protocol|POP3]]) Service Proxy
* [[DataPortability]]

=== Protocols implemented ===
Virtuoso supports a broad range of industry standard Web &amp; Internet protocols that includes:
[[Hypertext Transfer Protocol|HTTP]], [[WebDAV]], [[CalDAV]], [[CardDAV]], [[SOAP]], [[Universal Description Discovery and Integration|UDDI]], [[Web Services Description Language|WSDL]], [[WS-Policy]], [[WS-Security]], [[WS-ReliableMessaging]], [[WS-Routing]], [[WS-Referral]], [[WS-Attachment]], [[Business Process Execution Language|WS-BPEL]], [[SyncML]], [[GData]], [[SPARQL]], [[SPARUL]], [[Network News Transfer Protocol|NNTP]]

=== API support ===
For the database application developer and systems integrator, Virtuoso implements a variety of industry standard data access APIs (client and server) that includes:
[[Open Database Connectivity|ODBC]], [[Java Database Connectivity|JDBC]], [[OLE DB]], [[ADO.NET]], [[ADO.NET Entity Framework]], [[XML for Analysis|XMLA]]

=== Content syndication and interchange format support ===
For the Web application developer and content syndicate(s) publishers, and consumers, Virtuoso supports standards such as:
[[Atom (standard)|Atom]], [[RSS|RSS 2.0]], [[RSS|RSS 1.0]], [[OPML]], [[XBEL]], [[FOAF (software)|FOAF]], [[Semantically-Interlinked Online Communities|SIOC]]

== Query language support ==
[[SQL]], [[SPARQL]] (with numerous extensions), [[XQuery]] (implementation of Core functions library is seriously incomplete), [[XPath]] (1.0 only), [[XSL Transformations|XSLT]] (1.0 only)

=== Schema definition language support ===
[[SQL]]'s [[Data Definition Language]], [[XML Schema (W3C)|XML Schema]]

== Usage scenarios ==
Virtuoso is a solution for the following [[system integration]] challenges:
* [[Enterprise Information Integration]] (EII)
* Programming Language Independent [[Web application]] deployment
* [[Monolithic application]] decomposition that leverages the principles of [[service-oriented architecture]]
* [[Web service]] based [[enterprise application integration]] via a significant amount of [[WS-*]] protocols support
* [[Business process management]] via [[Business Process Execution Language|BPEL]]
* [[Semantic Web]] Data Spaces Generation
* Deployment Platform for injecting RDF-based [[Linked Data]] into the Semantic Data Web

== Related technology areas ==

=== Data management ===
* [[Relational database management system]]
* [[List of relational database management systems]]
* [[Comparison of object-relational database management systems]]
* [[Comparison of relational database management systems]]

=== Enterprise application, information, and data integration ===
* [[Web 2.0]]
* [[Enterprise service bus]]
* [[Service-oriented architecture]]
* [[Enterprise application integration]]
* [[Data integration]]
* [[Open Semantic Framework]]
* [[Web service]]
* [[Semantic Web]]
* [[Comparison of business integration software|Business Integration Severs Comparison Matrix]]

== Related products and tools ==
In addition to Virtuoso, OpenLink Software produces several related tools and applications.
* [[OpenLink Data Spaces]] a Virtuoso based platform  for cost-effective creation and management of [[Semantic Web]] / [[Linked Data]] Web presence. It provides a data junction box for integrating data across third party [[Social network service]], [[Blog]], [[File sharing]], Shared &amp; [[Social bookmarking]], [[Wiki]], [[E-mail]], Photo Sharing, [[RSS|RSS 2.0]], [[Atom (standard)|Atom]], and [[RSS|RSS 1.1]] Content Aggregation services. In addition, to its third party integration functionality, it also includes its own rich collection of [[Linked Data]] compliant distributed collaborative applications, across each of the aforementioned Web application realms.
* [[Universal Data Access Drivers]] - High-performance data access drivers for [[Open Database Connectivity|ODBC]], [[Java Database Connectivity|JDBC]], [[ADO.NET]], and [[OLE DB]] that provide transparent access to enterprise databases across multiple platforms and databases.

== Platforms ==
Virtuoso is supported on a number of 32- &amp; 64-bit platforms including cross-platform [[Microsoft Windows|Windows]], UNIX ([[HP-UX|HP]], [[IBM AIX|AIX]], [[Solaris (operating system)|Sun]], DEC, BSD, [[UnixWare|SCO]]), Linux ([[Red Hat Linux|Red Hat]], [[SUSE Linux distributions|SUSE]]), and [[Mac OS X]].

== Licensing ==
In April 2006, an [[open source]] version of Virtuoso was made available under the [[GNU General Public License]] v2.
The software is now available in Commercial and Open Source license variants.&lt;ref&gt;{{cite web |url=http://www.openlinksw.com/press/VOSPressRelease.htm |title=Open Source Edition of OpenLink Virtuoso, Unleashed! |date=11 April 2006 |author=OpenLink Software |accessdate=2010-02-03 }}&lt;/ref&gt;

==References==
{{Reflist}}

== External links ==
{{Portal|Free software}}
*[http://virtuoso.openlinksw.com/ Main Virtuoso Web Site]
*[http://sourceforge.net/projects/virtuoso/ OpenLink Virtuoso (Open-Source Edition)] at [[SourceForge]]
*[https://github.com/openlink/virtuoso-opensource OpenLink Virtuoso (Open-Source Edition)] at [[GitHub]]

[[Category:Atom (standard)]]
[[Category:Big data]]
[[Category:Client-server database management systems]]
[[Category:Column-oriented DBMS software for Linux]]
[[Category:Cross-platform free software]]
[[Category:Cross-platform software]]
[[Category:Database engines]]
[[Category:Distributed computing architecture]]
[[Category:Document-oriented databases]]
[[Category:Enterprise application integration]]
[[Category:Free database management systems]]
[[Category:Free file sharing software]]
[[Category:Free software programmed in C]]
[[Category:Free web server software]]
[[Category:FTP server software]]
[[Category:Message-oriented middleware]]
[[Category:Metadata]]
[[Category:Middleware]]
[[Category:NewSQL]]
[[Category:NoSQL]]
[[Category:Online databases]]
[[Category:ORDBMS software for Linux]]
[[Category:OS X database-related software]]
[[Category:Products introduced in 1998]]
[[Category:Relational database management systems]]
[[Category:RSS]]
[[Category:Semantic Web]]
[[Category:SQL data access]]
[[Category:Structured storage]]
[[Category:Triplestores]]
[[Category:Unix Internet software]]
[[Category:Unix network-related software]]
[[Category:Web services]]
[[Category:Windows database-related software]]
[[Category:XML software]]
[[Category:XSLT processors]]</text>
      <sha1>4cdbf5wjty52ii4xwppt9iem14nezzw</sha1>
    </revision>
  </page>
  <page>
    <title>Attivio</title>
    <ns>0</ns>
    <id>32827590</id>
    <revision>
      <id>636089053</id>
      <parentid>633662263</parentid>
      <timestamp>2014-11-30T23:06:39Z</timestamp>
      <contributor>
        <username>Meatsgains</username>
        <id>15420856</id>
      </contributor>
      <comment>removed [[Category:Companies founded in 2007]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3564">{{advert|date=November 2011}}
{{Infobox company
| logo	       = [[File:attivio logo.png|230px]]
| name = Attivio, Inc.
| type = [[Private company|Private]]
| foundation = 2007
| location = [[Newton, Massachusetts|Newton]], [[Massachusetts]], [[USA]]
| area_served = [[Americas]] &lt;br /&gt; [[Europe]] &lt;br /&gt; [[Asia]] &lt;br /&gt; [[Australia]] &lt;br /&gt; [[Middle East]] &lt;br /&gt; Africa
| key_people = Ali Riaz (CEO)&lt;br /&gt;
Sid Probstein (CTO)&lt;br /&gt;
Alan Cooke (CFO &amp; General Counsel)&lt;br /&gt;
Stephen Baker (COO)
| industry = [[Information technology]] &lt;br /&gt; [[Information access]]
| products = Active Intelligence Engine
| homepage = http://www.attivio.com
}}

'''Attivio''', Inc., is a privately held [[Newton, Massachusetts]]-based enterprise software company that produces and sells a [[Unified Information Access|unified information access]] platform that lets users find all types of information with a single query and analyze data extracted from text along with traditional data.

Attivio was founded in 2007, and in 2012 raised $34 million in funding from Oak Investment Partners and in 2013 raised $8 million from General Electric Pension Trust.&lt;ref&gt;{{Citation
|title=Attivio Raises $8M From GE Pension Trust
|date=2013-01-07
|url=http://www.pehub.com/2013/01/attivio-raises-8m-from-ge-pension-trust/
|accessdate=2014-06-02|publisher=pehub.com}}&lt;/ref&gt;

Attivio's core product is the Active Intelligence Engine (AIE),&lt;ref&gt;{{Citation
|title=Oracle Competitor Attivio Appoints New COO
|date=2014-04-10
|url=http://www.bizjournals.com/boston/blog/techflash/2014/04/oracle-competitor-attivio-appoints-new.html
|accessdate=2014-06-02|publisher=bizjournals.com}}&lt;/ref&gt; a software platform that unites core capabilities offered separately by [[enterprise search]], [[business intelligence]], [[data warehousing]], [[process automation]] and [[business analytics]]&lt;ref&gt;{{Citation
|title=This May Be the Most Vital Use Of &quot;Big Data&quot; We've Ever Seen
|date=2013-07-12
|url=http://www.fastcolabs.com/3014191/this-may-be-the-most-vital-use-of-big-data-weve-ever-seen
|accessdate=2014-06-02|publisher=fastcolabs.com}}&lt;/ref&gt; to deliver complete information to people, automate business processes and enrich user experience.

AIE integrates structured data and unstructured content in a single index so it can be searched, manipulated and analyzed comprehensively.&lt;ref&gt;{{Citation
|title=Attivio Applies Predictive Analytics to Indexed Data
|date=2014-02-12
|url=http://www.itbusinessedge.com/blogs/it-unmasked/attivio-applies-predictive-analytics-to-indexed-data.html
|accessdate=2014-06-02|publisher=itbusinessedge.com}}&lt;/ref&gt; Both search queries and SQL can be used to retrieve data, allowing a full range of query types, from &quot;fuzzy&quot; queries that focus on relevance to highly precise queries.&lt;ref&gt;{{Citation
|title=Big Data Startup Attivio Takes On Big Rivals
|date=2013-02-04
|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-startup-attivio-takes-on-big-rivals/d/d-id/1108493
|accessdate=2014-06-02|publisher=informationweek.com}}&lt;/ref&gt; AIE can be used to build business-related applications that depend on access to information from diverse repositories; common applications include customer experience management, enterprise performance management, IT incident management, SharePoint integration, social brand management and risk management.

==References==
{{reflist}}

[[Category:Software companies based in Massachusetts]]
[[Category:Newton, Massachusetts]]
[[Category:Search engine software]]
[[Category:Big data]]
[[Category:Business software companies]]</text>
      <sha1>mx2ldv35bi29qpfdf4d093frpdtq5lc</sha1>
    </revision>
  </page>
  <page>
    <title>Datameer</title>
    <ns>0</ns>
    <id>29856124</id>
    <revision>
      <id>646315600</id>
      <parentid>644138874</parentid>
      <timestamp>2015-02-09T08:05:48Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes, added Empty section (1) tag using [[Project:AWB|AWB]] (10820)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5731">{{Infobox company
|name             = Datameer
| logo     =

|industry         = [[Data analytics|Big Data Analytics]]
|founder          = Stefan Groschupf ([[CEO]])
|location_city    = [[San Francisco, California]],
|num_employees    = 101
|homepage         = {{URL|http://www.datameer.com|Datameer.com}}
}}

'''Datameer, Inc.''' is a [[big data]] Analytics and [[Data visualization|Visualization]] company based in [[San francisco|San Francisco]], [[California]]. Datameer offers self-service and schema-free Big Data analytics application for Hadoop. Founded by Hadoop veterans in 2009, Datameer scales up to thousands of nodes and is available for all major Hadoop distributions.

Datameer specializes in analysis of large volumes of data&lt;ref&gt;{{cite web|url=http://radar.oreilly.com/2010/04/big-data-tool-for-business-analysts.html |title='&amp;#39;O'Reilly Radar, April 19, 2010 |publisher=Radar.oreilly.com |date=2010-04-19 |accessdate=2010-12-04}}&lt;/ref&gt; for business users of Apache [[Hadoop]].&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/ |title=Apache Hadoop |publisher=Hadoop.apache.org |date= |accessdate=2010-12-04}}&lt;/ref&gt; The company's product, Datameer Analytics Solution (DAS), is a [[business integration]] platform for Hadoop and includes data source integration, an analytics engine with a spreadsheet interface designed for business users with over 200 analytic functions and visualization including reports, charts and dashboards. DAS is available for major Hadoop distributions including [[Apache Software Foundation|Apache]], [[Cloudera]], EMC Greenplum HD, IBM BigInsights, MapR, [[Yahoo!]] and Amazon.

==Company History and Operations==
The company was founded in 2009.&lt;ref&gt;{{cite web|last=Rao |first=Leena |url=http://www.techcrunchit.com/2010/04/13/datameer-raises-2-5-million-for-apache-hadoop-based-analytics-platform/ |title='&amp;#39;TechCrunchIT'&amp;#39;, April 14, 2010 |publisher=Techcrunchit.com |date=2010-04-13 |accessdate=2010-12-04}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Vance |first=Ashlee |url=http://bits.blogs.nytimes.com/2010/04/22/start-up-goes-after-big-data-with-hadoop-helper/?pagemode=prin |title='&amp;#39;The New York Times'&amp;#39;, April 22, 2010 |publisher=Bits.blogs.nytimes.com |date=2010-04-22 |accessdate=2010-12-04}}&lt;/ref&gt; Datameer announced the general availability of DAS in October 2010 and is currently on its Version 2.1 release.&lt;ref&gt;[http://www.datameer.com/blog/uncategorized/datameer-2-1-and-the-datameer-analytics-app-market-available-now.html Datameer Press Release, November 16, 2012]&lt;/ref&gt;

Datameer was voted the second most promising [[cloud computing]] company at [[GigaOM]]'s Structure 2010 LaunchPad&lt;ref&gt;{{cite web|url=http://gigaom.com/2010/06/23/structure-2010-launchpad-cloudswitch-wins/ |title='&amp;#39;GigaOM'&amp;#39;, June 23, 2010 |publisher=Gigaom.com |date=2010-06-23 |accessdate=2010-12-04}}&lt;/ref&gt; and [[Information Today, Inc.|KMWorld]] named DAS a Trend-Setting Product of 2010.&lt;ref&gt;{{cite web|url=http://www.kmworld.com/Articles/Editorial/Feature/KMWorld-Trend-Setting-Products-of-2010-69565.aspx |title='&amp;#39;KMWorld'&amp;#39;, August 31, 2010 |publisher=Kmworld.com |date= |accessdate=2010-12-04}}&lt;/ref&gt; Datameer has also been recognized as a Dow Jones VentureWire Start-Up to Watch.&lt;ref&gt;{{cite web|last=Capital |first=Venture |url=http://blogs.wsj.com/venturecapital/2010/10/12/venturewires-fastech-conference-to-spotlight-most-promising-start-ups/ |title='&amp;#39;The Wall Street Journal'&amp;#39;, October 12, 2010 |publisher=Blogs.wsj.com |date=2010-10-12 |accessdate=2010-12-04}}&lt;/ref&gt; Datameer was named a 2014 Data Discovery Leader by [[GigaOm]]&lt;ref&gt;{{cite web |url=http://research.gigaom.com/report/sector-roadmap-data-discovery-in-2014/ |title=Sector RoadMap: data discovery in 2014 |last1=Brust |first1=Andrew J |date=19 February 2014}}&lt;/ref&gt;

On May 16, 2011, Datameer announced a $9.25M B-financing round led by venture capital firm Kleiner Perkins, Caufield and Byers.&lt;ref&gt;{{cite web|url=http://techcrunch.com/2011/05/16/kleiner-perkins-leads-9m-round-in-apache-hadoop-based-analytics-platform-datameer |title= Kleiner Perkins Leads 9M round In Apache Hadoop-Based Analytics Platform |publisher=www.techcrunch.com |date=2011-05-16 |accessdate=2011-06-04}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.nytimes.com/external/venturebeat/2011/05/16/16venturebeat-datameer-snags-925m-more-to-analyze-massive-87546.html?partner=rss&amp;emc=rss |title= Datameer Snags $9.25M More to Analyze Massive Amounts of Data |publisher=www.nytimes.com |date=2011-05-16 |accessdate=2011-06-04}}&lt;/ref&gt;

On December 18, 2013, Datameer announced a $19M D-financing round by  [[Software AG]], [[Redpoint Ventures]], [[Workday]], [[Citi | Citi Ventures]], [[Kleiner Perkins Caufield &amp; Byers| Kleiner Perkins Caufield &amp; Byers (KPCB)]], and Next World Capital&lt;ref&gt;{{cite web|url=http://techcrunch.com/2013/12/18/datameeer-raises-19m-as-market-for-hadoop-and-big-data-analytics-hits-an-inflection-point/ | title= Datameer Raises $19M As Market For Hadoop And Big Data Analytics Hits An Inflection Point}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://gigaom.com/2013/12/18/hadoop-analytics-startup-datameer-raises-19m/ | title= Hadoop analytics startup Datameer raises $19M}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://venturebeat.com/2013/12/18/datameer-picks-up-19m-as-companies-seek-analytics-along-with-hadoop/ | title= Datameer picks up $19M to help companies do analytics along with Hadoop}}&lt;/ref&gt;

==See also==

{{Empty section|date=February 2015}}

==References==
{{reflist}}

==External links==
*[http://www.datameer.com/ Datameer website]

[[Category:Business software companies]]
[[Category:Software companies based in California]]
[[Category:Cloud computing providers]]
[[Category:Big data]]
[[Category:Enterprise Software]]</text>
      <sha1>q0kspdjj0t1j6i2pzy1zr3ekqaxhy8v</sha1>
    </revision>
  </page>
  <page>
    <title>Medopad</title>
    <ns>0</ns>
    <id>43097720</id>
    <revision>
      <id>672891438</id>
      <parentid>672891339</parentid>
      <timestamp>2015-07-24T15:30:10Z</timestamp>
      <contributor>
        <ip>84.92.211.94</ip>
      </contributor>
      <comment>/* Involved parties and clients */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12612">{{Infobox company
| name             = Medopad Ltd
| logo             = [[File:Medopadlogo2.png|Medopad Logo|200px]]
| type             = [[Privately held company|Private]]
| industry       = [[healthcare]], [[big data]], [[small data]], [[mobile phone|mobile]], [[Internet of Things|internet of things]], [[Pharmaceutical industry|pharma]], [[wearables]]
| foundation       = 2011
| founder          = [[Dan Vahdat]] and Dr Rich Khatib
| location_city    = [[London]], UK
| homepage         = {{URL |www.medopad.com}}
}}

'''Medopad Ltd''' is a British healthcare technology company headquartered in [[London]], UK, with international offices in [[Salt Lake City]], USA, and [[Dubai]], UAE. It offers a suite of [[CE marking|CE]] certified [[iPad]] [[Application software|applications]] that integrate health data from existing hospital databases and securely sends it to healthcare professionals' [[mobile devices]].&lt;ref name=&quot;New Scientist&quot;&gt;[http://www.newscientist.com/article/mg22229734.700-doctors-with-ipads-could-transform-hospital-care.html#.U564po1dU4Q New Scientist], Retrieved 16 June 2014&lt;/ref&gt; The solution is registered with FDA and [[Health Insurance Portability and Accountability Act|HIPAA]] compliant. Primary clients are large hospital groups and large pharmaceutical companies, and since 2012, Medopad has been a supplier to the [[National Health Service (England)|NHS]].

==Adoption==

[[BMI Healthcare]], the largest private hospital group in the UK with nearly 70 hospitals,&lt;ref name=&quot;BMI Healthcare&quot;&gt;[http://www.bmihealthcare.co.uk/hospital BMI Healthcare], Retrieved 16 June 2014&lt;/ref&gt; has been the first to work with the company,&lt;ref name=&quot;Healthinvestor&quot;&gt;{{Cite web | title =BMI pilots ‘Medopad’ across hospital group  | url = http://www.healthinvestor.co.uk/ShowArticle.aspx?ID=2852
| accessdate=19 June 2014}}&lt;/ref&gt;&lt;ref name=&quot;Health Investor Power 50 2013&quot;&gt;{{Cite web | title =Health Investor Power 50 2013| url=http://www.healthinvestor.co.uk/pdfs/P50Brochure_web.pdf Health Investor Power 50 2013| accessdate= 17 June 2014}}&lt;/ref&gt; and Medopad is also an [[National Health Service (England)|NHS]] supplier.&lt;ref name=&quot;Informationweek&quot;&gt;{{Cite web | title =NHS Moves Slowly Toward Mobile Health IT |  url = http://www.informationweek.co.uk/healthcare/mobile-wireless/nhs-moves-slowly-toward-mobile-health-it/240152055| accessdate=17 June 2013}}&lt;/ref&gt; The company was named one of the top 35 European startups to watch by tech.eu in June 2014.&lt;ref name=&quot;Tech.eu&quot;&gt;{{Cite web | title =Health 2.0 in Europe – Here’s 35 startups to watch |  url = http://tech.eu/features/1472/health-startup-europe/| accessdate=16 June 2014}}&lt;/ref&gt; In November 2014, Medopad voted best medical app of the year by [[MEDICA]] 2014.&lt;ref name=&quot;medica-tradefair.com&quot;&gt;http://www.medica-tradefair.com&lt;/ref&gt; BMI Healthcare Medical Director Mark Ferreira said that Medopad is &quot;intuitive, and it kind of works the way doctors think.&quot; &lt;ref name=autogenerated1&gt;{{Cite web | title =Doctors with iPads could transform hospital care| url=http://www.newscientist.com/article/mg22229734.700-doctors-with-ipads-could-transform-hospital-care.html#.U570H41dU4Q| accessdate= 16 June 2014}}&lt;/ref&gt;

==Awards and recognition==

* In January 2015, Medopad named most innovative small business in London &lt;ref&gt;http://www.competition.greatbusiness.gov.uk/&lt;/ref&gt;
* In December 2014, the company valued around $80m in latest funding deal &lt;ref&gt;http://www.telegraph.co.uk/finance/businessclub/technology/11291181/App-for-doctors-Medopad-valued-at-50m-in-latest-funding-deal.html&lt;/ref&gt;
* In November 2014, Medopad voted best medical app of the year by [[MEDICA]] 2014&lt;ref name=&quot;medica-tradefair.com&quot;/&gt;
* In November 2014, Medopad listed among the top 100 global digital health companies &lt;ref&gt;http://www.thejournalofmhealth.com/#!digital-health-100/c1rdu&lt;/ref&gt;

==Technology and products==

Medopad allows hospitals to pool their patient data into a single platform so it can be served up to doctors on apps and mobile devices in real-time. Healthcare professionals can securely access lab results, images, clinical notes, and [[primary care]] data via [[Ipad|iPads]] and other mobile devices.&lt;ref name=&quot;University of Oxford’s Tata Idea Idol Competition&quot;&gt;{{Cite web | title =University of Oxford’s Tata Idea Idol Competition |  url = http://www.stacyblackman.com/tag/said-business-school/| accessdate=17 June 2014}}&lt;/ref&gt; In November 2013, Medopad was the first regulatory approved enterprise mobile health solution and received CE approval from the UK Medicines &amp; Healthcare Products Regulatory Agency.&lt;ref name=&quot;Health Investor&quot;&gt;{{Cite web | title =Mobile health provider becomes first to receive CE approval |  url = http://www.healthinvestor.co.uk/ShowArticle.aspx?ID=3072&amp;AspxAutoDetectCookieSupport=1| accessdate=16 June 2014}}&lt;/ref&gt;

Some of the clinical applications that Medopad includes are Patient Clinical Notes by voice recognition or typing, Scheduling, Lab Results, Image Viewer to view [[X-ray]]s to CT scans, Support Documents to digitalise all paper documents or to take photos, Video Conferencing, Primary Records, Realtime Vitals, Demographics and Contact Information, [[Apple Inc|Apple]] HealthKit integration, Realtime Data Stream, Traffic Light to sort and prioritise patients, Admission, and access to more third party applications integrated into Medopad through the Clinical App Store.&lt;ref name=autogenerated1 /&gt;&lt;ref name=&quot;LLGA 2013&quot;&gt;{{Cite web | title =Empowering Mobile Health| url=http://www.llga.org/SolutionHistory?sid=77| accessdate= 19 June 2014}}&lt;/ref&gt; The Medopad software platform allows programmers and inventors to build clever apps around hospital big data, wearable devices and sensors that can combine data about physical health, activity and location.

[[File:Medopad at CeBIT 2014.jpg|thumb|Medopad in the [[Vodafone]] pavilion at CeBIT 2014]]
Medopad launched the first two enterprise integrated [[Google Glass]] apps in 2013&lt;ref name=&quot;Medopad Media&quot;&gt;{{Cite web | title =Medopad Media| url=http://medopad.com/medopad_Ltd/Media.html| accessdate= 17 June 2014}}&lt;/ref&gt; and announced them to the world at [[CeBIT]] 2014 in Germany.&lt;ref name=autogenerated1 /&gt;&lt;ref name=&quot;CeBIT Global Conferences&quot;&gt;{{Cite web | title =Vodafone - How Medopad's Enterprise Mobile Health Operating System is Revolutionising Healthcare |  url = http://www.cebit.de/event/vodafone-how-medopads-enterprise-mobile-health-operating-system-is-revolutionising-healthcare/VOR/56388 | accessdate=16 June 2014}}&lt;/ref&gt;&lt;ref name=&quot;Hub Westminster&quot;&gt;{{Cite web | title =Google Glass &amp; Medopad: Rich and Dan Tell Their Story| url=http://westminster.impacthub.net/2014/03/31/google-glass-medopad-rich-and-dan-tell-their-story/ Hub Westminster| accessdate= 16 June 2014}}&lt;/ref&gt; Medopad [[Google Glass]] Integration allows doctors to communicate and collaborate, take and share pictures or record video, and view patient records.&lt;ref name=&quot;New Scientist&quot; /&gt; Medopad [[Google Glass]] Alert allows a doctor to identify items they would like to be alerted about and receive the information immediately as it becomes available.&lt;ref name=&quot;Hub Westminster&quot; /&gt;

In 24 April 2015, Medopad announced the global rollout of a chemotherapy application for monitoring cancer patients, which has been designed specifically for the [[Apple Watch]].&lt;ref name=&quot;Journal of mHealth&quot;&gt;http://www.thejournalofmhealth.com/#!Medopad’s-Apple-Watch-Chemotherapy-App-for-Cancer-Patients-Launches-in-Hospitals-Across-the-World/cuhk/553a40ac0cf2731334ee5961&lt;/ref&gt;  &lt;ref name=&quot;BBC World News&quot;&gt;https://vimeo.com/126517749&lt;/ref&gt; Some of the early adopters of the Medopad Apple Watch cancer app includes: [[Princess Royal University Hospital]] and [[King's College Hospital]]

==Medopad for pharmaceutical companies ==

Patient monitoring solutions built on the Medopad platform provide pharmaceutical companies with instant and near real-time access to patient data: &lt;br /&gt;

1. See how drugs are performing &lt;br /&gt;

2. Segment all patient data by demographic features &lt;br /&gt;

3. Identify and select applicants for field studies &lt;br /&gt;

==Involved parties and clients==

Medopad strategic partners and clients include [[Bupa]], [[Bayer]], [[General Healthcare Group]], [[Royal Free London NHS Foundation Trust]], [[Princess Royal University Hospital]], [[King's College Hospital]] [[Barnet Hospital]], [[Chase Farm Hospital]], [[Finchley Memorial Hospital]], [[North Middlesex University Hospital]], [[Bayer]], [[Vodafone]],&lt;ref name=&quot;CeBIT Global Conferences&quot; /&gt;[[BMI Healthcare]], [[East Kent Hospitals University NHS Foundation Trust]], Kent and Canterbury Hospital (Canterbury), William Harvey Hospital (Ashford), Queen Elizabeth The Queen Mother Hospital (Margate), Buckland Hospital (Dover), Royal Victoria Hospital (Folkestone), [[Intel]],&lt;ref name=&quot;Intel Partner Finder&quot;&gt;{{Cite web | title =Intel Partners|  url = http://software.intel.com/partner/search-detail?companyguid=49c7e2ad-4271-e211-ac1d-005056b20261| accessdate=17 June 2013}}&lt;/ref&gt; [[IBM]],&lt;ref name=&quot;Crunch Base&quot;&gt;{{Cite web | title =Company Overview| url=http://www.crunchbase.com/organization/medopad| accessdate= 19 June 2014}}&lt;/ref&gt; [[InterSystems]], [[Open Text Corporation]] and Healthcare Gateway.&lt;ref name=&quot;Crunch Base&quot; /&gt;&lt;ref name=&quot;Medopad Website&quot;&gt;{{Cite web | title =Medopad Website |  url = http://www.medopad.com/medopad_Ltd/Medopad.html | accessdate=17 June 2013}}&lt;/ref&gt;

Among the institutional investors are Healthbox,&lt;ref name=&quot;Healthbox&quot;&gt;{{Cite web | title =HealthBox |  url = http://healthbox.com/companies/13 | accessdate=17 June 2013}}&lt;/ref&gt;&lt;ref name=&quot;The Guardian&quot;&gt;{{Cite web | title ='Dragons' Den' event promotes innovation in healthcare |  url = http://www.guardian.co.uk/healthcare-network/2013/jan/23/dragons-den-event-innovation-healthcare| accessdate=17 June 2013}}&lt;/ref&gt; [[Sandbox Industries]],&lt;ref name=&quot;Hub Westminster&quot; /&gt;&lt;ref name=&quot;Sandbox&quot;&gt;{{Cite web | title =Sandbox Portfolio |  url = http://www.sandboxindustries.com/portfolio/medopad/ | accessdate=16 June 2014}}&lt;/ref&gt; &lt;ref name=&quot;Crunch Base&quot; /&gt; 

Among the angel investors are [[Howard Flight|Lord Howard Flight]], Tony Brown, Non-Executive Director of the [[National Health Service|NHS]],&lt;ref name=&quot;NHS Board of Directors&quot;&gt;{{Cite web | title =NHS Board of Directors| url=http://www.clch.nhs.uk/media/146847/clch_nhs_trust_board_papers_january_2014.pdf| accessdate= 17 June 2014}}&lt;/ref&gt; and Stefan Gries of [[BlackRock]].&lt;ref name=&quot;DueDil&quot;&gt;{{Cite web | title =Medopad Ltd | url=https://www.duedil.com//company/07725451/medopad-ltd/people| accessdate= 19 June 2014}}&lt;/ref&gt;

The supervisory board includes, among others, John Tate
,&lt;ref name=&quot;John Tate on LinkedIn&quot;&gt;{{Cite web | title = John Tate on LinkedIn |  url = http://uk.linkedin.com/pub/john-tate/71/b37/121 | accessdate=17 June 2013}}&lt;/ref&gt; [[BBC|BBC’s]] Director of Policy and Strategy
,&lt;ref name=&quot;John Tate on BBC&quot;&gt;{{Cite web | title = John Tate on BBC website |  url = http://www.bbc.co.uk/pressoffice/biographies/biogs/controllers/john_tate.shtml | accessdate=17 June 2013}}&lt;/ref&gt; and Paul Richardson, [[National Health Service|NHS]] executive.&lt;ref name=&quot;DueDil&quot; /&gt;&lt;ref name=&quot;Medopad Team&quot;&gt;{{Cite web | title =Medopad Team |  url = http://www.medopad.com/medopad_Ltd/Team.html | accessdate=16 June 2014}}&lt;/ref&gt;

Medopad is one of the founding members of the Manchester mHealth Ecosystem &lt;ref name=&quot;The Manchester mHealth Ecosystem&quot;&gt;{{Cite web | title =The Manchester mHealth Ecosystem| url=https://www.informatics.manchester.ac.uk/SiteCollectionDocuments/Ecosystem-Information-Sheets-and-Newsletters/Ecosystem%20Members%2029.07.2013%20NEW%20BRANDING.pdf| accessdate= 17 June 2014}}&lt;/ref&gt; which consists of six [[National Health Service|NHS]] trusts.

==References==
{{Reflist|30em}}

==Further reading==
*[http://mobihealthnews.com/19957/healthbox-london-holds-demo-day-irish-accelerator-healthxl-launches/ Healthbox London holds demo day; Irish accelerator HealthXL launches | mobihealthnews&lt;!-- Bot generated title --&gt;]
*[http://www.hah.co.uk/node/483 The Guardian features Ruth Poole's speech at major European Innovation Day | Healthcare At Home&lt;!-- Bot generated title --&gt;]
*[https://intranet.nibhi.org.uk/mi-old/Lists/Informatics%20News/DispForm.aspx?ID=213 Mi News - Medopad shortlisted in top 16 in LeWeb Competition&lt;!-- Bot generated title --&gt;]
*[http://www.rudebaguette.com/2012/05/28/16-startups/ A detailed breakdown of the 16 startups competing at LeWeb London&lt;!-- Bot generated title --&gt;]

==External links==
* {{official website|http://www.medopad.com/}}

[[Category:Big data]]
[[Category:Software companies based in London]]</text>
      <sha1>ipis4nyemjxnkbezv1s8f6qd3cx8eoc</sha1>
    </revision>
  </page>
  <page>
    <title>Big Data Partnership</title>
    <ns>0</ns>
    <id>42755403</id>
    <revision>
      <id>654809177</id>
      <parentid>643667513</parentid>
      <timestamp>2015-04-03T18:14:43Z</timestamp>
      <contributor>
        <username>Wikiisawesome</username>
        <id>3495083</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[EMEA]] (link changed to [[Europe, the Middle East and Africa]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2793">{{multiple issues|
{{no footnotes|date=May 2014}}
{{notability|Companies|date=May 2014}}
{{COI|date=May 2014}}
}}

{{Infobox company
| name             = Big Data Partnership
| logo             = Big Data Partnership Logo.png
| caption          = 
| type             = Private
| foundation          = 2012
| headquarters     = 
| location_country = 
| location         = [[London, United Kingdom]] 
| locations        = 
| area_served      = 
| key_people       = Mike Merritt-Holmes, CEO&lt;br&gt;Pinal Gandhi, COO&lt;br&gt;Tim Seears, CTO
| industry         = [[Big Data]]&lt;br&gt;[[Data_science|Data Science]]&lt;br&gt;Computer Services
| products         = 
| services         = Big Data Consulting&lt;br&gt;Certified Training&lt;br&gt;Support
| homepage         = {{URL|www.bigdatapartnership.com}}
}}
'''Big Data Partnership''' is a specialist [[big data]] professional services company based in [[London]], UK. It provides [[big data]] consultancy, certified training and support to [[Europe, the Middle East and Africa|EMEA]]-based enterprises.

Big Data Partnership provides expertise in platforms including [[Apache Hadoop]], [[Apache Cassandra]], [[Elasticsearch]], [[Apache HBase]], [[Apache Spark]], [[Storm (event processor)|Apache Storm]], [[Couchbase]] and other big data tools &amp; technologies. 

Big Data Partnership was founded in 2012 by Mike Merritt-Holmes, Pinal Gandhi and Tim Seears. As of January 2015, the company had 50 employees. 

The company have formed partnerships with companies including [[Hortonworks]], [[MapR]], [[wanDISCO]], [[Databricks]], [[Amazon Web Services]], [[Intel]] and [[Elasticsearch]]. 

In June 2014, Big Data Partnership announced Series A funding led by Beringea LLP, an international venture capital firm with offices in London and Detroit.&lt;ref&gt;{{cite news |url= http://techcitynews.com/2014/06/05/big-data-partnership-raises-1-25m-to-improve-the-way-you-use-big-data/ |title= Big Data Partnership raises £1.25m to improve the way you use big data |work= [[TechCityNews]] |accessdate=5 June 2014}}&lt;/ref&gt; 

==References==
{{reflist}}

==External links==
* [http://www.bigdatapartnership.com Official website]
* [http://twitter.com/bigdataexperts Big Data Partnership on Twitter]
* http://www.forbes.com/sites/trevorclawson/2014/02/06/small-players-in-a-big-data-world/
* http://www.builtinlondon.co/big-data-partnership 
* http://www.prnewswire.com/news-releases/big-data-partnership-expands-hadoop-offerings-for-emea-enterprises-245789061.html
* http://hortonworks.com/partner/big-data-partnership/
* http://www.channelweb.co.uk/crn-uk/news/2329296/big-data-partnership-offers-intel-hadoop-software
* http://techcitynews.com/directory/big-data-partnership/



[[Category:Big data]]
[[Category:Technology companies based in London]]
[[Category:Companies established in 2012]]</text>
      <sha1>6sklfjqubvrssaws0ctoke620eyq16n</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Machine to machine</title>
    <ns>14</ns>
    <id>43297012</id>
    <revision>
      <id>616892449</id>
      <timestamp>2014-07-14T09:57:11Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <comment>Robot: Moved from Category:Machine to Machine. Authors: Good Olfactory, Swapnil.Sinha</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="34">{{Cat main}}
[[Category:Big data]]</text>
      <sha1>g2dh2u7cqigqnaskqo5bg64g5xelfg9</sha1>
    </revision>
  </page>
  <page>
    <title>TubeMogul</title>
    <ns>0</ns>
    <id>43376970</id>
    <revision>
      <id>668184100</id>
      <parentid>652845320</parentid>
      <timestamp>2015-06-22T21:16:42Z</timestamp>
      <contributor>
        <ip>64.201.252.132</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19736">{{Infobox company
| name             = TubeMogul Inc.
| logo             = TubeMogul company logo.png
| traded_as        = {{NASDAQ|TUBE}}
| industry         = Marketing and Advertising, Advertising Software
| foundation       = {{start date|2006}}
| founder          = John Hughes and Brett Wilson
| location_city    = [[Emeryville, California|Emeryville, CA]]&lt;ref&gt;{{cite news | title = 14 Silicon Valley area IPOs are coming — but only 5 are tech companies | author = Cromwell Schubarth | work = Silicon Valley Business Journal | date = July 9, 2014 | url = http://www.bizjournals.com/sanjose/news/2014/07/09/14-silicon-valley-area-ipos-are-up-coming-but-only.html?page=all }}&lt;/ref&gt;
| location_country = US
| Global Offices        = 11
| area_served      = 70+ Countries &lt;ref&gt;{{cite news | title = TubeMogul Passes $100 Million Milestone | author = David Burch | work = MarketWatch.com – The Wall Street Journal | date = July 5, 2013 | url = http://www.marketwatch.com/story/tubemogul-passes-100-million-milestone-2013-12-05 }}&lt;/ref&gt;
| num_employees    = 500
| homepage         = {{URL|http://www.TubeMogul.com/}}
}}

'''TubeMogul''' is a publicly traded company whose main product is an enterprise software platform for digital video advertising.&lt;ref name=&quot;TubeMogul IPO A&quot;&gt;{{cite web|url=http://blogs.wsj.com/cmo/2014/07/18/tubemogul-is-latest-ad-tech-firm-to-ride-public-market-roller-coaster/|title=TubeMogul Is Latest Ad Tech Firm to Ride Public Market Roller Coaster|publisher=Wall Street Journal – CMO Today |author=Jack Marshall |date=18 July 2014}}&lt;/ref&gt; TubeMogul’s programmatic software platform leverages [[real-time bidding]] (RTB) technology to help major brands, advertising agencies, trading desks, and publishers advertise to global audiences using online video across multiple devices.&lt;ref name=&quot;Real-Time bidding A&quot;&gt;{{cite web|url=http://www.thedrum.com/knowledge-bank/2013/05/02/benefits-rtb-brands|title=Benefits of RTB for brands|publisher=The Drum|author= Marc Galens|date=2 May 2013}}&lt;/ref&gt;

The company filed an S-1 form with the SEC on March 26, 2014 and went public on July 18, 2014, offering its shares on the ({{NASDAQ|TUBE}})  Global Select Market.&lt;ref name=&quot;TubeMogul IPO A&quot;/&gt;&lt;ref name=&quot;TubeMogul Files S-1&quot;&gt;{{cite web|url=http://techcrunch.com/2014/03/26/tubemogul-files-for-75m-ipo-with-57m-in-revenue-and-a-7m-net-loss-for-2013/|title=TubeMogul Files For $75M IPO, With $57M In Revenue And A $7M Net Loss For 2013|publisher=Tech Crunch|author=  Ryan Lawler|date=26 March 2014}}&lt;/ref&gt;&lt;ref name=&quot;TubeMogul S-1 SEC Application&quot;&gt;{{cite web|url=http://www.sec.gov/Archives/edgar/data/1449278/000119312514174317/d658316ds1a.html|title=TubeMogul, Inc.|publisher=Securities and Exchange Commission|author=  Eric Deeds, Esq.|date=1 May 2014}}&lt;/ref&gt;&lt;ref name=&quot;NASDAQ Ticker TUBE&quot;&gt;{{cite web|url=http://www.nasdaq.com/article/video-ad-platform-provider-tubemogul-sets-terms-for-75-million-ipo-cm368133|title=Video ad platform provider TubeMogul sets terms for $75 million IPO|publisher=NASDAQ|author= Renaissance Capital|date=7 July 2014}}&lt;/ref&gt; TubeMogul is headquartered in [[Emeryville, California]] and has global offices located in Chicago, Detroit, Kiev, New York, London, Los Angeles, Singapore, Shanghai, Sydney, Toronto, and Tokyo.&lt;ref name=&quot;Playtime Platform Launches C&quot;&gt;{{cite web|url=http://techcrunch.com/2011/04/12/tubemogul-video-advertising/|title=TubeMogul Relaunches As A Video Advertising Platform|publisher=Tech Crunch|author=Erick Schonfeld|date=12 April 2011}}&lt;/ref&gt;&lt;ref name=&quot;Global Offices&quot;&gt;{{cite web|url=http://www.webwire.com/ViewPressRel.asp?aId=188902#.U8blPo1dXSI|title=TubeMogul Named First Partner for Vungle’s Mobile Video Ad Exchange|publisher=WebWire|author= WebWire|date=27 June 2014}}&lt;/ref&gt;

==Financing==
TubeMogul received its initial funding after winning the Lester Center’s Business Plan Competition while co-founders John Hughes and Brett Wilson were studying at [[UC Berkeley]]’s [[Haas School of Business]] in 2007.&lt;ref name=&quot;Winners Business Plan Competition&quot;&gt;{{cite web|url=http://newsroom.haas.berkeley.edu/article/ipo-bound-tubemogul-traces-origins-berkeley-haas|title=IPO-bound TubeMogul Traces Origins to Berkeley-Haas |publisher=Haas School of Business|author= Haas School of Business|date=28 March 2014}}&lt;/ref&gt; The company received seed funding from NetService Ventures later that same year.&lt;ref name=&quot;Seed Funding A&quot;&gt;{{cite web|url=http://gigaom.com/2007/10/15/419-online-video-analytics-firm-tubemogul-sets-seed-funding/|title=Online Video Analytics Firm TubeMogul Sets Seed Funding|publisher=GIGAON|author=Rafat Ali|date=15 October 2007}}&lt;/ref&gt;&lt;ref name=&quot;Seed Funding B&quot;&gt;{{cite web|url=http://www.prweb.com/releases/2007/10/prweb560976.html|title=TubeMogul Secures Seed Investment from NetService Ventures|publisher=PRWeb|author=TubeMogul Inc.|date=15 October 2007}}&lt;/ref&gt; They also received angel funding from Roger Ehrenberg of IA Capital in early 2008.&lt;ref name=&quot;Angel Funding&quot;&gt;{{cite web|url=http://www.crunchbase.com/organization/tubemogul|title=TubeMogul Company Overview|publisher=CrunchBase|author=CrunchBase|date=16 July 2014}}&lt;/ref&gt; In February 2008, they received $3 million in [[Series A funding]] led by Trinity Ventures.&lt;ref name=&quot;Series A Funding&quot;&gt;{{cite web|url=http://gigaom.com/2009/03/31/tubemogul-gets-3m-for-video-distribution/|title=TubeMogul Gets $3M for Video Distribution|publisher=GIGAOM|author= Liz Gannes|date=31 March 2009}}&lt;/ref&gt; In October 2008, they acquired Illuminex, a video analytics company founded by Jason Lopatecki and Adam Rose, for an undisclosed amount.&lt;ref name=&quot;Illuminex Acquisition A&quot;&gt;{{cite web|url=http://gigaom.com/2008/10/27/tubemogul-acquires-illuminex/|title=TubeMogul Acquires Illumenix|publisher=GIGAOM|author=  Chris Albrecht|date=27 October 2008}}&lt;/ref&gt;&lt;ref name=&quot;Illuminex Acquisition B&quot;&gt;{{cite web|url=http://www.businessinsider.com/2008/10/tubemogul-buys-flash-analytics-firm-illuminex|title=TubeMogul Buys Flash Analytics Firm Illuminex|publisher=Business Insider|author=Vasanth Sridharan|date=28 October 2008}}&lt;/ref&gt; They raised a combined $10 million in their [[Series B]] round in March 2009, led by [[Foundation Capital]].&lt;ref name=&quot;Series B Funding&quot;&gt;{{cite web|url=http://gigaom.com/2010/10/08/tubemogul-raises-10m-for-ads-international-expansion/|title=TubeMogul Raises $10M For Ads, International Expansion|publisher=GIGAOM|author=Janko Roettgers|date=8 October 2010}}&lt;/ref&gt;  In December 2012 they raised $28 million in the first tranche of their [[Series C]], led by Northgate Capital. The second tranche of the Series C was led by SingTel Innov8, corporate venture capital arm of the SingTel Group, along with Cross Creek Capital, for $10 million dollars in May 2013.&lt;ref name=&quot;Series C Second Tranche Funding&quot;&gt;{{cite web|url=http://www.xconomy.com/san-francisco/2013/05/30/tubemogul-secures-10000000-series-c-financing-round/|title=TubeMogul Secures $10,000,000 Series C Financing Round|publisher=Xconomy|author=Xconomy|date=30 May 2013}}&lt;/ref&gt;

[[File:Brett Wilson Rings Nasdaq Opening Bell 7.18.2014.jpg|thumb|TubeMogul CEO Brett Wilson rings NASDAQ's opening bell July 18, 2014 on the first morning of the company's public offering.]]

===IPO===
TubeMogul filed its S-1 form with the SEC on March 26, 2014.&lt;ref name=&quot;TubeMogul Files S-1&quot;/&gt;&lt;ref name=&quot;TubeMogul S-1 SEC Application&quot;/&gt;

On July 18, 2014 TubeMogul became a publicly traded company.&lt;ref name=&quot;TubeMogul IPO A&quot;/&gt; They made 6.3 million shares available to investors at $7 per share to raise a total of $43.8 million in their initial offering.&lt;ref name=&quot;Offering A&quot;&gt;{{cite web|url=http://blogs.wsj.com/venturecapital/2014/07/18/trinity-ventures-foundation-capital-increase-stakes-in-tubemogul-ipo/|title=Trinity Ventures, Foundation Capital Increase Stakes in TubeMogul IPO|publisher=Wall Street Journal – Tech |author= Lizette Chapman|date=18 July 2014}}&lt;/ref&gt;&lt;ref name=&quot;Offering B&quot;&gt;{{cite web|url=http://www.businessinsider.com/tubemogul-surges-in-ipo-2014-7|title=Video Advertising Company TubeMogul Had A Gigantic First-Day IPO Pop|publisher=Business Insider|author= Myles Udland|date=18 July 2014}}&lt;/ref&gt;  The company listed with the [[NASDAQ|NASDAQ Global Select Market]] using the ticker symbol &quot;TUBE&quot;.&lt;ref name=&quot;Nasdaq Ticker TUBE&quot;&gt;{{cite web|url=http://www.nasdaq.com/article/video-ad-platform-provider-tubemogul-sets-terms-for-75-million-ipo-cm368133|title=Video ad platform provider TubeMogul sets terms for $75 million IPO|publisher=Nasdaq|author= Renaissance Capital|date=7 July 2014}}&lt;/ref&gt; TubeMogul became the first and currently the only enterprise software company offering programmatic digital brand advertising to be publicly traded.&lt;ref name=&quot;Offering C&quot;&gt;{{cite web|url=http://www.forbes.com/sites/alexkonrad/2014/07/18/tubemogul-ipo-jump-looks-great-but-ad-tech-struggles/|title=TubeMogul's 50% IPO Jump Looks Great, But It's Actually A Sign Of Ad Tech Struggles|publisher=Forbes|author= Alex Konrad|date=18 July 2014}}&lt;/ref&gt;

==Company History==
TubeMogul was originally founded as a [[cross-platform]] online [[video analytics]] tool in 2007.&lt;ref name=&quot;Online Analytics Tool Launches A&quot;&gt;{{cite web|url=http://mashable.com/2007/07/23/tubemogul/|title=TubeMogul Launches Cross-Network Video Tracking Tool|publisher=Mashable|author=Kristen Nicole|date=23 July 2007}}&lt;/ref&gt; Video producers uploaded content through TubeMogul, which would then distribute and track performance across video sharing sites.&lt;ref name=&quot;Online Analytics Tool Launches B&quot;&gt;{{cite web|url=http://vator.tv/news/2009-06-07-tubemogul-20-democratizes-video-analytics|title=TubeMogul 2.0 democratizes video analytics – Web video distribution and analytics company sets new standard for online viewing metrics|publisher=Vator News|author=Larry Kless|date=7 June 2009}}&lt;/ref&gt;  In 2010, TubeMogul launched Playtime, an online video [[ad network]], to help advertisers deliver ads to their target audience.&lt;ref name=&quot;Playtime Platform Launches A&quot;&gt;{{cite web|url=http://gigaom.com/2010/03/15/tubemoguls-ad-revenues-already-surpass-analytics-sales/|title=TubeMogul’s Ad Revenues Already Surpass Analytics Sales|publisher=GIGAOM|author=Ryan Lawler|date=15 March 2010}}&lt;/ref&gt; Playtime differentiated itself from other online ad networks by its self-serve aspect as well as the level of transparency it provided.  In 2011, TubeMogul combined the Playtime ad network with the video syndication platform to effectively become a [[demand-side platform]] (DSP) for brand advertisers.&lt;ref name=&quot;Playtime Platform Launches C&quot;/&gt; TubeMogul’s DSP aggregates multiple inventory sources, including advertising exchanges, supply-side platforms and advertising networks, allowing advertisers to integrate pre-negotiated deals with publishers. In 2012, TubeMogul introduced its BrandSafe technology, which ensured that advertisements did not appear alongside objectionable content or run in small video players.&lt;ref name=&quot;New Platform Features&quot;&gt;{{cite web|url=http://techcrunch.com/2012/05/30/tubemogul-pitches-brand-safety-with-pagesafe-a-tool-to-see-where-ads-really-appear/|title=TubeMogul Pitches Brand Safety With PageSafe, A Tool To See Where Ads Really Appear|publisher=Tech Crunch|author= Ryan Lawler|date=30 May 2012}}&lt;/ref&gt;  In 2013, TubeMogul launched BrandPoint, which allows marketers to execute digital video buys on a [[gross rating point]] (GRP) basis, traditionally used by TV advertisers to measure a campaign’s effectiveness.&lt;ref name=&quot;BrandPoint GRP Buying&quot;&gt;{{cite web|url=http://www.adweek.com/videowatch/tubemogul-down-grps-150632|title=TubeMogul Is Down With GRPs Video ad buying firm rolls out planning tool BrandPoint|publisher=ADWEEK|author= Mike Shields|date=25 June 2013}}&lt;/ref&gt; Unlike programmatic solutions that cater to both the buy-side (advertisers) and sell-side (publishers) as well as various marketing objectives (direct response and branding), TubeMogul specializes in brand, or video, advertising and is solely aligned with advertisers.&lt;ref name=&quot;Buy-Side Platform&quot;&gt;{{cite web|url=http://www.mediapost.com/publications/article/228973/tubemogul-plugs-into-vungles-mobile-video-ad-exch.html|title=TubeMogul Plugs Into Vungle's Mobile Video Ad Exchange|publisher=Media Post|author= Tyler Loechner|date=27 June 2014}}&lt;/ref&gt;

==Industry Initiatives==

===Open Video Viewability (OpenVV)===
In May 2013, TubeMogul and several other advertising technology vendors formed the Open Video View ([http://www.openvv.org/ OpenVV]) consortium to help facilitate the adoption of a viewability standard for online video advertising.&lt;ref name=&quot;Video Viewability Standard&quot;&gt;{{cite web|url=http://www.adexchanger.com/analytics/video-ad-rivals-collaborate-on-open-source-viewability/|title=Video Ad Rivals Collaborate On Open-Source Viewability|publisher=Ad Exchanger|author=David Kaplan|date=23 May 2013}}&lt;/ref&gt; OpenVV is an [[open-source]] code that provides marketers verification that their ad was actually seen by human eyes and reasons for non-viewability.&lt;ref name=&quot;OpenVV is Open Source&quot;&gt;{{cite web|url=http://www.videoadnews.com/2014/02/27/vivaki-becomes-first-agency-member-sign-openvv-viewability-gets/|title=VivaKi Becomes First Agency Member to Sign Up to OpenVV Viewability|publisher=Video Ad News|author= Vincent Flood|date=27 February 2014}}&lt;/ref&gt; TubeMogul founded the initiative along with video technology vendors BrightRoll, Innovid, SpotXChange, and LiveRail; current members include [[Nielsen Media Research|Nielsen]], [[comScore]], [[TrustE]], and [[VivaKi]].&lt;ref name=&quot;Video Viewability Standard&quot;/&gt;

===Fraud/Fake Pre-Roll===
In 2011, TubeMogul launched fakepreroll.com to raise awareness about video ads that were shown in inventory normally reserved for display advertisements, oftentimes without the marketer’s knowledge.&lt;ref name=&quot;Fake Pre Roll&quot;&gt;{{cite web|url=http://fakepreroll.com/|title=Fake Pre Roll|publisher=www.FakePreRoll.com|date=7 May 2012}}&lt;/ref&gt;
The site was taken down after several companies sent TubeMogul cease-and-desist orders.

===Botnet Detection===
In 2014, TubeMogul reported the existence of three [[botnets]], responsible for defrauding advertisers for a potential $10 million each month.&lt;ref name=&quot;Botnet's Outed by TubeMogul&quot;&gt;{{cite web|url=http://www.adweek.com/news/technology/fraud-alert-millions-video-views-faked-sophisticated-new-bot-scam-156883|title=Fraud Alert: Millions of Video Views Faked in Sophisticated New Bot Scam – TubeMogul outs dozens of suspect sites|publisher=ADWeek|author= Garett Sloane|date=8 April 2014}}&lt;/ref&gt;&lt;ref name=&quot;Botnet's Outed by TubeMogul B&quot;&gt;{{cite web|url=http://www.huffingtonpost.com/andy-plesser/video-tubemoguls-brett-wi_b_5358880.html|title=TubeMogul's Brett Wilson is Battling Bots, Non-Human Video Views and Low Ad Rates|publisher=Huffington Post|author= Andy Plesser|date=20 May 2014}}&lt;/ref&gt;

===IPG Internship===
In March 2014, [[Interpublic Group of Companies|IPG Mediabrands]] and TubeMogul announced the “Ad-Tech Apprenticeship,” a one-year intensive training program designed to give college graduates a holistic view of the digital advertising industry.&lt;ref name=&quot;IPG Internship A&quot;&gt;{{cite web|url=http://www.mediabistro.com/agencyspy/ipg-mediabrands-tubemogul-collaborate-on-ad-tech-apprenticeship_b61990|title=IPG Mediabrands, TubeMogul Collaborate on Ad-Tech Apprenticeship|publisher=Media Bistro|author=Erik Oster |date=6 March 2014}}&lt;/ref&gt;&lt;ref name=&quot;IPG Internship B&quot;&gt;{{cite web|url=http://www.mediapost.com/publications/article/220919/ipg-tubemogul-team-to-offer-one-year-ad-tech-inte.html|title=IPG, TubeMogul Team To Offer One-Year Ad Tech Internship|publisher=Media Post|author= Tyler Loechner|date=6 March 2014}}&lt;/ref&gt;

==Awards==
# 2009 South by Southwest ([[SXSW]]) Best Online Video-Related Technology Winner&lt;ref name=&quot;SXSW&quot;&gt;{{cite web|url=http://sxsw.com/interactive/accelerator-alums|title=SXSW Accelerator Alums: Over $587 Million in Funding|publisher=SXSW|accessdate=14 July 2014}}&lt;/ref&gt;&lt;ref name=&quot;Go Banking Rates&quot;&gt;{{cite web|url=http://www.gobankingrates.com/savings-account/sxsw-south-by-southwest-millionaires/|title=How SXSW Made These 5 Struggling Entrepreneurs Into Millionaires|publisher=Go Banking Rates |author=Christina Lavingia|date=6 March 2014|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=&quot;Surfly&quot;&gt;{{cite web|url=http://www.surfly.com/category/sxsw/|title=The nominees for the SXSW Accelerator 2014 [infographic]|publisher=www.Surfly.com|date=28 February 2014}}&lt;/ref&gt;
# 2009 AlwaysOn OnMedia 100 Award Winner&lt;ref name=&quot;Ad Ops Online&quot;&gt;{{cite web|url=http://www.adoperationsonline.com/2009/01/27/alwayson-announces-onmedia-100-award-winners/|title=AlwaysOn Announces OnMedia 100 Award Winners|publisher=Ad Ops Online|author=Otilia Otlacan|date=27 January 2009}}&lt;/ref&gt;
# 2012 AlwaysOn Global 250 Winner&lt;ref name=&quot;AlwaysOn Global 250&quot;&gt;{{cite web|url=http://www.mediapost.com/publications/article/178540/tubemogul-recognized-as-an-alwayson-global-250-win.html/|title=TubeMogul Recognized as an AlwaysOn Global 250 Winner|publisher=Media Post|author=Tyler Loechner|date=12 July 2012}}&lt;/ref&gt;
# 2013 [[Inc. 500|Inc.]] Top 100 Advertising and Marketing Companies #29&lt;ref name=&quot;Top 100 Marketing and Advertising Companies&quot;&gt;{{cite web|url=http://www.inc.com/profile/tubemogul|title=TubeMogul Company Profile|publisher=Inc.|author=Inc.}}&lt;/ref&gt;
# 2013 [[Inc. 500|Inc.]] Hire Power Award Top 10 Advertising and Marketing Companies #9&lt;ref name=&quot;Top 100 Marketing and Advertising Companies&quot;/&gt;
# 2013 Lead 411 Tech200 #14&lt;ref name=&quot;Tech200 Award&quot;&gt;{{cite web|url=http://www.lead411.com/tech200/2013/list.php|title=Tech 200 Winners|publisher=Lead 411|author=Lead 411}}&lt;/ref&gt;
# 2013 [[Deloitte]] Fast 500 #35&lt;ref name=&quot;Deloitte Fast 500 Award&quot;&gt;{{cite web|url=http://www.deloitte.com/assets/Dcom-UnitedStates/Local%20Assets/Documents/TMT_us_tmt/us_tmt_fast500_rankings_110713.pdf|title=Fast 500 Award Winners|publisher=Deloitte|author=Deloitte}}&lt;/ref&gt;
# 2013 iMedia Connection ASPY Awards – Winner Best Customer Service Award&lt;ref name=&quot;Best Customer Service&quot;&gt;{{cite web|url=http://blogs.imediaconnection.com/blog/2013/05/08/aspy-awards-honors-buzzfeed-google-and-more/|title=ASPY Awards Honor BuzzFeed, Google, and More|publisher=iMedia Connection|author=iMedia Editors |date=8 May 2013}}&lt;/ref&gt;
# 2014 San Francisco Business Times Best Places to Work #35&lt;ref name=&quot;Best Places to Work&quot;&gt;{{cite web|url=http://www.bizjournals.com/sanfrancisco/subscriber-only/2014/04/18/best-places-to-work-midsize-2014.html|title=Best Places to Work in the Bay Area – Midsize Companies|publisher=San Francisco Business Times|author=Julia Cooper|date=18 April 2014}}&lt;/ref&gt;
#2014 The Drum Digital Trading Awards – Winner Advertiser's Choice of Ad technology&lt;ref name=&quot;The Drum Digital Trading Awards&quot;&gt;{{cite web|url=http://www.digitaltradingawards.com/results|title=The Drum Digital Trading Awards 2014 |publisher=The Drum|author=The Drum|date=24 April 2014}}&lt;/ref&gt;
#2015 Glassdoor – Winner “Best Places to Work” People’s Choice Awards, TubeMogul was ranked 5th for companies with less than 1,000 employees.&lt;ref name=&quot;Glassdoor Best Places to Work&quot;&gt;{{cite web|url=http://www.glassdoor.com/Best-Small-and-Medium-Companies-to-Work-For-LST_KQ0,43.htm|title=Best Small &amp; Medium Companies to Work For |publisher=Glassdoor|author=Glassdoor|date=9 December 2014}}&lt;/ref&gt;

==See also==
* [[Demand-Side Platform]]
* [[Online Advertising]]
* [[Programmatic Marketing]]
* [[Real-Time Bidding]]
* [[Video Ad Platform]]
* [[Video advertising|Video Advertising]]

==References==
{{reflist|33em}}

[[Category:Internet advertising]]
[[Category:Big data]]
[[Category:Technology companies established in 2006]]
[[Category:Software companies based in California]]
[[Category:Companies based in Emeryville, California]]</text>
      <sha1>fvf5tw7tf8zhzeimcx2iiyub21ypyq7</sha1>
    </revision>
  </page>
  <page>
    <title>Big Data to Knowledge</title>
    <ns>0</ns>
    <id>43413047</id>
    <revision>
      <id>651816867</id>
      <parentid>651809533</parentid>
      <timestamp>2015-03-17T18:59:01Z</timestamp>
      <contributor>
        <username>Bluerasberry</username>
        <id>7830073</id>
      </contributor>
      <comment>Reverted to revision 651806484 by [[Special:Contributions/Bluerasberry|Bluerasberry]] ([[User talk:Bluerasberry|talk]]): See talk page - I will revet back if I am in error, but please reply... ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1854">'''Big Data to Knowledge''' (BD2K) is a project of the [[National Institutes of Health]] for [[knowledge extraction]] from [[big data]].

BD2K was founded in 2013 in response to a report from the Working Group on Data and Informatics for the Advisory Committee to the Director of the National Institutes of Health.&lt;ref name=&quot;Ohno-Machado2014&quot;&gt;{{cite journal|last1=Ohno-Machado|first1=L.|title=NIH's Big Data to Knowledge initiative and the advancement of biomedical informatics|journal=Journal of the American Medical Informatics Association|volume=21|issue=2|year=2014|pages=193–193|issn=1067-5027|doi=10.1136/amiajnl-2014-002666}}&lt;/ref&gt;

A significant part of BD2K's plans is to have organizations make plans to share their research data when they make a proposal in response to a [[funding opportunity announcement]].&lt;ref&gt;{{cite web |url= http://biomedicalcomputationreview.org/content/nih-announcement-big-data-gets-big-support |title=NIH Announcement: Big Data Gets Big Support &amp;#124; Biomedical Computation Review |first=Katharine |last=Miller |work=biomedicalcomputationreview.org |date=19 February 2013 |accessdate=28 July 2014}}&lt;/ref&gt;

[[Philip Bourne]] is the lead in managing the project.&lt;ref name=&quot;MargolisDerr2014&quot;&gt;{{cite journal|last1=Margolis|first1=R.|last2=Derr|first2=L.|last3=Dunn|first3=M.|last4=Huerta|first4=M.|last5=Larkin|first5=J.|last6=Sheehan|first6=J.|last7=Guyer|first7=M.|last8=Green|first8=E. D.|title=The National Institutes of Health's Big Data to Knowledge (BD2K) initiative: capitalizing on biomedical big data|journal=Journal of the American Medical Informatics Association|year=2014|issn=1067-5027|doi=10.1136/amiajnl-2014-002974}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*{{official website|http://bd2k.nih.gov/#sthash.MEHYjf3j.dpbs}}

[[Category:National Institutes of Health]]
[[Category:Big data]]</text>
      <sha1>r8mjwf2jxvo18g48ojrmrm2znjlhvii</sha1>
    </revision>
  </page>
  <page>
    <title>Lambda architecture</title>
    <ns>0</ns>
    <id>43539426</id>
    <revision>
      <id>663236298</id>
      <parentid>657225137</parentid>
      <timestamp>2015-05-20T09:31:57Z</timestamp>
      <contributor>
        <ip>67.169.46.199</ip>
      </contributor>
      <comment>Corrected surname to Kreps instead of Krebs</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9437">[[File:Diagram of Lambda Architecture (generic).png|thumb|Flow of data through the processing and serving layers of a generic lambda architecture]]
'''Lambda architecture''' is a [[data processing|data-processing]] architecture designed to handle massive quantities of data by taking advantage of both [[batch processing|batch]]- and [[stream processing|stream-processing]] methods. This approach to architecture attempts to balance latency, throughput, and fault-tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation. The rise of lambda architecture is correlated with the growth of [[big data]], real-time analytics, and the drive to mitigate the latencies of [[map-reduce]].&lt;ref&gt;{{cite web|last1=Schuster|first1=Werner|title=Nathan Marz on Storm, Immutability in the Lambda Architecture, Clojure|url=http://www.infoq.com/interviews/marz-lambda-architecture|website=www.infoq.com}} Interview with Nathan Marz, 6 April 2014&lt;/ref&gt;

Lambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record.&lt;ref name=bijnens-slide&gt;Bijnens, Nathan. [http://lambda-architecture.net/architecture/2013-12-11-a-real-time-architecture-using-hadoop-and-storm-devoxx/ &quot;A real-time architecture using Hadoop and Storm&quot;]. 11 December 2013.&lt;/ref&gt;{{rp|32}} It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data.

==Overview==
Lambda architecture describes a system consisting of three layers: batch processing, speed (or real-time processing), and a serving layer for responding to queries.&lt;ref name=big-data&gt;Marz, Nathan; Warren, James. ''Big Data: Principles and best practices of scalable realtime data systems''. Manning Publications, 2013.&lt;/ref&gt;{{rp|13}} The processing layers ingest from an immutable master copy of the entire data set.

===Batch layer===
The batch layer precomputes results using a distributed processing system that can handle very large quantities of data. The batch layer aims at perfect accuracy by being able to process ''all'' available data when generating views. This means it can fix any errors by recomputing based on the complete data set, then updating existing views. Output is typically stored in a read-only database, with updates completely replacing existing precomputed views.&lt;ref name=big-data /&gt;{{rp|18}}

[[Hadoop|Apache Hadoop]] is the de facto standard batch-processing system used in most high-throughput architectures.&lt;ref&gt;Kar, Saroj. [http://cloudtimes.org/2014/05/28/hadoop-sector-will-have-annual-growth-of-58-for-2013-2020/ &quot;Hadoop Sector will Have Annual Growth of 58% for 2013-2020&quot;], 28 May 2014. ''Cloud Times''.&lt;/ref&gt;

===Speed layer===
[[File:Diagram of Lambda Architecture (named components).png|thumb|Diagram showing the flow of data through the processing and serving layers of lambda architecture. Example named components are shown.]]
The speed layer processes data streams in real time and without the requirements of fix-ups or completeness. This layer sacrifices throughput as it aims to minimize latency by providing real-time views into the most recent data. Essentially, the speed layer is responsible for filling the &quot;gap&quot; caused by the batch layer's lag in providing views based on the most recent data. This layer's views may not be as accurate or complete as the ones eventually produced by the batch layer, but they are available almost immediately after data is received, and can be replaced when the batch layer's views for the same data become available.&lt;ref name=big-data /&gt;{{rp|203}}

Stream-processing technologies typically used in this layer include [[Storm (event processor)|Apache Storm]], [[Sqlstream|SQLstream]] and [[Apache Spark]]. Output is typically stored on fast NoSQL databases.&lt;ref name=kinley&gt;Kinley, James. [http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting &quot;The Lambda architecture: principles for architecting realtime Big Data systems&quot;], retrieved 26 August 2014.&lt;/ref&gt;&lt;ref&gt;Ferrera Bertran, Pere. [http://www.datasalt.com/2014/01/lambda-architecture-a-state-of-the-art/ &quot;Lambda Architecture: A state-of-the-art&quot;]. 17 January 2014, Datasalt.&lt;/ref&gt;

===Serving layer===
[[File:Diagram of Lambda Architecture (Druid data store).png|thumb|Diagram showing a lambda architecture with a Druid data store.]]
Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data.

Examples of technologies used in the serving layer include [[Druid (open-source data store)|Druid]], which provides a single cluster to handle output from both layers.&lt;ref name=metamarkets-lambda&gt;Yang, Fangjin, and Merlino, Gian. [https://speakerdeck.com/druidio/real-time-analytics-with-open-source-technologies-1 &quot;Real-time Analytics with Open Source Technologies&quot;]. 30 July 2014.&lt;/ref&gt; Dedicated stores used in the serving layer include [[Apache Cassandra]] or [[Apache HBase]] for speed-layer output, and [https://github.com/nathanmarz/elephantdb Elephant DB] or [[Cloudera Impala]] for batch-layer output.&lt;ref name=bijnens-slide /&gt;{{rp|45}}&lt;ref name=kinley /&gt;

==Optimizations==
To optimize the data set and improve query efficiency, various rollup and aggregation techniques are executed on raw data,&lt;ref name=metamarkets-lambda /&gt;{{rp|23}} while estimation techniques are employed to further reduce computation costs.&lt;ref&gt;Ray, Nelson. [https://metamarkets.com/2013/histograms/ &quot;The Art of Approximating Distributions: Histograms and Quantiles at Scale&quot;]. 12 September 2013. Metamarkets.&lt;/ref&gt; And while expensive full recomputation is required for fault tolerance, incremental computation algorithms may be selectively added to increase efficiency, and techniques such as ''partial computation'' and resource-usage optimizations can effectively help lower latency.&lt;ref name=big-data /&gt;{{rp|93,287,293}}

==Lambda architecture in use==
Metamarkets, which provides analytics for companies in the programmatic advertising space, employs a version of the lambda architecture that uses [[Druid (open-source data store)|Druid]] for storing and serving both the streamed and batch-processed data.&lt;ref name=metamarkets-lambda /&gt;{{rp|42}}

For running analytics on its advertising data warehouse, [[Yahoo]] has taken a similar approach, also using [[Storm (event processor)|Apache Storm]], [[Hadoop|Apache Hadoop]], and [[Druid (open-source data store)|Druid]].&lt;ref name=yahoo-lambda&gt;Rao, Supreeth; Gupta, Sunil. [http://www.slideshare.net/Hadoop_Summit/interactive-analytics-in-human-time?next_slideshow=1 &quot;Interactive Analytics in Human Time&quot;]. 17 June 2014&lt;/ref&gt;{{rp|9,16}}

The [[Netflix]] Suro project has separate processing paths for data, but does not strictly follow lambda architecture since the paths may be intended to serve different purposes and not necessarily to provide the same type of views.&lt;ref name=netflix&gt;Bae, Jae Hyeon; Yuan, Danny; Tonse, Sudhir. [http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html &quot;Announcing Suro: Backbone of Netflix's Data Pipeline&quot;], ''[[Netflix]]'', 9 December 2013&lt;/ref&gt; Nevertheless, the overall idea is to make selected real-time event data available to queries with very low latency, while the entire data set is also processed via a batch pipeline. The latter is intended for applications that are less sensitive to latency and require a map-reduce type of processing.

==Criticism==
Criticism of lambda architecture has focused on its inherent complexity and its limiting influence. The batch and streaming sides each require a different code base that must be maintained and kept in sync so that processed data produces the same result from both paths. Yet attempting to abstract the code bases into a single framework puts many of the specialized tools in the batch and real-time ecosystems out of reach.&lt;ref&gt;{{cite web|last1=Kreps|first1=Jay|title=Questioning the Lambda Architecure|url=http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html|website=radar.oreilly.com|publisher=Oreilly|accessdate=15 August 2014|ref=kreps}}&lt;/ref&gt;

In a technical discussion over the merits of employing a pure streaming approach, it was noted that using a flexible streaming framework such as [[Apache Samza]] could provide some of the same benefits as batch processing without the latency.&lt;ref&gt;[https://news.ycombinator.com/item?id=7976785 Hacker News] retrieved 20 August 2014&lt;/ref&gt; Such a streaming framework could allow for collecting and processing arbitrarily large windows of data, accommodate blocking, and handle state.

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags, these references will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://lambda-architecture.net/ Repository of Information on Lambda of Architecture]

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Data processing]]
[[Category:Big data]]
[[Category:Data analysis]]
[[Category:Free software projects]]
[[Category:Software architecture]]</text>
      <sha1>ltlmm7irhyy570xe5144f4e78oyixoj</sha1>
    </revision>
  </page>
  <page>
    <title>Big Strategy</title>
    <ns>0</ns>
    <id>43954674</id>
    <revision>
      <id>629163117</id>
      <parentid>627935434</parentid>
      <timestamp>2014-10-11T11:14:21Z</timestamp>
      <contributor>
        <username>Iaritmioawp</username>
        <id>22040210</id>
      </contributor>
      <comment>Added three categories.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="438">{{Multiple issues|{{unreferenced|date=September 2014}}{{orphan|date=September 2014}}}}

{{Use dmy dates|date=September 2014}}

'''Big Strategy''' is an all-encompassing term for all [[Strategic management|business strategies]] that employ [[big data]] analysis in the decision-making process.

== See also ==
[[Theory of constraints]]

[[Category:Big data]]
[[Category:Business terms]]
[[Category:Strategic management]]

{{business-stub}}</text>
      <sha1>hfroe8n4h35am6vub71fgj3ygxviumy</sha1>
    </revision>
  </page>
  <page>
    <title>Big data</title>
    <ns>0</ns>
    <id>27051151</id>
    <revision>
      <id>673607112</id>
      <parentid>673607038</parentid>
      <timestamp>2015-07-29T09:07:45Z</timestamp>
      <contributor>
        <ip>119.235.53.46</ip>
      </contributor>
      <comment>/* Science and research */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="75661">{{About|large collections of data|the graph database|Graph database|the band|Big Data (band)}}
[[File:Viegas-UserActivityonWikipedia.gif|thumb|Visualization of daily Wikipedia edits created by IBM. At multiple [[terabyte]]s in size, the text and images of Wikipedia are an example of big data.]]

[[File:Hilbert InfoGrowth.png|thumb|right|350px|Growth of and Digitization of Global Information Storage Capacity [http://www.martinhilbert.net/WorldInfoCapacity.html Source]]]

'''Big data''' is a broad term for [[data set]]s so large or complex that traditional [[data processing]] applications are inadequate.  Challenges include analysis, capture, [[data curation]], search, [[Data sharing|sharing]], storage, transfer, visualization, and [[information privacy]].  The term often refers simply to the use of [[predictive analytics]] or other certain advanced methods to extract value from data, and seldom to a particular size of data set. Accuracy in big data may lead to more confident decision making. And better decisions can mean greater operational efficiency, cost reductions and reduced risk.
 
Analysis of data sets can find new correlations, to &quot;spot business trends, prevent diseases, combat crime and so on.&quot;{{r|Economist}} Scientists, business executives,  practitioners of media and advertising and [[Government database|governments]] alike regularly meet difficulties with large data sets in areas including [[Web search engine|Internet search]], [[finance]] and [[business informatics]].  Scientists encounter limitations in [[e-Science]] work, including [[meteorology]], [[genomics]],&lt;ref&gt;{{cite journal |title=Community cleverness required |journal=Nature |volume=455 |issue=7209 |page=1 |date=4 September 2008 |doi=10.1038/455001a |url=http://www.nature.com/nature/journal/v455/n7209/full/455001a.html}}&lt;/ref&gt; [[connectomics]], complex physics simulations,&lt;ref&gt;{{cite web |title=Sandia sees data management challenges spiral |date=4 August 2009 |work=HPC Projects |url=http://www.hpcprojects.com/news/news_story.php?news_id=922}}&lt;/ref&gt; and biological and environmental research.&lt;ref&gt;{{cite journal |last1=Reichman |first1=O.J. |last2=Jones |first2=M.B. |last3=Schildhauer |first3=M.P. |title=Challenges and Opportunities of Open Data in Ecology |journal=Science |volume=331 |issue=6018 |pages=703–5 |year=2011 |doi=10.1126/science.1197962 |pmid=21311007 }}&lt;/ref&gt;

Data sets grow in size in part because they are increasingly being gathered by cheap and numerous information-sensing [[mobile device]]s, aerial ([[remote sensing]]), software logs, [[Digital camera|cameras]], microphones, [[radio-frequency identification]] (RFID) readers, and [[wireless sensor networks]].&lt;ref&gt;{{cite web|title=Data Crush by Christopher Surdak|url=http://bookreviews.infoversant.com/data-crush-christopher-surdak/|accessdate=14 February 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |author=Hellerstein, Joe |title=Parallel Programming in the Age of Big Data |date=9 November 2008 |work=Gigaom Blog |url=http://gigaom.com/2008/11/09/mapreduce-leads-the-way-for-parallel-programming/}}&lt;/ref&gt;&lt;ref&gt;{{cite book |first1=Toby |last1=Segaran |first2=Jeff |last2=Hammerbacher |title=Beautiful Data: The Stories Behind Elegant Data Solutions |url=http://books.google.com/books?id=zxNglqU1FKgC |year=2009 |publisher=O'Reilly Media |isbn=978-0-596-15711-1 |page=257}}&lt;/ref&gt; The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;&lt;ref name=&quot;HilbertLopez2011&quot;&gt;{{harvnb|Hilbert|López|2011}}&lt;/ref&gt; {{As of|2012|lc=on}}, every day 2.5 [[exabyte]]s (2.5×10&lt;sup&gt;18&lt;/sup&gt;) of data were created;&lt;ref&gt;{{cite web|url=http://www.ibm.com/big-data/us/en/ |title=IBM What is big data? — Bringing big data to the enterprise |publisher=www.ibm.com |accessdate=2013-08-26}}&lt;/ref&gt; The challenge for large enterprises is determining who should own big data initiatives that straddle the entire organization.&lt;ref&gt;Oracle and FSN, [http://www.fsn.co.uk/channel_bi_bpm_cpm/mastering_big_data_cfo_strategies_to_transform_insight_into_opportunity#.UO2Ac-TTuys &quot;Mastering Big Data: CFO Strategies to Transform Insight into Opportunity&quot;], December 2012&lt;/ref&gt;

Work with big data is necessarily uncommon; most analysis is of &quot;PC size&quot; data, on a desktop PC or notebook&lt;ref&gt;{{cite web|url=http://www.kdnuggets.com/2015/04/computing-platforms-analytics-data-mining-data-science.html|title=Computing Platforms for Analytics, Data Mining, Data Science|work=kdnuggets.com|accessdate=15 April 2015}}&lt;/ref&gt; that can handle the available data set.

[[Relational database management system]]s and desktop statistics and visualization packages often have difficulty handling big data. The work instead requires &quot;massively parallel software running on tens, hundreds, or even thousands of servers&quot;.&lt;ref&gt;{{cite web |author=Jacobs, A. |title=The Pathologies of Big Data |date=6 July 2009 |work=ACMQueue |url=http://queue.acm.org/detail.cfm?id=1563874}}&lt;/ref&gt; What is considered &quot;big data&quot; varies depending on the capabilities of the users and their tools, and expanding capabilities make Big Data a moving target.  Thus, what is considered &quot;big&quot; one year becomes ordinary later. &quot;For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.&quot;&lt;ref&gt;{{cite journal |last1=Magoulas |first1=Roger |last2=Lorica |first2=Ben |title=Introduction to Big Data |journal=Release 2.0 |issue=11 |date=February 2009 |url=http://radar.oreilly.com/r2/release2-0-11.html |publisher=O’Reilly Media |location=Sebastopol CA}}&lt;/ref&gt;

== Definition ==
Big data usually includes data sets with sizes beyond the ability of commonly used software tools to [[data acquisition|capture]], [[data curation|curate]], manage, and process data within a tolerable elapsed time.&lt;ref name=&quot;Editorial&quot;&gt;{{cite journal | last1 = Snijders | first1 = C. | last2 = Matzat | first2 = U. | last3 = Reips | first3 = U.-D. | year = 2012 | title = 'Big Data': Big gaps of knowledge in the field of Internet | url = http://www.ijis.net/ijis7_1/ijis7_1_editorial.html | journal = International Journal of Internet Science | volume = 7 | issue = | pages = 1–5 }}&lt;/ref&gt; Big data &quot;size&quot; is a constantly moving target, {{As of|2012|lc=on}} ranging from a few dozen terabytes to many [[petabyte]]s of data. 
Big data is a set of techniques and technologies that require new forms of integration to uncover large hidden values from large datasets that are diverse, complex, and of a massive scale.&lt;ref&gt;{{cite journal | last1 = Ibrahim | first1 =  | last2 = Targio Hashem | first2 = Abaker | last3 = Yaqoob | first3 = Ibrar | last4 = Badrul Anuar | first4 = Nor | last5 = Mokhtar | first5 = Salimah | last6 = Gani | first6 = Abdullah | last7 = Ullah Khan | first7 = Samee | year = 2015 | title = big data&quot; on cloud computing: Review and open research issues | url = | journal = Information Systems | volume = 47 | issue = | pages = 98–115 | doi = 10.1016/j.is.2014.07.006 }}&lt;/ref&gt;

In a 2001 research report&lt;ref&gt;{{cite web |first=Douglas |last=Laney |title=3D Data Management: Controlling Data Volume, Velocity and Variety |url=http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf |publisher=Gartner |accessdate = 6 February 2001}}&lt;/ref&gt; and related lectures, [[META Group]] (now [[Gartner]]) analyst Doug Laney defined data growth challenges and opportunities as being three-dimensional, i.e. increasing [[volume]] (amount of data), [[velocity]] (speed of data in and out), and {{linktext|variety}} (range of data types and sources). Gartner, and now much of the industry, continue to use this &quot;3Vs&quot; model for describing big data.&lt;ref&gt;{{cite web |last=Beyer |first=Mark |title=Gartner Says Solving 'Big Data' Challenge Involves More Than Just Managing Volumes of Data |url=http://www.gartner.com/it/page.jsp?id=1731916 |publisher=Gartner |accessdate = 13 July 2011| archiveurl= http://web.archive.org/web/20110710043533/http://www.gartner.com/it/page.jsp?id=1731916| archivedate= 10 July 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; In 2012, [[Gartner]] updated its definition as follows: &quot;Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization.&quot;&lt;ref&gt;{{cite web |first=Douglas |last=Laney |title=The Importance of 'Big Data': A Definition |url=http://www.gartner.com/resId=2057415 |publisher=Gartner |accessdate = 21 June 2012}}&lt;/ref&gt; Additionally, a new V &quot;Veracity&quot; is added by some organizations to describe it.&lt;ref&gt;{{cite web|title=What is Big Data?|url=http://www.villanovau.com/university-online-programs/what-is-big-data/|publisher=[[Villanova University]]}}&lt;/ref&gt;

If Gartner’s definition (the 3Vs) is still widely used, the growing maturity of the concept fosters a more sound difference between big data and [[Business Intelligence]], regarding data and their use:&lt;ref&gt;http://www.bigdataparis.com/presentation/mercredi/PDelort.pdf?PHPSESSID=tv7k70pcr3egpi2r6fi3qbjtj6#page=4&lt;/ref&gt;
* Business Intelligence uses [[descriptive statistics]] with data with high information density to measure things, detect trends etc.;
* Big data uses [[inductive statistics]] and concepts from  [[nonlinear system identification]] &lt;ref name=&quot;SAB1&quot;&gt;Billings S.A. &quot;Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains&quot;. Wiley, 2013&lt;/ref&gt;  to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density&lt;ref&gt;Delort P., Big data Paris 2013 http://www.andsi.fr/tag/dsi-big-data/&lt;/ref&gt; to reveal relationships, dependencies and perform predictions of outcomes and behaviors.&lt;ref name=&quot;SAB1&quot; /&gt;&lt;ref&gt;Delort P., Big Data car Low-Density Data ? La faible densité en information comme facteur discriminant http://lecercle.lesechos.fr/entrepreneur/tendances-innovation/221169222/big-data-low-density-data-faible-densite-information-com&lt;/ref&gt;
A more recent, consensual definition states that &quot;Big Data represents the Information assets characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value&quot;.&lt;ref name=&quot;Big Data Definition&quot;&gt;{{cite journal | last1 = De Mauro | first1 = Andrea | last2 = Greco | first2 = Marco | last3 = Grimaldi | first3 = Michele | year = 2015 | title = What is big data? A consensual definition and a review of key research topics | url = http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.4907823 | journal = AIP Conference Proceedings | volume = 1644 | issue = | pages = 97–104 | doi=10.1063/1.4907823}}&lt;/ref&gt;

== Characteristics ==
Big data can be described by the following characteristics:

;Volume: The quantity of generated data is important in this context. The size of the data determines the value and potential of the data under consideration, and whether it can actually be considered big data or not. The name ‘big data’ itself contains a term related to size, and hence the characteristic.

;Variety: This is the category of big data, and an essential fact that data analysts must know. This helps people who analyze the data and are associated with it effectively use the data to their advantage and thus uphold the importance of the big data.

;Velocity: ‘Velocity’ in this context means how fast the data is generated and processed to meet the demands and the challenges that lie in the path of growth and development.

;Variability: This refers to inconsistency the data can show at times—which hampers the process of handling and managing the data effectively.

;Veracity: The quality of captured data can vary greatly. Accurate analysis depends on the veracity of source data.

;Complexity: Data management can be very complex, especially when large volumes of data come from multiple sources. Data must be linked, connected, and correlated so users can grasp the information the data is supposed to convey.

Factory work and [[Cyber-physical system]]s may have a 6C system:
* Connection (sensor and networks)
* Cloud (computing and data on demand)
* Cyber (model and memory)
* Content/context (meaning and correlation)
* Community (sharing and collaboration)
* Customization (personalization and value)

Data must be processed with advanced tools (analytics and algorithms) to reveal meaningful information. Considering visible and invisible issues in, for example, a factory, the information generation algorithm must detect and address invisible issues such as machine degradation, component wear, etc. on the factory floor.&lt;ref name=INDIN2014&gt;{{cite journal|last1=Lee|first1=Jay|last2=Bagheri|first2=Behrad|last3=Kao|first3=Hung-An|title=Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics|journal=IEEE Int. Conference on Industrial Informatics (INDIN) 2014|date=2014|url=https://www.researchgate.net/profile/Behrad_Bagheri/publication/266375284_Recent_Advances_and_Trends_of_Cyber-Physical_Systems_and_Big_Data_Analytics_in_Industrial_Informatics/links/542dc0100cf27e39fa948a7d?origin=publication_detail}}&lt;/ref&gt;&lt;ref name=MfgLetters&gt;{{cite journal|last1=Lee|first1=Jay|last2=Lapira|first2=Edzel|last3=Bagheri|first3=Behrad|last4=Kao|first4=Hung-an|title=Recent advances and trends in predictive manufacturing systems in big data environment|journal=Manufacturing Letters|volume=1|issue=1|pages=38–41|doi=10.1016/j.mfglet.2013.09.005|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114}}&lt;/ref&gt;

==Architecture==

In 2000, Seisint Inc. developed a C++ based distributed file sharing framework for data storage and query. The system stores and distributes structured, semi-structured, and unstructured data across multiple servers. Users can build queries in a modified C++ called [[ECL programming language|ECL]]. ECL uses an &quot;apply schema on read&quot; method to infer the structure of stored data at the time of the query. In 2004, [[LexisNexis]] acquired Seisint Inc.&lt;ref&gt;{{cite web|url=http://www.washingtonpost.com/wp-dyn/articles/A50577-2004Jul14.html|title=LexisNexis To Buy Seisint For $775 Million|publisher=Washington Post|accessdate=15 July 2004}}&lt;/ref&gt; and in 2008 acquired ChoicePoint, Inc.&lt;ref&gt;{{cite web|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/02/21/AR2008022100809.html|title=LexisNexis Parent Set to Buy ChoicePoint|publisher=Washington Post|accessdate=22 February 2008}}&lt;/ref&gt; and their high speed parallel processing platform. The two platforms were merged into [[HPCC]] Systems and in 2011 was open sourced under the Apache v2.0 License. Currently HPCC and [[Quantcast File System]]&lt;ref&gt;{{cite web|url=http://www.datanami.com/2012/10/01/quantcast_opens_exabyte_ready_file_system/|title=Quantcast Opens Exabyte-Ready File System|publisher=www.datanami.com|accessdate=1 October 2012}}&lt;/ref&gt; are the only publicly available platforms capable of analyzing multiple exabytes of data.

In 2004, Google published a paper on a process called [[MapReduce]] that used such an architecture. The MapReduce framework provides a parallel processing model and associated implementation to process huge amounts of data.  With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the Map step). The results are then gathered and delivered (the Reduce step). The framework was very successful,&lt;ref&gt;Bertolucci, Jeff [http://www.informationweek.com/big-data/news/software-platforms/hadoop-from-experiment-to-leading-big-d/240157176 &quot;Hadoop: From Experiment To Leading Big Data Platform&quot;], &quot;Information Week&quot;, 2013. Retrieved on 14 November 2013.&lt;/ref&gt; so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open source project named [[Apache Hadoop|Hadoop]].&lt;ref&gt;Webster, John. [http://research.google.com/archive/mapreduce-osdi04.pdf &quot;MapReduce: Simplified Data Processing on Large Clusters&quot;], &quot;Search Storage&quot;, 2004. Retrieved on 25 March 2013.&lt;/ref&gt;

[[MIKE2.0 Methodology|MIKE2.0]] is an open approach to information management that acknowledges the need for revisions due to big data implications in an article titled &quot;Big Data Solution Offering&quot;.&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Solution_Offering|title=Big Data Solution Offering|publisher=MIKE2.0|accessdate=8 Dec 2013}}&lt;/ref&gt; The methodology addresses handling big data in terms of useful [[permutation]]s of data sources, [[complexity]] in interrelationships, and difficulty in deleting (or modifying) individual records.&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Definition|title=Big Data Definition|publisher=MIKE2.0|accessdate=9 March 2013}}&lt;/ref&gt;

Recent studies show that the use of a multiple layer architecture is an option for dealing with big data. The Distributed Parallel architecture distributes data across multiple processing units and parallel processing units provide data much faster, by improving processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end user by using a front end application server.&lt;ref&gt;{{cite journal|last=Boja|first=C|author2=Pocovnicu, A |author3=Bătăgan, L. |title=Distributed Parallel Architecture for Big Data|journal=Informatica Economica|year=2012|volume=16|issue=2|pages=116–127}}&lt;/ref&gt;

Big Data Analytics for Manufacturing Applications can be based on a 5C architecture (connection, conversion, cyber, cognition, and configuration).&lt;ref&gt;[http://www.imscenter.net/cyber-physical-platform Intelligent Maintenance System]&lt;/ref&gt;

The [[data lake]] allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.&lt;ref&gt;http://www.hcltech.com/sites/default/files/solving_key_businesschallenges_with_big_data_lake_0.pdf&lt;/ref&gt;

==Technologies==

Big data requires exceptional technologies to efficiently process large quantities of data within tolerable elapsed times. A 2011 [[McKinsey &amp; Company|McKinsey]] report&lt;ref&gt;
{{cite book
 | last1 = Manyika
 | first1 = James
 | first2=Michael |last2=Chui |first3=Jaques |last3=Bughin |first4=Brad |last4=Brown |first5=Richard |last5=Dobbs |first6=Charles |last6=Roxburgh |first7=Angela Hung |last7=Byers
 | title = Big Data: The next frontier for innovation, competition, and productivity
 | publisher = McKinsey Global Institute
 | date = May 2011
 | url = http://www.mckinsey.com/Insights/MGI/Research/Technology_and_Innovation/Big_data_The_next_frontier_for_innovation
}}
&lt;/ref&gt;
suggests suitable technologies include [[A/B testing]],
[[crowdsourcing]],
[[data fusion]] and [[data integration|integration]],
[[genetic algorithms]],
[[machine learning]],
[[natural language processing]],
[[signal processing]],
[[simulation]],
[[time series analysis]] and
[[Visualization (computer graphics)|visualisation]].
Multidimensional big data can also be represented as [[tensor]]s, which can be more efficiently handled by tensor-based computation,&lt;ref&gt;{{cite web |title=Future Directions in Tensor-Based Computation and Modeling |date=May 2009|url=http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf}}&lt;/ref&gt; such as [[multilinear subspace learning]].&lt;ref name=&quot;MSLsurvey&quot;&gt;{{cite journal
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf
 |title=A Survey of Multilinear Subspace Learning for Tensor Data
 |journal=Pattern Recognition
 |volume=44 |number=7 |pages=1540–1551 |year=2011
 |doi=10.1016/j.patcog.2011.01.004
}}&lt;/ref&gt; Additional technologies being applied to big data include massively parallel-processing ([[Massive parallel processing|MPP]]) databases, [[search-based application]]s, [[data mining]], distributed file systems, distributed databases, cloud based infrastructure (applications, storage and computing resources) and the Internet.{{Citation needed|date=September 2011}}

Some but not all [[Massive parallel processing|MPP]] relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the [[RDBMS]].&lt;ref&gt;{{cite web |author=Monash, Curt |title=eBay’s two enormous data warehouses |date=30 April 2009 |url=http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/}}&lt;br/&gt;{{cite web |author=Monash, Curt |title=eBay followup&amp;nbsp;— Greenplum out, Teradata &gt; 10 petabytes, Hadoop has some value, and more |date=6 October 2010 |url=http://www.dbms2.com/2010/10/06/ebay-followup-greenplum-out-teradata-10-petabytes-hadoop-has-some-value-and-more/}}&lt;/ref&gt;

[[DARPA]]’s [[Topological Data Analysis]] program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called [[Ayasdi]].&lt;ref&gt;{{cite web|url=http://www.ayasdi.com/resources/|title=Resources on how Topological Data Analysis is used to analyze big data|publisher=Ayasdi}}&lt;/ref&gt;

The practitioners of big data analytics processes are generally hostile to slower shared storage,&lt;ref&gt;{{cite web |title=Storage area networks need not apply |author=CNET News |date=1 April 2011 |url=http://news.cnet.com/8301-21546_3-20049693-10253464.html}}&lt;/ref&gt; preferring direct-attached storage ([[Direct-attached storage|DAS]]) in its various forms from solid state drive ([[Ssd|SSD]]) to high capacity [[Serial ATA|SATA]] disk buried inside parallel processing nodes. The perception of shared storage architectures—[[Storage area network]] (SAN) and [[Network-attached storage]] (NAS) —is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.

Real or near-real time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in memory is good—data on spinning disk at the other end of a [[Fiber connector|FC]] [[Storage area network|SAN]] connection is not. The cost of a [[Storage area network|SAN]] at the scale needed for analytics applications is very much higher than other storage techniques.

There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners {{As of|2011|lc=on}} did not favour it.&lt;ref&gt;{{cite web |title=How New Analytic Systems will Impact Storage |date=September 2011 |url=http://www.evaluatorgroup.com/document/big-data-how-new-analytic-systems-will-impact-storage-2/}}&lt;/ref&gt;

==Applications==
[[File:2013-09-11 Bus wrapped with SAP Big Data parked outside IDF13 (9730051783).jpg|thumb|Bus wrapped with [[SAP AG|SAP]] Big data parked outside [[Intel Developer Forum|IDF13]].]]
Big data has increased the demand of information management specialists in that [[Software AG]], [[Oracle Corporation]], [[IBM]],  [[Microsoft]], [[SAP AG|SAP]], [[EMC Corporation|EMC]], [[HP]] and [[Dell]] have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10&amp;nbsp;percent a year: about twice as fast as the software business as a whole.{{r|Economist}}

Developed economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and  between 1 billion and 2 billion people accessing the internet.{{r|Economist}} Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people become more literate, which in turn leads to information growth. The world's effective capacity to exchange information through [[telecommunication]] networks was 281 [[petabytes]] in 1986, 471 [[petabytes]] in 1993, 2.2 exabytes in 2000, 65 [[exabytes]] in 2007&lt;ref name=&quot;HilbertLopez2011&quot;/&gt; and predictions put the amount of internet traffic at 667 exabytes annually by 2014.{{r|Economist}} According to one estimate, one third of the globally stored information is in the form of alphanumeric text and still image data,&lt;ref name=&quot;HilbertContent&quot;&gt;[http://www.tandfonline.com/doi/abs/10.1080/01972243.2013.873748 &quot;What Is the Content of the World's Technologically Mediated Information and Communication Capacity: How Much Text, Image, Audio, and Video?&quot;], Martin Hilbert (2014), [[The Information Society]]; free access to the article through this link: martinhilbert.net/WhatsTheContent_Hilbert.pdf&lt;/ref&gt; which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).

While many vendors offer off-the-shelf solutions for Big Data, experts recommend the development of in-house solutions custom-tailored to solve the company's problem at hand if the company has sufficient technical capabilities.&lt;ref&gt;{{cite web |url=http://www.kdnuggets.com/2014/07/interview-amy-gershkoff-ebay-in-house-BI-tools.html |title=Interview: Amy Gershkoff, Director of Customer Analytics &amp; Insights, eBay on How to Design Custom In-House BI Tools |last1=Rajpurohit |first1=Anmol |date=2014-07-11 |website=[http://kdnuggets.com KDnuggets] |accessdate=2014-07-14|quote=Dr. Amy Gershkoff: &quot;Generally, I find that off-the-shelf business intelligence tools do not meet the needs of clients who want to derive custom insights from their data. Therefore, for medium-to-large organizations with access to strong technical talent, I usually recommend building custom, in-house solutions.&quot;}}&lt;/ref&gt;

===Government===
The use and adoption of Big Data within governmental processes is beneficial and allows efficiencies in terms of cost, productivity, and innovation. That said, this process does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. Below are the thought leading examples within the Governmental Big Data space.

====United States of America====
* In 2012, the [[Presidency of Barack Obama|Obama administration]] announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government.&lt;ref name=WH_Big_Data&gt;{{cite web|last=Kalil|first=Tom|title=Big Data is a Big Deal|url=http://www.whitehouse.gov/blog/2012/03/29/big-data-big-deal|publisher=White House|accessdate=26 September 2012}}&lt;/ref&gt; The initiative is composed of 84 different big data programs spread across six departments.&lt;ref&gt;{{cite web|last=Executive Office of the President|title=Big Data Across the Federal Government|url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_fact_sheet_final_1.pdf|publisher=White House|accessdate=26 September 2012 |date=March 2012}}&lt;/ref&gt;
* Big data analysis played a large role in [[Barack Obama]]'s successful [[Barack Obama presidential campaign, 2012|2012 re-election campaign]].&lt;ref name=infoworld_bigdata&gt;{{cite web|last=Lampitt|first=Andrew|title=The real story of how big data analytics helped Obama win|url=http://www.infoworld.com/d/big-data/the-real-story-of-how-big-data-analytics-helped-obama-win-212862|work=[[Infoworld]]|accessdate=31 May 2014}}&lt;/ref&gt;
* The [[United States Federal Government]] owns six of the ten most powerful supercomputers in the world.&lt;ref&gt;{{cite web |last=Hoover |first=J. Nicholas |title=Government's 10 Most Powerful Supercomputers |url=http://www.informationweek.com/government/enterprise-applications/image-gallery-governments-10-most-powerf/224700271 |work=Information Week |publisher=UBM |accessdate=26 September 2012}}&lt;/ref&gt;
* The [[Utah Data Center]] is a data center currently being constructed by the [[United States]] [[National Security Agency]]. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few [[exabyte]]s.&lt;ref&gt;{{cite news | last=Bamford|first=James|title=The NSA Is Building the Country’s Biggest Spy Center (Watch What You Say)|url=http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1|work=Wired Magazine|accessdate=2013-03-18|date=15 March 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.nsa.gov/public_info/press_room/2011/utah_groundbreaking_ceremony.shtml|title=Groundbreaking Ceremony Held for $1.2 Billion Utah Data Center|publisher=National Security Agency Central Security Service|accessdate=2013-03-18}}&lt;/ref&gt;&lt;ref&gt;{{cite news | last=Hill|first=Kashmir|title=TBlueprints Of NSA's Ridiculously Expensive Data Center In Utah Suggest It Holds Less Info Than Thought|url=http://www.forbes.com/sites/kashmirhill/2013/07/24/blueprints-of-nsa-data-center-in-utah-suggest-its-storage-capacity-is-less-impressive-than-thought/|work=Forbes|accessdate=2013-10-31}}&lt;/ref&gt;

====India====
* Big data analysis was, in parts, responsible for the [[Bharatiya Janata Party|BJP]] and its allies to win a highly successful [[Indian general election, 2014|Indian General Election 2014]].&lt;ref&gt;{{Cite web|url = http://www.livemint.com/Industry/bUQo8xQ3gStSAy5II9lxoK/Are-Indian-companies-making-enough-sense-of-Big-Data.html|title = News: Live Mint|date = 2014-06-23|accessdate = 2014-11-22|website = Are Indian companies making enough sense of Big Data?|publisher = Live Mint - http://www.livemint.com/}}&lt;/ref&gt;
* The Indian Government utilises numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation

====United Kingdom====
Examples of uses of big data in public services: 
* Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify the considerable delay between the release of any given drug, and a UK-wide adaptation of the [[National Institute for Health and Care Excellence]] guidelines. This suggests that new/most up-to-date drugs take some time to filter through to the general patient. {{Citation needed|date=April 2015}}
* Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as 'meals on wheels'. The connection of data allowed the local authority to avoid any weather related delay.{{Citation needed|date=April 2015}}

===International development===
Research on the effective usage of [[information and communication technologies for development]] (also known as [[ICT4D]]) suggests that big data technology can make important contributions but also present unique challenges to [[International development]].&lt;ref&gt;UN GLobal Pulse (2012). Big Data for Development: Opportunities and Challenges (White p. by Letouzé, E.). New York: United Nations. Retrieved from http://www.unglobalpulse.org/projects/BigDataforDevelopment&lt;/ref&gt;&lt;ref&gt;WEF (World Economic Forum), &amp; Vital Wave Consulting. (2012). Big Data, Big Impact: New Possibilities for International Development. World Economic Forum. Retrieved 24 August 2012, from http://www.weforum.org/reports/big-data-big-impact-new-possibilities-international-development&lt;/ref&gt; Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as [[health care]], [[employment]], [[economic productivity]], crime, security, and [[natural disaster]] and resource management.&lt;ref name=&quot;HilbertBigData2013&quot;/&gt;&lt;ref&gt;{{cite web|url=http://blogs.worldbank.org/ic4d/four-ways-to-talk-about-big-data/|title=Elena Kvochko, Four Ways To talk About Big Data (Information Communication Technologies for Development Series)|publisher=worldbank.org|accessdate=2012-05-30}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Daniele Medri: Big Data &amp; Business: An on-going revolution|url=http://www.statisticsviews.com/details/feature/5393251/Big-Data--Business-An-on-going-revolution.html|publisher=[[Statistics Views]]|date=21 Oct 2013}}&lt;/ref&gt; However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.&lt;ref name=&quot;HilbertBigData2013&quot;/&gt;

===Manufacturing===
Based on TCS 2013 Global Trend Study, improvements in supply planning and product quality provide the greatest benefit of big data for manufacturing.&lt;ref name=&quot;TCS Big Data Study - Manufacturing&quot;&gt;{{cite web|url=http://sites.tcs.com/big-data-study/manufacturing-big-data-benefits-challenges/# |title=Manufacturing: Big Data Benefits and Challenges |work=[http://www.tcs.com/big-data-study/Pages/default.aspx TCS Big Data Study] |publisher=[[Tata Consultancy Services Limited]] |location=Mumbai, India |accessdate=2014-06-03}}&lt;/ref&gt; Big data provides an infrastructure for transparency in manufacturing industry, which is the ability to unravel uncertainties such as inconsistent component performance and availability. Predictive manufacturing as an applicable approach toward near-zero downtime and transparency requires vast amount of data and advanced prediction tools for a systematic process of data into useful information.&lt;ref&gt;{{cite journal|last=Lee|first=Jay|author2=Wu, F. |author3=Zhao, W. |author4=Ghaffari, M. |author5= Liao, L |title=Prognostics and health management design for rotary machinery systems—Reviews, methodology and applications|journal=Mechanical Systems and Signal Processing|date=Jan 2013|volume=42|issue=1}}&lt;/ref&gt; A conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics, vibration, pressure, current, voltage and controller data. Vast amount of sensory data in addition to historical data construct the big data in manufacturing. The generated big data acts as the input into predictive tools and preventive strategies such as [[Prognostics|Prognostics and Health Management]] (PHM).&lt;ref name=SMELettersPaper /&gt;

====Cyber-Physical Models====
Current PHM implementations mostly utilize data during the actual usage while analytical algorithms can perform more accurately when more information throughout the machine’s lifecycle, such as system configuration, physical knowledge and working principles, are included. There is a need to systematically integrate, manage and analyze machinery or process data during different stages of machine life cycle to handle data/information more efficiently and further achieve better transparency of machine health condition for manufacturing industry.

With such motivation a cyber-physical (coupled) model scheme has been developed. Please see http://www.imscenter.net/cyber-physical-platform  The coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge. It can also be described as a 5S systematic approach consisting of Sensing, Storage, Synchronization, Synthesis and Service. The coupled model first constructs a digital image from the early design stage. System information and physical knowledge are logged during product design, based on which a simulation model is built as a reference for future analysis. Initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation. After that step, the simulation model can be considered a mirrored image of the real machine—able to continuously record and track machine condition during the later utilization stage. Finally, with the increased connectivity offered by cloud computing technology, the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited.&lt;ref name=&quot;MfgLetters&quot;/&gt;&lt;ref&gt;[[Predictive manufacturing system]]&lt;/ref&gt;

=== Media ===

==== Internet of Things (IoT) ====
{{main|Internet of Things}}
To understand how the media utilises Big Data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that [[wikt:practitioner|practitioner]]s in Media and Advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead tap into consumers with technologies that reach targeted people at optimal times in optimal locations.  The ultimate aim is to serve, or convey, a message or content that is (statistically speaking) in line with the consumers mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various [[data-mining]] activities.&lt;ref&gt;{{cite journal|last1=Couldry|first1=Nick|last2=Turow|first2=Joseph|title=Advertising, Big Data, and the Clearance of the Public Realm: Marketers’ New Approaches to the Content Subsidy|journal=International Journal of Communication|date=2014|volume=8|pages=1710–1726}}&lt;/ref&gt;

* Targeting of consumers (for advertising by marketers) 
* Data-capture

Big Data and the IoT work in conjunction.  From a media perspective, data is the key derivative of device inter connectivity and allows accurate targeting.  The [[Internet of Things]], with the help of big data, therefore transforms the media industry, companies and even governments, opening up a new era of economic growth and competitiveness. The intersection of people, data and intelligent algorithms have far-reaching impacts on media efficiency. The wealth of data generated allows an elaborate layer on the present targeting mechanisms of the industry.

==== Technology ====
* [[eBay.com]] uses two data warehouses at 7.5 [[petabytes]] and 40PB as well as a 40PB [[Hadoop]] cluster for search, consumer recommendations, and merchandising.  [http://www.itnews.com.au/News/342615,inside-ebay8217s-90pb-data-warehouse.aspx Inside eBay’s 90PB data warehouse]
* [[Amazon.com]] handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world’s three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.&lt;ref&gt;{{cite web|last=Layton |first=Julia |url=http://money.howstuffworks.com/amazon1.htm |title=Amazon Technology |publisher=Money.howstuffworks.com |accessdate=2013-03-05}}&lt;/ref&gt;
* [[Facebook]] handles 50 billion photos from its user base.&lt;ref&gt;{{cite web|url=https://www.facebook.com/notes/facebook-engineering/scaling-facebook-to-500-million-users-and-beyond/409881258919 |title=Scaling Facebook to 500 Million Users and Beyond |publisher=Facebook.com |accessdate=2013-07-21}}&lt;/ref&gt;
* As of August 2012, [[Google]] was handling roughly 100 billion searches per month.&lt;ref&gt;{{cite web|url=http://searchengineland.com/google-1-trillion-searches-per-year-212940|title=Google Still Doing At Least 1 Trillion Searches Per Year|date=16 January 2015|work=Search Engine Land|accessdate=15 April 2015}}&lt;/ref&gt;
* [[Oracle NoSQL Database]] has been tested to past the 1M ops/sec mark with 8 shards and proceeded to hit 1.2M ops/sec with 10 shards.&lt;ref&gt;{{cite web |last=Lamb |first=Charles |url=https://blogs.oracle.com/charlesLamb/entry/oracle_nosql_database_exceeds_1 |title=Oracle NoSQL Database Exceeds 1 Million Mixed YCSB Ops/Sec}}&lt;/ref&gt;

===Private sector===

====Retail====
* [[Walmart]] handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data&amp;nbsp;– the equivalent of 167 times the information contained in all the books in the US [[Library of Congress]].{{r|Economist}}

====Retail Banking====
* FICO  Card Detection System protects accounts world-wide.&lt;ref name=&quot;fico.com&quot;&gt;{{cite web|url=http://www.fico.com/en/Products/DMApps/Pages/FICO-Falcon-Fraud-Manager.aspx |title=FICO® Falcon® Fraud Manager |publisher=Fico.com |accessdate=2013-07-21}}&lt;/ref&gt;
* The volume of business data worldwide, across all companies, doubles every 1.2 years, according to estimates.&lt;ref name=&quot;KnowWPCarey.com&quot;&gt;{{cite web|url=http://knowwpcarey.com/article.cfm?cid=25&amp;aid=1171 |title=eBay Study: How to Build Trust and Improve the Shopping Experience |publisher=Knowwpcarey.com |date=2012-05-08 |accessdate=2013-03-05}}&lt;/ref&gt;&lt;ref&gt;[http://www.statista.com/statistics/280444/global-leading-priorities-for-big-data-according-to-business-and-it-executives/ Leading Priorities for Big Data for Business and IT]. eMarketer. October 2013. Retrieved January 2014.&lt;/ref&gt;

====Real Estate====
* [[Windermere Real Estate]] uses anonymous GPS signals from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.&lt;ref&gt;{{cite news|last=Wingfield |first=Nick |url=http://bits.blogs.nytimes.com/2013/03/12/predicting-commutes-more-accurately-for-would-be-home-buyers/ |title=Predicting Commutes More Accurately for Would-Be Home Buyers - NYTimes.com |publisher=Bits.blogs.nytimes.com |date=2013-03-12 |accessdate=2013-07-21}}&lt;/ref&gt;

===Science===
The  [[Large Hadron Collider]] experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99.99995% &lt;ref&gt;{{cite web|last1=Alexandru|first1=Dan|title=Prof|url=https://cds.cern.ch/record/1504817/files/CERN-THESIS-2013-004.pdf|website=cds.cern.ch|publisher=CERN|accessdate=24 March 2015}}&lt;/ref&gt; of these streams, there are 100 collisions of interest per second.&lt;ref&gt;{{cite web |title=LHC Brochure, English version. A presentation of the largest and the most powerful particle accelerator in the world, the Large Hadron Collider (LHC), which started up in 2008. Its role, characteristics, technologies, etc. are explained for the general public. |url=http://cds.cern.ch/record/1278169?ln=en |work=CERN-Brochure-2010-006-Eng. LHC Brochure, English version. |publisher=CERN |accessdate=20 January 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=LHC Guide, English version. A collection of facts and figures about the Large Hadron Collider (LHC) in the form of questions and answers. |url=http://cds.cern.ch/record/1092437?ln=en |work=CERN-Brochure-2008-001-Eng. LHC Guide, English version. |publisher=CERN |accessdate=20 January 2013}}&lt;/ref&gt;&lt;ref name=&quot;nature&quot;&gt;{{Cite news |title=High-energy physics: Down the petabyte highway |work= Nature |date= 19 January 2011 |first=Geoff |last=Brumfiel |doi= 10.1038/469282a |volume= 469 |pages= 282–83 |url= http://www.nature.com/news/2011/110119/full/469282a.html }}&lt;/ref&gt;
* As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.
* If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 [[exabyte]]s per day, before replication. To put the number in perspective, this is equivalent to 500 [[quintillion]] (5×10&lt;sup&gt;20&lt;/sup&gt;) bytes per day, almost 200 times more than all the other sources combined in the world.

The [[Square Kilometre Array]] is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day.&lt;ref&gt;http://www.zurich.ibm.com/pdf/astron/CeBIT%202013%20Background%20DOME.pdf&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://arstechnica.com/science/2012/04/future-telescope-array-drives-development-of-exabyte-processing/|title=Future telescope array drives development of exabyte processing|work=Ars Technica|accessdate=15 April 2015}}&lt;/ref&gt; It is considered one of the most ambitious scientific projects ever undertaken.{{cn|date=July 2015}}

====Science and research====
* When the [[Sloan Digital Sky Survey]] (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the [[Large Synoptic Survey Telescope]], successor to SDSS, comes online in 2016, its designers expect it to acquire that amount of data every five days.{{r|Economist}}
* Decoding the [[Human Genome Project|human genome]] originally took 10 years to process, now it can be achieved in less than a day. the DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by [[Moore's Law]].&lt;ref&gt;Delort P., OECD ICCP Technology Foresight Forum, 2012. http://www.oecd.org/sti/ieconomy/Session_3_Delort.pdf#page=6&lt;/ref&gt;
* The [http://www.nasa.gov/centers/goddard/news/releases/2010/10-051.html [[Nasa|NASA]] Center for Climate Simulation (NCCS)] stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.&lt;ref&gt;{{cite web|last=Webster|first=Phil|title=Supercomputing the Climate: NASA's Big Data Mission|url=http://www.csc.com/cscworld/publications/81769/81773-supercomputing_the_climate_nasa_s_big_data_mission|work=CSC World|publisher=Computer Sciences Corporation|accessdate=2013-01-18}}&lt;/ref&gt;
* Google’s DNAStack, compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any ‘friction points,’ or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics allows scientists to use the vast sample of resources from Google’s search server to scale social experiments that would usually take years, instantly.{{cn|date=July 2015}}

==Research activities==
Encrypted search and cluster formation in big data was demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at ''Tackling the challenges of Big Data'' by [[MIT Computer Science and Artificial Intelligence Laboratory]] and Dr. Amir Esmailpour at UNH Research Group investigated the key features of big data as formation of clusters and their interconnections. They focused on the security of big data and the actual orientation of the term towards the presence of different type of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.&lt;ref&gt;{{cite conference |url=http://asee-ne.org/proceedings/2014/Student%20Papers/210.pdf |title=Encrypted Search &amp; Cluster Formation in Big Data |last1=Siwach |first1=Gautam |last2=Esmailpour |first2=Amir |date=March 2014 |year= |conference=ASEE 2014 Zone I Conference |conference-url=http://ubconferences.org/ |location=[[University of Bridgeport]], [[Bridgeport, Connecticut]], [[USA]] }}&lt;/ref&gt;

In March 2012, The White House announced a national &quot;Big Data Initiative&quot; that consisted of six Federal departments and agencies committing more than $200 million to big data research projects.&lt;ref&gt;{{cite web |title=Obama Administration Unveils &quot;Big Data&quot; Initiative:Announces $200 Million In New R&amp;D Investments|publisher=The White House |url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_press_release_final_2.pdf}}&lt;/ref&gt;

The initiative included a National Science Foundation &quot;Expeditions in Computing&quot; grant of $10 million over 5 years to the AMPLab&lt;ref&gt;{{cite web|url=http://amplab.cs.berkeley.edu |title=AMPLab at the University of California, Berkeley |publisher=Amplab.cs.berkeley.edu |accessdate=2013-03-05}}&lt;/ref&gt; at the University of California, Berkeley.&lt;ref&gt;{{cite web |title=NSF Leads Federal Efforts In Big Data|date=29 March 2012|publisher=National Science Foundation (NSF) |url=http://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&amp;org=NSF&amp;from=news}}&lt;/ref&gt; The AMPLab also received funds from [[DARPA]], and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion&lt;ref&gt;{{cite conference|url=https://amplab.cs.berkeley.edu/publication/scaling-the-mobile-millennium-system-in-the-cloud-2/|author1=Timothy Hunter|date=October 2011|author2=Teodor Moldovan|author3=Matei Zaharia|author4=Justin Ma|author5=Michael Franklin|author6=Pieter Abbeel|author7=Alexandre Bayen|title=Scaling the Mobile Millennium System in the Cloud}}&lt;/ref&gt; to fighting cancer.&lt;ref&gt;{{cite news|title=Computer Scientists May Have What It Takes to Help Cure Cancer|author=David Patterson|publisher=The New York Times|date=5 December 2011|url=http://www.nytimes.com/2011/12/06/science/david-patterson-enlist-computer-scientists-in-cancer-fight.html?_r=0}}&lt;/ref&gt;

The White House Big Data Initiative also included a commitment by the  Department of Energy to provide $25 million in funding over 5 years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,&lt;ref&gt;{{cite web|title=Secretary Chu Announces New Institute to Help Scientists Improve Massive Data Set Research on DOE Supercomputers |publisher=&quot;energy.gov&quot; |url=http://energy.gov/articles/secretary-chu-announces-new-institute-help-scientists-improve-massive-data-set-research-doe}}&lt;/ref&gt; led by the Energy Department’s [[Lawrence Berkeley National Laboratory]]. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the Department’s supercomputers.

The U.S. state of [[Massachusetts]] announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.&lt;ref&gt;{{cite web |title=Governor Patrick announces new initiative to strengthen Massachusetts’ position as a World leader in Big Data |publisher=Commonwealth of Massachusetts |url=http://www.mass.gov/governor/pressoffice/pressreleases/2012/2012530-governor-announces-big-data-initiative.html}}&lt;/ref&gt;  The [[Massachusetts Institute of Technology]] hosts the Intel Science and Technology Center for Big Data in the [[MIT Computer Science and Artificial Intelligence Laboratory]], combining government, corporate, and institutional funding and research efforts.&lt;ref&gt;{{cite web|url=http://bigdata.csail.mit.edu/ |title=Big Data @ CSAIL |publisher=Bigdata.csail.mit.edu |date=2013-02-22 |accessdate=2013-03-05}}&lt;/ref&gt;

The European Commission is funding the 2-year-long [http://big-project.eu Big Data Public Private Forum] through their [[Seventh Framework Program]] to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for [http://ec.europa.eu/research/horizon2020/index_en.cfm?pg=h2020 Horizon 2020], their next [[Framework Programmes for Research and Technological Development|framework program]].&lt;ref&gt;{{cite web|url=http://cordis.europa.eu/search/index.cfm?fuseaction=proj.document&amp;PJ_RCN=13267529 |title=Big Data Public Private Forum |publisher=Cordis.europa.eu |date=2012-09-01 |accessdate=2013-03-05}}&lt;/ref&gt;

The British government announced in March 2014 the founding of the [[Alan Turing Institute]], named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyse large data sets.&lt;ref&gt;{{cite news|url=http://www.bbc.co.uk/news/technology-26651179|title=Alan Turing Institute to be set up to research big data|publisher=[[BBC News]]|accessdate=2014-03-19|date=19 March 2014}}&lt;/ref&gt;

At the [[University of Waterloo Stratford Campus]] Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.&lt;ref&gt;{{cite web|url=http://www.betakit.com/event/inspiration-day-at-university-of-waterloo-stratford-campus/|title=Inspiration day at University of Waterloo, Stratford Campus |publisher=http://www.betakit.com/|accessdate=2014-02-28}}&lt;/ref&gt;

To make manufacturing more competitive in the United States (and globe), there is a need to integrate more American ingenuity and innovation into manufacturing ; Therefore, National Science Foundation has granted the Industry University cooperative research [http://www.imscenter.net center for Intelligent Maintenance Systems (IMS)] at [[university of Cincinnati]] to focus on developing advanced predictive tools and techniques to be applicable in a big data environment.&lt;ref name=SMELettersPaper&gt;{{cite web|title=Center for Intelligent Maintenance Systems (IMS Center)|url=http://imscenter.net}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Lee|first=Jay|author2=Lapira, Edzel |author3=Bagheri, Behrad |author4= Kao, Hung-An |title=Recent Advances and Trends in Predictive Manufacturing Systems in Big Data Environment|journal=Manufacturing Letters|year=2013|volume=1|issue=1|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114|DOI=10.1016/j.mfglet.2013.09.005 |pages=38–41}}&lt;/ref&gt;  In May 2013, IMS Center held an industry advisory board meeting focusing on big data where presenters from various industrial companies discussed their concerns, issues and future goals in Big Data environment.

Computational social sciences&amp;nbsp;— Anyone can use Application Programming Interfaces (APIs) provided by Big Data holders, such as Google and Twitter, to do research in the social and behavioral sciences.&lt;ref name=pigdata&gt;{{cite journal|last=Reips|first=Ulf-Dietrich|author2=Matzat, Uwe |title=Mining &quot;Big Data&quot; using Big Data Services |journal=International Journal of Internet Science|year=2014|volume=1|issue=1|pages=1–8 | url=http://www.ijis.net/ijis9_1/ijis9_1_editorial_pre.html}}&lt;/ref&gt; Often these APIs are provided for free.&lt;ref name=&quot;pigdata&quot;/&gt; [[Tobias Preis]] ''et al.'' used [[Google Trends]] data to demonstrate that Internet users from countries with a higher per capita gross domestic product (GDP) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviour and real-world economic indicators.&lt;ref&gt;{{cite journal |first1=Tobias |last1=Preis |first2=Helen Susannah |last2=Moat, |first3=H. Eugene |last3=Stanley |first4=Steven R. |last4=Bishop |title=Quantifying the Advantage of Looking Forward |journal=Scientific Reports |volume= 2 |page=350 |year=2012 |doi=10.1038/srep00350 |pmid=22482034 |pmc=3320057}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.newscientist.com/article/dn21678-online-searches-for-future-linked-to-economic-success.html | title=Online searches for future linked to economic success |first=Paul |last=Marks |work=New Scientist | date=5 April 2012 | accessdate=9 April 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://arstechnica.com/gadgets/news/2012/04/google-trends-reveals-clues-about-the-mentality-of-richer-nations.ars | title=Google Trends reveals clues about the mentality of richer nations |first=Casey |last=Johnston |work=Ars Technica | date=6 April 2012 | accessdate=9 April 2012}}&lt;/ref&gt; The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (‘2011’) to the volume of searches for the previous year (‘2009’), which they call the ‘[[future orientation index]]’.&lt;ref&gt;{{cite web | url = http://www.tobiaspreis.de/bigdata/future_orientation_index.pdf | title = Supplementary Information: The Future Orientation Index is available for download | author = Tobias Preis | date = 2012-05-24 | accessdate = 2012-05-24}}&lt;/ref&gt; They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP. The results hint that there may potentially be a relationship between the economic success of a country and the information-seeking behavior of its citizens captured in big data.

[[Tobias Preis]] and his colleagues [[Helen Susannah Moat]] and [[H. Eugene Stanley]] introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.&lt;ref&gt;{{cite web | url=http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=[[Philip Ball]] | work=Nature | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt; Their analysis of [[Google]] search volume for 98 terms of varying financial relevance, published in ''[[Scientific Reports]]'',&lt;ref&gt;{{cite journal | author=Tobias Preis, Helen Susannah Moat and H. Eugene Stanley | title=Quantifying Trading Behavior in Financial Markets Using Google Trends | journal=[[Scientific Reports]] | volume= 3 | pages=1684 | year=2013 | doi=10.1038/srep01684}}&lt;/ref&gt; suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.&lt;ref&gt;{{cite news | url=http://bits.blogs.nytimes.com/2013/04/26/google-search-terms-can-predict-stock-market-study-finds/ | title= Google Search Terms Can Predict Stock Market, Study Finds | author=Nick Bilton | work=[[New York Times]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://business.time.com/2013/04/26/trouble-with-your-investment-portfolio-google-it/ | title=Trouble With Your Investment Portfolio? Google It! | author=Christopher Matthews | work=[[TIME Magazine]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url= http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=Philip Ball |work=[[Nature (journal)|Nature]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.businessweek.com/articles/2013-04-25/big-data-researchers-turn-to-google-to-beat-the-markets | title='Big Data' Researchers Turn to Google to Beat the Markets | author=Bernhard Warner | work=[[Bloomberg Businessweek]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.independent.co.uk/news/business/comment/hamish-mcrae/hamish-mcrae-need-a-valuable-handle-on-investor-sentiment-google-it-8590991.html | title=Hamish McRae: Need a valuable handle on investor sentiment? Google it | author=Hamish McRae | work=[[The Independent]] | date=28 April 2013 | accessdate=9 August 2013 | location=London}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.ft.com/intl/cms/s/0/e5d959b8-acf2-11e2-b27f-00144feabdc0.html | title= Google search proves to be new word in stock market prediction | author=Richard Waters | work=[[Financial Times]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.forbes.com/sites/davidleinweber/2013/04/26/big-data-gets-bigger-now-google-trends-can-predict-the-market/ | title=Big Data Gets Bigger: Now Google Trends Can Predict The Market | author=David Leinweber | work=[[Forbes]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.bbc.co.uk/news/science-environment-22293693 | title=Google searches predict market moves | author=Jason Palmer | work=[[BBC]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;

==Critique==
Critiques of the big data paradigm come in two flavors, those that question the implications of the approach itself, and those that question the way it is currently done.
[[File:Big data cartoon t gregorius.jpg|thumb|Cartoon critical of big data application, by T. Gregorius]]

===Critiques of the big data paradigm===
&quot;A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data&quot;.&lt;ref name=&quot;Editorial&quot;/&gt; In their critique, Snijders, Matzat, and [[Ulf-Dietrich Reips|Reips]] point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. [[Mark Graham]] has leveled broad critiques at [[Chris Anderson (writer)|Chris Anderson]]'s assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts.&lt;ref&gt;{{cite news |author=Graham M. |title=Big data and the end of theory? |newspaper=The Guardian |url=http://www.guardian.co.uk/news/datablog/2012/mar/09/big-data-theory |location=London |date=9 March 2012}}&lt;/ref&gt; Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, &quot;big data&quot;, no matter how comprehensive or well analyzed, must be complemented by &quot;big judgment,&quot; according to an article in the Harvard Business Review.&lt;ref&gt;{{cite web|title=Good Data Won't Guarantee Good Decisions. Harvard Business Review|url=http://hbr.org/2012/04/good-data-wont-guarantee-good-decisions/ar/1|work=Shah, Shvetank; Horne, Andrew; Capellá, Jaime;|publisher=HBR.org|accessdate=8 September 2012}}&lt;/ref&gt;

Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably &quot;informed by the world as it was in the past, or, at best, as it currently is&quot;.&lt;ref name=&quot;HilbertBigData2013&quot;&gt;[http://papers.ssrn.com/abstract=2205145 &quot;Big Data for Development: From Information- to Knowledge Societies&quot;], Martin Hilbert (2013), SSRN Scholarly Paper No. ID 2205145). Rochester, NY: Social Science Research Network; http://papers.ssrn.com/abstract=2205145&lt;/ref&gt;  Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change, the past can say little about the future. For this, it would be necessary to have a thorough understanding of the systems dynamic, which implies theory.&lt;ref&gt;Anderson, C. (2008, 23 June). The End of Theory: The Data Deluge Makes the Scientific Method Obsolete. Wired Magazine, (Science: Discoveries). http://www.wired.com/science/discoveries/magazine/16-07/pb_theory&lt;/ref&gt;  As a response to this critique it has been suggested to combine big data approaches with computer simulations, such as [[agent-based model]]s&lt;ref name=&quot;HilbertBigData2013&quot;/&gt; and [[Complex Systems]].&lt;ref name=&quot;corporate&quot;&gt;Braha, D.;  Stacey, B.; Bar-Yam, Y. (2011). [http://necsi.edu/affiliates/braha/Journal_Version_SON_Braha.pdf &quot;Corporate Competition: A Self-organized Network&quot;.] Social Networks 33: 219–230.&lt;/ref&gt; Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.&lt;ref&gt;Rauch, J. (2002). Seeing Around Corners. The Atlantic, (April), 35–48. http://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/&lt;/ref&gt;&lt;ref&gt;Epstein, J. M., &amp; Axtell, R. L. (1996). Growing Artificial Societies: Social Science from the Bottom Up. A Bradford Book.&lt;/ref&gt; In addition, use of multivariate methods that probe for the latent structure of the data, such as [[factor analysis]] and [[cluster analysis]], have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.

In health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.&lt;ref&gt;Delort P., Big data in Biosciences, Big Data Paris, 2012 http://www.bigdataparis.com/documents/Pierre-Delort-INSERM.pdf#page=5&lt;/ref&gt;
A new postulate is accepted now in biosciences: the information provided by the data in huge volumes ([[omics]]) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.{{Citation needed|date=April 2015}}
In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor.{{Citation needed|date=April 2015}} The search logic is reversed and the limits of induction (&quot;Glory of Science and Philosophy scandal&quot;, [[C. D. Broad]], 1926) are to be considered.{{Citation needed|date=April 2015}}

[[Consumer privacy|Privacy]] advocates are concerned about the threat to privacy represented by increasing storage and integration of [[personally identifiable information]]; expert panels have released various policy recommendations to conform practice to expectations of privacy.&lt;ref&gt;{{cite web |first=Paul |last=Ohm |title=Don't Build a Database of Ruin |publisher=Harvard Business Review |url=http://blogs.hbr.org/cs/2012/08/dont_build_a_database_of_ruin.html}}&lt;/ref&gt;&lt;ref&gt;Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/12/03/iron-cagebook/ Iron Cagebook - The Logical End of Facebook's Patents],'' [[Counterpunch.org]], 2013.12.03&lt;/ref&gt;&lt;ref&gt;Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/09/11/inside-the-tech-industrys-startup-conference/  Inside the Tech industry’s Startup Conference],'' [[Counterpunch.org]], 2013.09.11&lt;/ref&gt;

===Critiques of big data execution===
Big data has been called a &quot;fad&quot; in scientific research and its use was even made fun of as an absurd practice in a satirical example on &quot;pig data&quot;.&lt;ref name=&quot;pigdata&quot;/&gt; &lt;!-- Please stop edit warring, and go to the discussion page on the proper capitalization of danah boyd. Thank you! Wikipedia uses *English language*, and the first word in a sentence is capitalized. --&gt;Researcher [[danah boyd]]&lt;!-- Read the previous comment. Sentences start with uppercase in English. Go to the discussion page before changing! --&gt; has raised concerns about the use of big data in [[science]] neglecting principles such as choosing a [[Sampling (statistics)|representative sample]] by being too concerned about actually handling the huge amounts of data.&lt;ref name=&quot;danah&quot;&gt;{{cite web | url=http://www.danah.org/papers/talks/2010/WWW2010.html | title=Privacy and Publicity in the Context of Big Data | author=[[danah boyd]] | work=[[World Wide Web Conference|WWW 2010 conference]] | date=2010-04-29 | accessdate = 2011-04-18}}&lt;/ref&gt; This approach may lead to results [[Bias (statistics)|bias]] in one way or another. Integration across heterogeneous data resources—some that might be considered &quot;big data&quot; and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.&lt;ref&gt;{{cite journal |last1=Jones |first1=MB |last2=Schildhauer |first2=MP |last3=Reichman |first3=OJ |last4=Bowers |first4=S |title=The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere |journal=Annual Review of Ecology, Evolution, and Systematics |volume=37 |issue=1 |pages=519–544 |year=2006 |doi=10.1146/annurev.ecolsys.37.091305.110031 |url=http://www.pnamp.org/sites/default/files/Jones2006_AREES.pdf |format=PDF}}&lt;/ref&gt;
In the provocative article &quot;Critical Questions for Big Data&quot;,&lt;ref name=&quot;danah2&quot;&gt;{{cite doi | 10.1080/1369118X.2012.678878}}&lt;/ref&gt; the authors title big data a part of [[mythology]]: &quot;large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy&quot;. Users of big data are often &quot;lost in the sheer volume of numbers&quot;, and &quot;working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth&quot;.&lt;ref name=&quot;danah2&quot; /&gt; Recent developments in BI domain, such as pro-active reporting especially target improvements in usability of Big Data, through automated filtering of non-useful data and correlations.&lt;ref name=&quot;Big Decisions White Paper&quot;&gt;[http://www.fortewares.com/Administrator/userfiles/Banner/forte-wares--pro-active-reporting_EN.pdf Failure to Launch: From Big Data to Big Decisions], Forte Wares.&lt;/ref&gt;

Big data analysis is often shallow compared to analysis of smaller data sets.&lt;ref name=&quot;kdnuggets-berchthold&quot;&gt;{{cite web|url=http://www.kdnuggets.com/2014/08/interview-michael-berthold-knime-research-big-data-privacy-part2.html|title=Interview: Michael Berthold, KNIME Founder, on Research, Creativity, Big Data, and Privacy, Part 2|date=2014-08-12|author=Gregory Piatetsky|authorlink=Gregory I. Piatetsky-Shapiro|publisher=[[KDnuggets]]|accessdate=2014-08-13}}&lt;/ref&gt; In many big data projects, there is no large data analysis happening, but the challenge is the [[extract, transform, load]] part of data preprocessing.&lt;ref name=&quot;kdnuggets-berchthold&quot;/&gt;

Big data is a [[buzzword]] and a &quot;vague term&quot;,&lt;ref name=&quot;ft-harford&quot;&gt;{{cite web |url=http://www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.html |title=Big data: are we making a big mistake? |last1=Harford |first1=Tim |date=2014-03-28 |website=[[Financial Times]] |publisher=[[Financial Times]] |accessdate=2014-04-07}}&lt;/ref&gt; but at the same time an &quot;obsession&quot;&lt;ref name=&quot;ft-harford&quot;/&gt; with entrepreneurs, consultants, scientists and the media. Big data showcases such as [[Google Flu Trends]] failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, [[Academy awards]] and election predictions solely based on Twitter were more often off than on target.
Big data often poses the same challenges as small data; and adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. [[Google Translate]]—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.
On the other hand, big data may also introduce new problems, such as the [[multiple comparisons problem]]: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.
Ioannidis argued that &quot;most published research findings are false&quot; &lt;ref name=&quot;Ioannidis&quot;&gt;{{cite doi | 10.1371/journal.pmed.0020124 }}&lt;/ref&gt; due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a &quot;significant&quot; result being actually false grows fast - even more so, when only positive results are published.
&lt;!-- sorry, this started overlapping with above section more and more... merging is welcome; I already dropped the intended subheadline &quot;Hype cycle and inflated expectations&quot;. --&gt;

==See also==
{{portal|Information technology}}
{{colbegin|colwidth=30em}}
* [[Apache Accumulo]]
* [[Apache Hadoop]]
* [[Big Data to Knowledge]]
* [[Data Defined Storage]]
* [[Data mining]]
* [[Cask (company)]]
* [[Cloudera]]
* [[HPCC]] Systems
* [[Intelligent Maintenance Systems]]
* [[Internet of Things]]
* [[MapReduce]]
* [[Hortonworks]]
* [[Oracle NoSQL Database]]
* [[Nonlinear system identification]]
* [[Operations research]]
* [[Programming with Big Data in R]] (a series of [[R (programming language)|R]] packages)
* [[Sqrrl]]
* [[Supercomputer]]
* [[Talend]]
* [[Transreality gaming]]
* [[Tuple space]]
* [[Unstructured data]]
{{colend}}

==References==
{{Reflist|2|refs=

&lt;ref name=&quot;Economist&quot;&gt;
{{cite news |title=Data, data everywhere |url=http://www.economist.com/node/15557443 |newspaper=The Economist |date=25 February 2010 |accessdate=9 December 2012}}
&lt;/ref&gt;

}}

==Further reading==
* {{cite journal | last1 = Sharma | first1 = Sugam | last2 = Tim | first2 = Udoyara S | last3 = Wong | first3 = Johnny | last4 = Gadia | first4 = Shashi | last5 = Sharma | first5 = Subhash | year = 2014 | title = A BRIEF REVIEW ON LEADING BIG DATA MODELS | url = https://www.jstage.jst.go.jp/article/dsj/13/0/13_14-041/_pdf | journal = Data Science Journal | volume = 13 | issue = | page = }}
* [http://arxiv.org/abs/1312.4722 Big Data Computing and Clouds: Challenges, Solutions, and Future Directions]. Marcos D. Assuncao, Rodrigo N. Calheiros, Silvia Bianchi, Marco A. S. Netto, Rajkumar Buyya. Technical Report CLOUDS-TR-2013-1, Cloud Computing and Distributed Systems Laboratory, The University of Melbourne, 17 Dec. 2013.
* [http://ubconferences.org/wp-content/uploads/2014/04/ASEE-Zone-1-Conference-Outcome-Report.pdf Encrypted search &amp; cluster formation in Big Data]. Gautam Siwach, Dr. A. Esmailpour. American Society for Engineering Education, Conference at the University of Bridgeport, Bridgeport, Connecticut 3–5 April 2014.
* {{cite web|url=http://www.odbms.org/download/BigDataforGood.pdf|title=Big Data for Good|publisher=ODBMS.org|date=5 June 2012|accessdate=2013-11-12}}
* {{cite journal
 | last1 = Hilbert
 | first1 = Martin
 | first2 = Priscila |last2=López
 | title = The World's Technological Capacity to Store, Communicate, and Compute Information
 | journal = Science
 | volume = 332
 | issue = 6025
 | pages = 60–65
 | year = 2011
 | doi = 10.1126/science.1200970
 | pmid = 21310967 |url=http://martinhilbert.net/WorldInfoCapacity.html |ref=harv
}}
* {{cite web|url=http://www.ge-ip.com/library/detail/13476/?cid=wiki_Rise_of_Industrial_Big_Data|title=The Rise of Industrial Big Data|publisher=GE Intelligent Platforms|accessdate=2013-11-12}}
* [http://www.winshuttle.com/big-data-timeline/ History of Big Data Timeline]. A visual history of Big Data with links to supporting articles.

==External links==
*{{Commons category inline|Big data}}
*{{Wiktionary-inline|big data}}
*[http://bigdata.csail.mit.edu MIT Big Data Initiative] / [https://mitprofessionalx.edx.org Tackling the Challenges of Big Data]

{{Database models}}
{{Databases}}
{{Software engineering}}
{{Use dmy dates|date=January 2012}}

{{Authority control}}

{{DEFAULTSORT:Big Data}}
[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Technology forecasting]]
[[Category:Transaction processing]]
[[Category:Big data| ]]</text>
      <sha1>nxvslwrencfgaxjax86p0fi97f9xck6</sha1>
    </revision>
  </page>
  <page>
    <title>IT Operations Analytics</title>
    <ns>0</ns>
    <id>44234764</id>
    <revision>
      <id>664473609</id>
      <parentid>658965501</parentid>
      <timestamp>2015-05-28T21:16:31Z</timestamp>
      <contributor>
        <username>Pcampbell21223</username>
        <id>13193715</id>
      </contributor>
      <minor/>
      <comment>/* Removed Vendor Link*/</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6376">&lt;!--- Important, do not remove this line before article has been created. ---&gt;

In the fields of [[information technology]] and [[systems management]], '''IT Operations Analytics (ITOA)''' is an approach or method applied to application software designed to retrieve, analyze and report data for IT operations. ITOA has been described as applying [[big data analytics]] to the IT realm.&lt;ref&gt;{{cite web|title=The Time Has Come: Analytics Delivers for IT Operations|url=http://www.datacenterjournal.com/it/time-analytics-delivers-operations/|publisher=Data Center Journal|accessdate=18 February 2013}}&lt;/ref&gt; In its ''Hype Cycle Report'', [[Gartner]] rated the business impact of ITOA as being ‘high’, meaning that its use will see businesses enjoy significantly increased revenue or cost saving opportunities.&lt;ref&gt;{{cite web|title=IT operations analytics: Changing the IT perspective|url=http://www.information-age.com/technology/data-centre-and-it-infrastructure/123457805/it-operations-analytics-changing-it-perspective#sthash.qCUlOnyW.dpuf|publisher=Information Age|accessdate=13 March 2014}}&lt;/ref&gt;

==Definition==
IT Operations Analytics (ITOA) (also known as Advanced Operational Analytics,&lt;ref&gt;{{cite web|title=Advanced Operations Analytics - What the Data Shows!|url=http://www.apmdigest.com/advanced-operations-analytics-what-the-data-shows|publisher=APM Digest |accessdate=17 September 2014}}&lt;/ref&gt; or IT Data Analytics&lt;ref&gt;{{cite web|title=Quintica offers BMC’s TrueSight
|url=http://it-online.co.za/2014/10/27/quintica-offers-bmcs-truesight/|publisher=IT-Online |accessdate=27 October 2014}}&lt;/ref&gt;)  technologies are primarily used to discover complex patterns in high volumes of often &quot;noisy&quot; IT system availability and performance data.&lt;ref&gt;{{cite web|title=Hype Cycle for IT Operations Management, 2013|url=https://www.gartner.com/doc/2556718/hype-cycle-it-operations-management|publisher=Gartner|accessdate=23 July 2013}}&lt;/ref&gt; [[Forrester Research]] defines IT analytics as &quot;The use of mathematical algorithms and other innovations to extract meaningful information from the sea of raw data collected by management and monitoring technologies.&quot;&lt;ref&gt;{{cite web|title=Turn Big Data Inward With IT Analytics|url=https://www.forrester.com/Turn+Big+Data+Inward+With+IT+Analytics/fulltext/-/E-RES75501?objectid=RES75501|publisher=Forrester Research|accessdate=5 December 2012}}&lt;/ref&gt;

==Applications==
ITOA systems tend to be used by IT operations teams, and [[Gartner]] describes five applications of ITOA systems:&lt;ref&gt;{{cite web|title=IT Market Clock for IT Operations Management, 2013 |url=https://www.gartner.com/doc/2573216/it-market-clock-it-operations|publisher=Gartner|accessdate=13 August 2013}}&lt;/ref&gt; 
* '''Root Cause Analysis:'''  The models, structures and pattern descriptions of IT infrastructure or application stack being monitored can help users pinpoint fine-grained and previously unknown root causes of overall system behavior pathologies.
* '''Proactive Control of Service Performance and Availability:''' Predicts future system states and the impact of those states on performance.
* '''Problem Assignment:''' Determines how problems may be resolved or, at least, direct the results of inferences to the most appropriate individuals or communities in the enterprise for problem resolution.
* '''Service Impact Analysis:''' When multiple root causes are known, the  analytics system's output is used to determine and rank the relative impact, so that resources can be devoted to correcting the fault in the most timely and cost-effective way possible.
* '''Complement Best-of-breed Technology:''' The models, structures and pattern descriptions of IT infrastructure or application stack being monitored are used to correct or extend the outputs of other discovery-oriented tools to improve the fidelity of information used in operational tasks (e.g., service dependency maps, application runtime architecture topologies, network topologies).

==Types==
In their ''Data Growth Demands a Single, Architected IT Operations Analytics Platform'', [[Gartner]] Research describes five types of analytics technologies:&lt;ref&gt;{{cite web|title=Data Growth Demands a Single, Architected IT Operations Analytics Platform|url=https://www.gartner.com/doc/2599016/data-growth-demands-single-architected|publisher=Gartner|accessdate=30 September 2013}}&lt;/ref&gt; 
* [[Log analysis]]
* Unstructured text indexing, search and inference (UTISI)
* Topological analysis (TA)
* Multidimensional database search and analysis (MDSA)
* Complex operations event processing (COEP)
* Statistical pattern discovery and recognition (SPDR)

==Tools and ITOA Platforms==
&lt;!-- Do not list product without articles or add external links, they will be removed. --&gt;
A number of vendors operate in the ITOA space:
{{columns-list|3|
*[[Accelops]]
*[[BMC Software|BMC]]
*[[CA Technologies|CA]]
*[[ExtraHop Networks]]
*[[Evolven]]
*[[HP]]
*[[IBM]]
*[[Metafor Software]]
*[[Nastel]]
*[[Nexthink]]
*[[Riverbed Technology|Riverbed]]
*[[SolarWinds]]
*[[Splunk]]
*[[Sumo Logic]]
*[[TeamQuest]]
*[[VMTurbo]]
*[[VMWare]]
*[[Zenoss]]
}}

==External links==
* ITOA Landscape: ''[http://www.itoa-landscape.org ITOA Landscape]'' 
* International Data Corporation (IDC): ''[http://www.idc.com/getdoc.jsp?containerId=248847 Service Management: Big Data Opportunities Abound for IT Operations Analytics]'' (May 2014)
* NetworkWorld: ''[http://www.networkworld.com/article/2451067/big-data-business-intelligence/it-operations-analytics-itoa-provides-real-time-monitoring-of-large.html Understanding big data analytics]'' (July 7, 2014)
* Enterprise Management Associates (EMA): ''[http://www.enterprisemanagement.com/research/asset.php/2803/EMA-Research-Report:-The-Many-Faces-of-Advanced-Operations-Analytics The Many Faces of Advanced Operations Analytics]'' (September 23, 2014)

==See also==
*[[Application Performance Management]]
*[[Big Data]]
*[[Business intelligence tools]]
*[[Information technology operations]]

==References==
{{reflist|25em}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;
*
*
*
*

[[Category:Application software]]
[[Category:Analytics]]
[[Category:Big data]]
[[Category:Enterprise architecture]]</text>
      <sha1>5hyfwfo37abqzvx7vwhfr1rqlpc0dfi</sha1>
    </revision>
  </page>
  <page>
    <title>Michael B.T. Bell</title>
    <ns>0</ns>
    <id>41829700</id>
    <revision>
      <id>651977040</id>
      <parentid>651974333</parentid>
      <timestamp>2015-03-18T20:35:04Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Primary sources}} {{Blp sources}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8023">{{multiple issues|
{{blp sources|date=March 2015}}
{{primary sources|date=March 2015}}
}}

{{Infobox person
| name = Michael B.T. Bell
| image =Bell bt michael.jpg
| caption = 
| alt = 
| birth_name = 
| birth_date = 1957
| birth_place =  
| death_date = 
| death_place = 
| death_cause = 
| body_discovered = 
| nationality = American
| other_names = 
| years_active = 
| spouse = 
| alma_mater = [[City University of New York]] 
| children = 
| parents = 
| known_for = 
| occupation = architect
| callsign = 
}}

'''Michael B.T. Bell''' (born 1957) is an American enterprise [[software architect]], chiefly recognized for developing the [[service-oriented modeling#Service-oriented modeling framework|Service-oriented modeling framework (SOMF)]]&lt;ref name=&quot;Bell&quot;&gt;{{cite book |last=Bell |first=Michael|title=Service-Oriented Modeling: Service Analysis, Design, and Architecture|year= 2008 |publisher=Wiley &amp; Sons|isbn=978-0-470-14111-3 |chapter=Introduction to Service-Oriented Modeling}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last= Thuraisingham |first=Bhavani|title= Secure Semantic Service-Oriented Systems    |year= 2010|publisher=CRC Press|isbn=9781420073324 | pp=42,43,152,153}}&lt;/ref&gt;&lt;ref name=&quot;Hyberstson and Duane W.&quot;&gt;{{cite book |last=Hybertson|first=Duane|title=Model-oriented Systems Engineering Science: A Unifying Framework for Traditional and Complex Systems (Complex and Enterprise Systems Engineering)|year= 2012 |publisher=Auerbach Publication|isbn=978-1420072518|pages=256, 329}}&lt;/ref&gt;&lt;ref name=&quot;Thuraisingham&quot;&gt;{{cite book |last=Thuraisingham|first=Bhavani|title=Developing and Securing the Cloud|year= 2013|publisher=CRC Press|isbn=9781439862919 |page=87}}&lt;/ref&gt;&lt;ref name=&quot;Buyya&quot;&gt;{{cite book |last=Buyya|first=Rugkumar|title=Mastering Cloud Computing |year= 2013|publisher=Tata McGraw-Hill |isbn=9781259029950 |pages=2–30}}&lt;/ref&gt; and the [[cloud computing]] modeling notation (CCMN).&lt;ref&gt;{{cite web|url=  http://sparxsystems.com/somf|title=Cloud Computing Modeling Notation|publisher= Sparx Systems}}&lt;/ref&gt; His innovative research and publications in the fields of [[software architecture]], [[service-oriented architecture]], [[model-driven engineering]], [[cloud computing]], and [[big data]] are recognized internationally for their contribution to the [[software design]] and development communities.

== Biography ==

Bell earned his computer science [[master’s degree]] in 1992 from the [[City University of New York]] ([[CUNY]]).

After graduation, as a software developer and [[enterprise architect]] consultant, he dedicated his career to improving business and technological operations of financial institutions in [[Wall Street]]. He developed innovative software [[algorithm]]s and methodologies for high-volume [[Electronic trading platform]]s. This included modules for execution of trading applications, persistence methods for large volumes of data, and design of  high-speed network and [[internet]] software implementations.

He has worked for [[J.P. Morgan Chase]], [[Citibank]], [[UBS| UBS-Paine Webber]], [[Deutsche Bank]], [[American Express]], [[TD Waterhouse]], [[Pfizer]], [[AIG]], and [[Prudential Financial|Prudential]].

== Bell’s Service-Oriented Modeling Methodology ==

[[File:SOMF V 2.0.jpg|thumb|360px|right|Service-Oriented Modeling Framework (SOMF) Version 2.0]]
In 2008 Bell introduced the [[service-oriented modeling#Service-oriented modeling framework|Service-oriented modeling framework (SOMF)]] &lt;ref name=&quot;Sosinsky&quot;&gt;{{cite book |last=Sosinsky |first= Barrie |title=Cloud Computing Bible|year= 2010 |publisher=Wiley &amp; Sons|isbn=9781118023990 |pp=288,289}}&lt;/ref&gt;&lt;ref name=&quot;Juan Trujillo at All&quot;&gt;{{cite book |last=Trujillo |first= Juan |title= Advances in Conceptual Modeling – Applications and Challenges|year= 2010 |publisher= Springer Science &amp; Business Media |isbn=9783642163845 |pp=87,88}}&lt;/ref&gt; to the software development community in his book Service-Oriented Modeling.&lt;ref&gt;{{cite web|url=http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470141115.html|title=Service-Oriented Modeling|first=Michael|last=Bell|publisher= Wiley &amp; Sons}}&lt;/ref&gt;

The service framework, driven by [[service-oriented modeling#Discipline-specific modeling|Discipline-specific modeling]], was devised to encourage consolidation of software assets, reduction of systems redundancy, and acceleration of time-to-market. SOMF  &lt;ref name=&quot;Dustdar&quot;&gt;{{cite book |last=Dustdar |first=Schahram|title=Service Engineering: European Research Results  |year= 2010 |publisher=Springer Science &amp; Business Media|isbn=9783709104156|pp=112,xi}}&lt;/ref&gt; includes a modeling language and a life cycle methodology (see image on the far right) suited for narrowing the gap between the business and the information technology organizations in the enterprise.

The framework also includes modeling disciplines and practices of software systems, for the purpose of designing software applications. Furthermore, SOMF &lt;ref name=&quot;Bell&quot;&gt;{{cite book |last=Bell |first=Michael|title=SOA Modeling Patterns for Service Oriented Discovery and Analysis|year= 2009 |publisher=Wiley &amp; Sons|isbn=9780470579695 |pp=185,240}}&lt;/ref&gt;           offers a variety of architectural styles, such as [[enterprise architecture]], [[application architecture]], [[service-oriented architecture]],&lt;ref&gt;{{cite web|url=http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471768944.html|title=Service-Oriented Architecture|first=Michael|last=Bell|publisher= Wiley &amp; Sons}}&lt;/ref&gt; and [[cloud computing]].

== Publications ==

Michael Bell has published several books and articles. The following is a selection:
* 2005. &quot;An Organizational Model: The AOM-3, Architecture Organization Structure and Role Models&quot;. IP Publishing. ISBN 978-0-9896935-3-0
* 2006. &quot;Service-Oriented Architecture: A Planning and Implementation Guide for Business and Technology&quot;. With Eric Marks. Wiley &amp; Sons. ISBN 978-0471768944
* 2008. &quot;Service-Oriented Modeling: Service Analysis, Design, and Architecture&quot;. Wiley &amp; Sons. ISBN 978-0470141113
* 2010. &quot;SOA Modeling Patterns for Service Oriented Discovery and Analysis&quot;. Wiley &amp; Sons. ISBN 978-0470481974
* 2011. Service Oriented Modeling Specifications for SOMF. Includes Service design and cloud computing.

== References ==

{{reflist}}

== External links ==

* {{cite web
 | url = http://www.modelingconcepts.com/pages/download.htm
 | title = SOMF Examples &amp; Language Notation
 | format = Softcopy | publisher = Michael Bell/Methodologies Corporation
}}

* {{cite web
 | url = http://itknowledgeexchange.techtarget.com/soa-talk/cloud-computing-modeling-notation-ccmn-for-somf-targets-cloud-services/
 | title = Interview with Michael Bell about Cloud Computing Modeling Notation 
 | format = Softcopy | publisher = tknowledgeexchange.techtarget.com online publishing
}}

* {{cite web
 | url = http://dghart-cse564.wikispaces.asu.edu/Service-Oriented+Modeling+Framework/
 | title = State University of Arizona: Service-Oriented Modeling Framework (SOMF) studies and research  
 | format = Softcopy | publisher = Arizona State University
}}

* {{cite web
 | url = http://www.youtube.com/watch?v=ohPFC-9zuLs/
 | title = Service-Oriented Architecture strategies video   
 | format = Softcopy | publisher = Michael Bell
}}

* {{cite web
 | url = http://www.sparxsystems.com/somf
 | title = Service-Oriented and Cloud Computing SOMF Modeling Tools   
 | format = Softcopy | publisher = Sparx Systems
}}

* {{cite web
 | url = http://www.enterprisemodelingsolutions.com/content/somf
 | title = Service-Oriented and Cloud Computing SOMF Modeling Solutions   
 | format = Softcopy | publisher = CEPHAS Corporation 
}}

{{Software engineering}}

{{Persondata
| NAME = Bell, Michael
| SHORT DESCRIPTION = Software architect
| DATE OF BIRTH = 1957
| PLACE OF BIRTH = 
| DATE OF DEATH = 
| PLACE OF DEATH = 
}}
{{DEFAULTSORT:Bell, Michael}}
[[Category:Living people]]
[[Category:Software engineers]]
[[Category:Service-oriented architecture]]
[[Category:American software engineers]]
[[Category:Big data]]
[[Category:1957 births]]</text>
      <sha1>cap05xdcd1gc29gc6ev74hgw8uurge8</sha1>
    </revision>
  </page>
  <page>
    <title>People analytics</title>
    <ns>0</ns>
    <id>44311693</id>
    <revision>
      <id>633494425</id>
      <parentid>633494406</parentid>
      <timestamp>2014-11-12T07:17:59Z</timestamp>
      <contributor>
        <username>Berek</username>
        <id>79</id>
      </contributor>
      <comment>added [[Category:Big data]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8338">'''People analytics''' is a rapidly growing area of [[business intelligence]] and [[big data]] technology that uses snippets of people-related data to optimize business outcomes and solve business problems. Experts within the field, including [[Ben Waber]] and [[Alex Pentland]], contend that people analytics tools can be effectively applied across all parts of the modern enterprise in order to transform operations and workforces. The proliferation of electronic communications combined with big data advancements provides unprecedented insights for organizational managers via people analytics applications.

== Applications ==
People analytics applications are capable of implementation across a range of industries, from public utilities to software firms. Additionally, people analytics is effectively applied to a wide variety of organizational units, including human resources&lt;ref&gt;{{cite web|author= |url=http://www.oracle.com/us/products/applications/human-capital-management/talent-analytics-and-big-data-2063584.pdf|title=Talent analytics and big data- the challenge for HR|publisher=Chartered Institute of Personnel and Development|date=2013-11-01}}&lt;/ref&gt; (using retention data and talent acquisition) and sales&lt;ref&gt;{{cite web|author=Tawheed Kader|url=http://insights.wired.com/profiles/blogs/how-analytics-is-about-to-transform-the-salesperson-s-job#axzz3IEK4Sj56|title=How Analytics is About to Transform the Salesperson's Job|publisher=WIRED|date=2014-03-14}}&lt;/ref&gt; (using outreach tactics and communication practices), as well as overall employee engagement.&lt;ref&gt;{{cite web|author=Thomas Davenport, Jeanne Harris, &amp; Jeremy Shapiro |url=http://hbr.org/2010/10/competing-on-talent-analytics/ar/1|title=Competing on Talent Analytics|publisher=Harvard Business Review|date=2010-10-01}}&lt;/ref&gt;
 
Data science research has repeatedly identified positive correlations between strong internal networks within organizations and employee productivity and engagement. People analytics tools are also capable of comprehensive analysis at multiple levels within organizations. Specifically, people analytics data can be reviewed at an individual level (e.g. for hiring purposes), at the unit level (e.g. to identify significant team-related trends), and at the organizational level (e.g. to identify optimal mix of resources for specific organizational goals).

In a 2013 [[Bloomberg Businessweek]] article, Ben Waber describes the versatility of people analytics tools:&lt;ref&gt;{{cite web|author=Ben Waber|url=http://www.businessweek.com/articles/2013-05-16/the-next-big-thing-in-big-data-people-analytics|title=The Next Big Thing in Big Data: People Analytics|publisher=Bloomberg Businessweek|date=2013-05-16}}&lt;/ref&gt;
 
{{quotation |  “People analytics transforms our understanding of socialization in the workplace, the impact of office layout, and even concepts as “soft” as creativity. In the future, we will use this knowledge to create new ways of organizing people that radically improve the way we work. Office layouts that respond to social context and real-time feedback on communication patterns and interaction styles are new levers enabled by people analytics that no one could have imagined.”}}

== Business Cases ==
The benefits of people analytics applications have been documented in a number of studies conducted within high-profile global firms.

[[Bank of America]] implemented people analytics solutions in order to mitigate widespread employee dissatisfaction in specific business units. At times, the company experienced annual employee turnover as high as 40% in its US call centers. After introducing a tailored big data analytics program to compare performance metrics, the company recognized that inter-office collaboration was highly correlated with employee success. In order to increase the level of organizational collaboration, Bank of America revised the employee break schedule in order to maximize time for communication and network building. This simple update to work schedules resulted a 23% increase in call handling rates and an 18% improvement in employee cohesion within just 3 months, saving the company $15 million.&lt;ref&gt;{{cite web|author=Alison Griswold|url=http://www.businessinsider.com/bank-of-america-call-center-management-2014-2 |title=This One Simple Management Change Saved Bank of America $15 Million|publisher=Business Insider|date=2014-02-15}}&lt;/ref&gt;

In a 2014 [[Harvard Business Review]] article, [[VoloMetrix]] CEO Ryan Fuller discusses how his firm provided people analytics solutions to the sales unit of a large B2B software company.&lt;ref&gt;{{cite web|author=Ryan Fuller|url=http://blogs.hbr.org/2014/08/3-behaviors-that-drive-successful-salespeople/|title=3 Behaviors that Drive Successful Salespeople|publisher=Harvard Business Review|date=2014-08-20}}&lt;/ref&gt;  Sales data was collected over 18 months in order to analyze successful employee sales habits. At the conclusion of the study, data scientists discovered that the size of internal networks and time spent with managers was highly predictive of exceptional sales achievements. The insights extracted from the people analytics tools were especially beneficial due to their actionability. Employees and managers were offered specific metrics to analyze personal sales tactics, as well as clear, prioritized goals for improved sales operations and productivity.

== Occupational Impact ==
The recent recognition of people analytics as a valuable business segment has paved the way for a multitude of new career paths in the corporate world, largely among global enterprises including [[Google]]&lt;ref&gt;{{cite web|author=Adam Bryant|url=http://www.nytimes.com/2011/03/13/business/13hire.html?pagewanted=all|title=Google's Quest to Build a Better Boss|publisher=The New York Times|date=2011-03-12}}&lt;/ref&gt; and [[Facebook]].&lt;ref&gt;{{cite web|author=|url=https://www.facebook.com/careers/department?dept=it&amp;req=a0IA000000G3OM0MAN|title=Quantitative Analyst - People Analytics|publisher=Facebook|date=}}&lt;/ref&gt; Interpretation of people analytics data often requires advanced understanding of statistics and data science.&lt;ref&gt;{{cite web|author=Steven Pearlstein |url=http://www.washingtonpost.com/business/people-analytics-moneyball-for-human-resources/2014/08/01/3a8fb6ac-1749-11e4-9e3b-7f2f110c6265_story.html|title=People analytics: ‘Moneyball’ for human resources|publisher=Washington Post|date=2014-08-01}}&lt;/ref&gt; Due to the rapid advancement of big data capabilities and people analytics applications, many firms predict a surge in data-related career opportunities over the next decade. Brian Sullivan, CEO of CTParters (a global executive search firm), made the following statement regarding the firm's &quot;Hot Jobs 2015&quot; report&lt;ref&gt;{{cite web|author=|url=http://www.businesswire.com/news/home/20141104006117/en/Hot-Jobs-2015-Hiring-Trends-Forecast-Leading#.VFvr4PldWEU|title=Hot Jobs 2015: Hiring Trends Forecast from Leading Executive Search Firm CTPartners|publisher=BusinessWire|date=2014-11-04}}&lt;/ref&gt; released in Fall 2014:

{{quotation | &quot;We are seeing a proliferation of executive-level roles borne out of the global influence of big data. Businesses are looking for leaders who can not only understand the massive amounts of information available to them but also identify the threats and opportunities that come as a result of this evolving landscape. As such, analytical roles such as head of personalization and head of people analytics ... are the new ‘in-demand’ roles that will be key to maintaining a company’s competitive advantage in 2015.&quot;&lt;ref&gt;{{cite web|author=|url=http://finance.yahoo.com/news/hot-jobs-2015-hiring-trends-151900442.html|title=Hot Jobs 2015: Hiring Trends Forecast from Leading Executive Search Firm CTPartners|publisher=Yahoo! Finance|date=2014-11-04}}&lt;/ref&gt;}}

== Research and Literature ==
[[Alex Pentland]] has described the benefits of people analytics and big data applications at length in Social Physics.&lt;ref&gt;{{cite web|author= |url=http://socialphysics.media.mit.edu/book/|title=Social Physics|publisher=MIT|date= }}&lt;/ref&gt; Pentland highlights numerous people analytics applications and studies conducted at large global enterprises by his team and various [[MIT]] research programs.

==References==
{{Reflist}}



[[Category:Business intelligence]]
[[Category:Big data]]</text>
      <sha1>phn8vpnpe4lv4346wr4yab45jjqe160</sha1>
    </revision>
  </page>
  <page>
    <title>Industry 4.0</title>
    <ns>0</ns>
    <id>39773873</id>
    <revision>
      <id>671419125</id>
      <parentid>670008970</parentid>
      <timestamp>2015-07-14T15:53:25Z</timestamp>
      <contributor>
        <ip>74.219.229.178</ip>
      </contributor>
      <comment>/* Impact of the Industry 4.0 */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14774">{{advert|date=December 2014}}
'''Industry 4.0''' is a collective term for technologies and concepts of [[value chain organization]].&lt;ref name=&quot;Definition-I4.0&quot;&gt;Hermann, Pentek, Otto, 2015: [http://www.snom.mb.tu-dortmund.de/cms/de/forschung/Arbeitsberichte/Design-Principles-for-Industrie-4_0-Scenarios.pdf Design Principles for Industrie 4.0 Scenarios], Last download on 03. February 2015&lt;/ref&gt; Based on the technological concepts of [[cyber-physical system]]s, the [[Internet of Things]]&lt;ref&gt;[[Jürgen Jasperneite]]:''[http://www.computer-automation.de/steuerungsebene/steuernregeln/fachwissen/article/93559/0/Was_hinter_Begriffen_wie_Industrie_40_steckt/ Was hinter Begriffen wie Industrie 4.0 steckt].'' In: ''Computer &amp; Automation'', 19. Dezember 2012; Last download on 23. December 2012&lt;/ref&gt; and the [[Internet of Services]],&lt;ref&gt;Kagermann, H., W. Wahlster and J. Helbig, eds., 2013: Recommen-dations for implementing the strategic initiative Industrie 4.0: Final report of the Industrie 4.0 Working Group.&lt;/ref&gt; it facilitates the vision of the Smart Factory. Within the modular structured Smart Factories of Industry 4.0, [[cyber-physical system]]s monitor physical processes, create a virtual copy of the physical world and make decentralized decisions. Over the [[Internet of Things]], [[Cyber-physical system]]s communicate and cooperate with each other and humans in real time. Via the [[Internet of Services]], both internal and cross-organizational services are offered and utilized by participants of the value chain.&lt;ref name=&quot;Definition-I4.0&quot; /&gt;

Meanwhile, in the United States, an initiative known as the Smart Manufacturing Leadership Coalition is also working on the future of manufacturing. Smart Manufacturing Leadership Coalition (SMLC) is a non-profit organization of manufacturing practitioners, suppliers, and technology companies; manufacturing consortia; universities; government agencies and laboratories. The aim of this coalition is to enable stakeholders in the manufacturing industry to form collaborative R &amp; D, implementation and advocacy groups for development of the approaches, standards, platforms and shared infrastructure that facilitate the broad adoption of manufacturing intelligence.&lt;ref&gt;[http://smartmanufacturingcoalition.org/ smartmanufacturingcoalition.org]&lt;/ref&gt;

Similarly, GE has been working on an initiative called 'The Industrial Internet'.&lt;ref&gt;[The Industrial Internet, http://www.ge.com/docs/chapters/Industrial_Internet.pdf]&lt;/ref&gt; The Industrial Internet aims to bring together the advances of two transformative revolutions: the myriad machines, facilities, fleets and networks that arose from the Industrial
Revolution, and the more recent powerful advances in computing, information and communication systems brought to the fore by the Internet Revolution. According to GE, together these developments bring together three elements, which embody the essence 
of the Industrial Internet: Intelligent machines, advanced analytics and people at work.

== Name ==
The term &quot;Industrie 4.0&quot; refers to the fourth [[industrial revolution]]. It originates from a project in the high-tech strategy of the [[German government]], which promotes the [[Digital Revolution|computerization]] of manufacturing.&lt;ref&gt;[http://www.bmbf.de/de/19955.php Zukunftsprojekt Industrie 4.0]&lt;/ref&gt; The first industrial revolution was the mechanization of production using water and steam power. The [[second industrial revolution]] then introduced mass production with the help of [[electric power]], followed by the [[digital revolution]] and the use of electronics and IT to further automate production.&lt;ref&gt;[http://boerse.ard.de/meldungen/chart-infografik-evolution-industrie100~_v-large.jpg Die Evolution zur Industrie 4.0 in der Produktion] Last download on 14. April 2013&lt;/ref&gt;

The term Industrie 4.0 was first used in 2011 at the [[Hanover Fair]].&lt;ref&gt;[http://www.vdi-nachrichten.com/artikel/Industrie-4-0-Mit-dem-Internet-der-Dinge-auf-dem-Weg-zur-4-industriellen-Revolution/52570/1 Industrie 4.0: Mit dem Internet der Dinge auf dem Weg zur 4. industriellen Revolution, VDI-Nachrichten, April 2011]&lt;/ref&gt; In October 2012 the Working Group on Industry 4.0 chaired by Siegfried Dais ([[Robert Bosch GmbH]]) and [[Henning Kagermann|Kagermann]] ([[acatech]]) presented a set of Industry 4.0 implementation recommendations to the German federal government. On 8 April 2013 at the Hanover Fair the final report of the Working Group Industry 4.0 was presented.&lt;ref&gt;[http://www.plattform-i40.de Industrie 4.0 Plattform] Last download on 15. Juli 2013&lt;/ref&gt;

== Design Principles ==
There are six design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios.&lt;ref name=&quot;Definition-I4.0&quot; /&gt;
*Interoperability: the ability of [[cyber-physical system]]s (i.e. workpiece carriers, assembly stations and products), humans and Smart Factories to connect and communicate with each other via the [[Internet of Things]] and the [[Internet of Services]]
*Virtualization: a virtual copy of the Smart Factory which is created by linking sensor data (from monitoring physical processes) with virtual plant models and simulation models 
*Decentralization: the ability of [[cyber-physical system]]s within Smart Factories to make decisions on their own
*Real-Time Capability: the capability to collect and analyse data and provide the derived insights immediately 
*Service Orientation: offering of services (of [[cyber-physical system]]s, humans or Smart Factories) via the [[Internet of Services]] 
*Modularity: flexible adaptation of Smart Factories to changing requirements by replacing or expanding individual modules

== Meaning ==
Characteristic for industrial production in an Industry 4.0 environment are the strong customization of products under the conditions of high flexibilized (mass-) production. The required  automation technology is improved by the introduction of methods of self-optimization, self-configuration,&lt;ref&gt;[http://www.youtube.com/watch?v=JtC3DAfLTxw  Selbstkonfiguierende Automation für Intelligente Technische Systeme], Video, last download on 27. Dezember 2012&lt;/ref&gt; Self-diagnosis, cognition and intelligent support of workers in their increasingly complex work.&lt;ref&gt;[[Jürgen Jasperneite]]; Oliver, Niggemann: Intelligente Assistenzsysteme zur Beherrschung der Systemkomplexität in der Automation. In: ATP edition - Automatisierungstechnische Praxis, 9/2012, Oldenbourg Verlag, München, September 2012&lt;/ref&gt; The largest project in Industry 4.0 at the present time is the BMBF leading-edge cluster &quot;Intelligent Technical Systems OstWestfalenLippe (it's OWL)&quot;. Another major project is the BMBF project RES-COM,&lt;ref&gt;[http://www.res-com-projekt.de/ Projekt RES-COM]&lt;/ref&gt; as well as the Cluster of Excellence &quot;Integrative Production Technology for High-Wage Countries&quot;.&lt;ref&gt;[http://www.production-research.de/ Webseite Exzellenzcluster &quot;Integrative Produktionstechnik für Hochlohnländer&quot;, Last download on 15. July 2013]&lt;/ref&gt;

== Effects ==
In June 2013, consultancy firm McKinsey &lt;ref&gt;[http://www.mckinsey.com/insights/business_technology/the_internet_of_things_and_the_future_of_manufacturing The Internet of Things and the future of manufacturing],&lt;/ref&gt; released an interview featuring an expert discussion between executives at Robert Bosch - Siegfried Dais (Partner of the Robert Bosch Industrietreuhand KG) and Heinz Derenbach (CEO of Bosch Software Innovations GmbH), and McKinsey experts. This interview addressed the prevalence of the Internet of Things in manufacturing and the consequent technology-driven changes that promise to trigger a new industrial revolution. At Bosch, and generally in Germany, this phenomenon is referred to as Industry 4.0. The basic principle of Industry 4.0 is that by connecting machines, work pieces and systems, we are creating intelligent networks along the entire value chain that can control each other autonomously.

Some examples for Industry 4.0 are machines that predict failures and trigger maintenance processes autonomously or self-organized logistics that react to unexpected changes in the production.

According to Siegfried Dais, “it is highly likely that the world of production will become more and more networked until everything is interlinked with everything else.” While this sounds like a fair assumption and the driving force behind the Internet of Things, it also means that the complexity of production and supplier networks will grow enormously. Networks and processes have so far been limited to one factory. But in an Industry 4.0 scenario, these boundaries of individual factories will most likely no longer exist. Instead, they will be lifted in order to interconnect multiple factories or even geographical regions.

There are differences between a typical factory today and an Industry 4.0 factory. In the current industry environment, providing high-end quality service or product with the least cost is the key to success and industrial factories are trying to achieve as much performance as possible to increase their profit as well as their reputation. In this way, various data sources are available to provide worthwhile information about different aspects of the factory. In this stage, the utilization of data for understanding the current condition and detecting faults and failures is an important topic to research. e. g. in production, there are various commercial tools available to provide OEE (Overall Equipment Effectiveness) information to factory management in order to highlight root cause of problems and possible faults in the system. In contrast, in an Industry 4.0 factory, in addition to condition monitoring and fault diagnosis, components and systems are able to gain self-awareness and self-predictiveness, which will provide management with more insight on the status of the factory. Furthermore, peer-to-peer comparison and fusion of health information from various components provides a precise health prediction in component and system levels and force factory management to trigger required maintenance at the best possible time to reach just-in time maintenance and gain near zero downtime.&lt;ref&gt;Lee, Jay, Industry 4.0 in Big Data Environment, Harting Tech News 26, 2013, http://www.harting.com/fileadmin/harting/documents/lg/hartingtechnologygroup/news/tec-news/tec-news26/EN_tecNews26.pdf&lt;/ref&gt;

== Challenges ==
# Lack of adequate skill-sets to expedite the march towards fourth industrial revolution
# Threat of redundancy of the corporate IT department
# General reluctance to change by stakeholders

== Role of big data and analytics ==
Modern information and communication technologies like Cyber-Physical Systems, Big Data or Cloud Computing will help predict the possibility to increase productivity, quality and flexibility within the manufacturing industry and thus to understand advantages within the competition.

Big Data Analytics consists of 6Cs in the integrated Industry 4.0 and Cyber Physical Systems environment. 6C system that is consist of Connection (sensor and networks), Cloud (computing and data on demand), Cyber (model &amp; memory), Content/context (meaning and correlation), Community (sharing &amp; collaboration), and Customization (personalization and value).  In this scenario and in order to provide useful insight to the factory management and gain correct content, data has to be processed with advanced tools (analytics and algorithms) to generate meaningful information. Considering the presence of visible and invisible issues in an industrial factory, the information generation algorithm has to be capable of detecting and addressing invisible issues such as machine degradation, component wear, etc in the factory floor.&lt;ref name=INDIN2014&gt;{{cite journal|last1=Lee|first1=Jay|last2=Bagheri|first2=Behrad|last3=Kao|first3=Hung-An|title=Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics|journal=IEEE Int. Conference on Industrial Informatics (INDIN) 2014|date=2014|url=https://www.researchgate.net/profile/Behrad_Bagheri/publication/266375284_Recent_Advances_and_Trends_of_Cyber-Physical_Systems_and_Big_Data_Analytics_in_Industrial_Informatics/links/542dc0100cf27e39fa948a7d?origin=publication_detail}}&lt;/ref&gt;&lt;ref name=MfgLetters&gt;{{cite journal|last1=Lee|first1=Jay|last2=Lapira|first2=Edzel|last3=Bagheri|first3=Behrad|last4=Kao|first4=Hung-an|title=Recent advances and trends in predictive manufacturing systems in big data environment|journal=Manufacturing Letters|volume=1|issue=1|pages=38–41|doi=10.1016/j.mfglet.2013.09.005|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114}}&lt;/ref&gt;

== Impact of the Industry 4.0 ==
The fourth industrial revolution will affect many areas. Five key impact areas emerge:
#Machine safety
#Industry value chain
#Workers
#Socio-economic
#Industry Demonstration: To help industry understand the impact of Industry 4.0, Cincinnati Mayor, John Cranley, signed a proclamation to state &quot;Cincinnati to be Industry 4.0 Demonstration City&quot;.&lt;ref&gt;http://www.imscenter.net/IMS_news/cincinnati-mayor-proclaimed-cincinnati-to-be-industry-4-0-demonstration-city&lt;/ref&gt;

==See also==
* [[Big data]]
* [[Computer-integrated manufacturing]]
* [[Digital modeling and fabrication]]
* [[Industrial control system]]
* [[Industrial Internet]]
* [[Intelligent Maintenance Systems]]
* [[Internet of Things]]
* [[Machine to machine]]
* [[Predictive manufacturing system]]
* [[SCADA]]

==References==
{{reflist}}

==External links==
* [http://carre-strauss.com/documents/IoT_Roadmap.pdf Roadmap to the Internet of Things - Its Impact, Architecture &amp; Future Governance] - Mark Fell, Carré &amp; Strauss, 2014.
* [[Cloud-Based Design and Manufacturing|Cloud-based design and manufacturing]]
* [http://www.hightech-strategie.de/de/59.php Industrie 4.0] – Hightech-Strategie der Bundesregierung
* [http://www.bmbf.de/de/19955.php Bundesministerium für Forschung und Entwicklung] - Zukunftsprojekt Industrie 4.0
* [http://www.plattform-i40.de Plattform Industrie 4.0]
* [http://www.plattform-i40.de/sites/default/files/Report_Industrie%204.0_engl_1.pdf Recommendations for implementing the strategic initiative INDUSTRIE 4.0] English edition - www.plattform-i40.de/
* [http://www.its-owl.de BMBF-Spitzencluster„Intelligente technische Systeme OstwestfalenLippe it's OWL]
* [http://www.production-research.de Exzellenzcluster Integrative Produktionstechnik für Hochlohnländer]
* [http://www.imscenter.net]

{{DEFAULTSORT:Industry 4.0}}
[[Category:Industrial automation]]
[[Category:Industrial computing]]
[[Category:Internet of Things]]
[[Category:Technology forecasting]]
[[Category:Big data]]
[[Category:Industrial Revolution]]</text>
      <sha1>grszjzosx7uykxli9ystnezo0se9yyt</sha1>
    </revision>
  </page>
  <page>
    <title>Barclays Capital Inc. v. Theflyonthewall.com, Inc.</title>
    <ns>0</ns>
    <id>44379989</id>
    <revision>
      <id>662245175</id>
      <parentid>644980476</parentid>
      <timestamp>2015-05-14T02:33:14Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor/>
      <comment>/* Story of the Case */per [[WP:HYPHEN]], sub-subsection 3, points 3,4,5, replaced: highly- → highly using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19086">{{Infobox court case
| name = Barclay's Capital Inc. v. Theflyonthewall.com, Inc.
| court = [[United States Court of Appeals for the Second Circuit]]
| date argued = August 6, 2010
| date decided = June 20, 2011
| opinions = Sack - majority, Raggi - separate concurring
| full name = Barclay's Capital Inc., Merrill Lynch, Pierce, Fenner &amp; Smith Inc., and Morgan Stanley &amp; Co. Inc., Plaintiffs-Appellees, v. Theflyonthewall.com, Inc., Defendant-Appellant.
| judges =  Pooler, Raggi, and Sack
| keywords = [[Hot News]], [[Misappropriation]], [[Intellectual Property]], [[Unfair Competition]]
}}

'''''Barclay's Capital Inc. v. Theflyonthewall.com, Inc.''''', [[case citation|650 F. 3d 876]] (2nd Cir. 2011), was a case decided in the [[United States Court of Appeals for the Second Circuit]] where the Second Circuit, reversing the decision&lt;ref name=&quot;DistrictOpinion&quot; /&gt; of the US District Court below it, found that the claims of three major financial investment firms ([[Barclays Investment Bank]], [[Morgan Stanley]], and [[Merrill Lynch]]) against an [[internet]] subscription stock news service (theflyonthewall.com) for [[&quot;Hot-news&quot; Misappropriation]] under state [[common law]] doctrine could not stand, as they were pre-empted by several sections of the Federal [[Copyright Act]] ({{usc|17|106}}, {{usc|17|102}}, and {{usc|17|103}}).&lt;ref name=&quot;2ndCircuitOpinion&quot; /&gt;

The case was significant because the Second Circuit redefined elements of the &quot;Hot-news&quot; Misappropriation [[tort]], breaking with earlier Second Circuit [[precedent]] by holding that such precedent was not binding, but merely ''[[obiter dicta]]''.&lt;ref name=&quot;2ndCircuitOpinion&quot; /&gt; According to some legal commentators, the decision would have the effect of making it more difficult for future plaintiffs to bring claims under the hot-news misappropriation doctrine, at least in the Second Circuit.&lt;ref name=&quot;skadden&quot; /&gt;

==Story of the Case==

The [[defendant]], Theflyonthewall.com, Inc., was a company that was in the business of obtaining, by various means, daily [[stock]] recommendations prepared by the [[plaintiffs]], [[Lehman Brothers]] (later purchased by Barclays Investment Bank), Morgan Stanley, and Merrill Lynch, in their highly sought-after research reports. The defendant sold a service wherein subscribers could receive these recommendations prior to their release by the plaintiffs to the general public. The plaintiffs claimed that the value of the reports to their own clients was predicated on the reports' exclusivity, accuracy, and timely release (they were often only valuable during the few hours before the [[stock market]] would open) and whose value to the firms in turn was predicated on their potential to attract and retain clients, to entice clients to execute [[stock trades]] through them, and to differentiate themselves from other financial services firms and for generating revenue by way of commissions earned from facilitating trades on behalf of those clients.&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 879.&lt;/ref&gt;

On June 26, 2006, the plaintiffs filed a [[lawsuit]] seeking an [[injunction]] against Theflyonthewall.com ordering them to cease distributing portions of the exclusive research reports as well as the recommendations within them.&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 885.&lt;/ref&gt; The plaintiffs sought to base their request for injunctive relief on two sets of legal claims: 1) claims that the defendant had violated their [[copyright]] in the research reports by verbatim copying and dissemination of portions of the reports and; 2) claims that the defendant had committed the common-law tort of &quot;hot-news&quot; misappropriation by publishing the recommendations within the reports.&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 885-6.&lt;/ref&gt;

In the procedural lead-up to the trial, the defendants essentially conceded their liability under the copyright claim. Thus, at a [[bench trial]] before the District Court for the Southern District of New York, the remaining issues to be tried were the appropriate relief for the copyright claim, as well as liability and relief for the hot-news misappropriation claim.&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 886-7.&lt;/ref&gt; On March 18, 2010, the District Court found in favour of the plaintiffs, issuing an injunction ordering the defendants to cease their publication of the plaintiffs' reports and recommendations (see summary of the court's decision below.)

Shortly thereafter,&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 889.&lt;/ref&gt; the defendants appealed the decision to the US Court of Appeals for the Second Circuit. The defendants abandoned their defence against the copyright claims and focused solely on the hot-news misappropriation findings. They argued that the District Court had erred in finding that the hot-news misappropriation claim had been established, that the claim (made under state law) was pre-empted and therefore barred by federal copyright legislation, and that the resulting injunction was improperly granted, was overbroad, and violated the defendant's [[free speech]] rights under the [[First Amendment]].&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 890.&lt;/ref&gt;

On June 20, 2011, the Second Circuit released its decision - reversing the decision of the District Court below, and instructing it to dismiss the claim.&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra''.&lt;/ref&gt; A summary of the Second Circuit's decision can also be found below.

==Opinion of the District Court==

===Hot News Misappropriation===

Hot-news misappropriation as a cause of action originated from [[International News Service v. Associated Press]] wherein the Supreme Court held that hot news, defined as time-sensitive information, is protectable as &quot;quasi-property.&quot;&lt;ref name=&quot;INS&quot; /&gt; This misappropriation doctrine was developed further with the aim to &quot;protect costly efforts to gather commercially valuable, time-sensitive information that would otherwise be unprotected by law.&quot;&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 332.&lt;/ref&gt; Significantly, the 'hot' news doctrine is concerned with &quot;the copying and publication of information gathered by another before he has been able to utilize his competitive edge.&quot; &lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 333.&lt;/ref&gt;&lt;ref name=&quot;FII&quot; /&gt; Given that the [[Copyright Act of 1976]] deals specifically with copyright misappropriation for original works, different judges' holdings questioned the tort of hot-news misappropriation as a cause of action. These discussions included copyright protection as only belonging to the realm of originality and not effort as well as holdings that the creative organization of information was copyrightable but not the information itself. Seventy-nine years later, [[National Basketball Association v. Motorola, Inc.]] (&quot;NBA&quot;),&lt;ref name=&quot;NBA&quot; /&gt; provided answers to the questions that preceded it and also provided a five-element test for determining hot-news misappropriation.

====Test for Determining Hot-News Misappropriation====

In Barclay's Capital Inc. v. Theflyonthewall.com, Inc., the court used the five-element test set out in NBA, which was cited in the District Court's opinion as follows:

1) A plaintiff generates or gathers information at a cost; &lt;br&gt;
2) The information is time-sensitive; &lt;br&gt;
3) A defendant's use of the information constitutes free riding on the plaintiff's efforts; &lt;br&gt;
4) The defendant is in direct competition with a product or service offered by the plaintiffs; &lt;br&gt; 
5) The ability of other parties to free-ride on the efforts of the plaintiff or others would so reduce the incentive to produce the product or service that its existence or quality would be substantially threatened. &lt;br&gt;&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 334-5.&lt;/ref&gt;

====District Court Ruling on Firms' Hot-News Misappropriation Claim====

=====Element 1=====
The plaintiffs satisfied the first element of the hot-news misappropriation test because the plaintiffs spent &quot;hundreds of millions of dollars&quot; annually towards the production of equity research reports, including the employment of highly skilled analysts. The defendant did not dispute that the plaintiffs engaged in costly information gathering.&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 335.&lt;/ref&gt;

=====Element 2=====
The plaintiffs' reports and recommendations were held to be &quot;clearly time-sensitive&quot; by virtue of the fact that the plaintiffs' clients used the information for their actions in anticipation of stock price movement. Moreover, the time-sensitivity was especially important as the plaintiffs expended resources to be the first to communicate these findings to the client and generate commissions revenue. The defendant did not dispute that the plaintiffs' reports and recommendations were time-sensitive.&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 335-6.&lt;/ref&gt;

=====Element 3=====
For the third element concerning free-riding, the court found that the defendant's &quot;core business [was] its free-riding off the sustained, costly efforts by the Firms and other investment institutions to generate equity research that is highly valued by investors.&quot; The court defined free-riding as where there is very little investment by the defendant to profit off of &quot;information generated or collected by the plaintiff at great cost.&quot;&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 336.&lt;/ref&gt; Whereas the defendant did not on its own generate original research for the &quot;Recommendations&quot; it published, and its ability to profit off of the reproductions of the Recommendations relied on both the plaintiffs reputations as well as the plaintiffs' expenditure of resources toward generating expert analysis, the court found that defendant free-rode off of the plaintiffs' work.&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 337.&lt;/ref&gt;

=====Element 4=====
The court found that the parties were in direct competition in that both were attempting to provide time-sensitive information to clientele, and the defendant's dissemination of that information prior to the plaintiffs being able to satisfy the demand for pre-market hours stock movement reports and trade recommendations. In doing so, the plaintiffs' clientele, already identified as more likely to trade through the source from which they ascertained the news, were less likely to do their trading through the plaintiffs. Additionally, the court noted that the defendant and the plaintiffs both used similar channels of information distribution including access-controlled media, license third-party distributors to disseminate their information to entitled recipients.&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 339-41.&lt;/ref&gt;

=====Element 5=====
The court found that this element required the demonstration that the defendant's conduct, and others like the defendant, if allowed to continue, would be &quot;likely substantially to threaten plaintiffs' ability to continue to participate in the market.&quot; The court noted that the plaintiffs relied on being the first to report their reports and recommendations to their clients. As such, if other organizations were to preempt the plaintiffs, the knowledge that the plaintiffs had spent money to gather and analyze would be rendered basic. Thus, the court argued that if the defendant, and others like it, were allowed to continue copying and disseminating the research reports of the plaintiffs, they would lose their commission revenue, and not be able to offset the cost of producing the research, and discontinue their research production.

===District Court Ruling on Firms' Copyright Claim and Resulting Remedy===
The court found that the defendant had infringed the copyrights of two of the plaintiffs - Morgan Stanley and Barclays Capital. To satisfy these requirements, Morgan Stanley provided eight research reports and Barclays Capital provided nine research reports (a total of seventeen research reports), their associated registration certificates, and seventeen corresponding examples of the defendant's &quot;direct, verbatim copying of key excerpts.&quot;&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 328.&lt;/ref&gt; While throughout the pre-trial process, Fly defended its copying of the reports as [[fair use]] under {{usc|17|107}}, at trial, the defendant did not dispute that it infringed the copyrights in these seventeen reports. As such, the court ruled in favor of the these two plaintiffs and ordered the defendant to pay them nominal damages, pre-judgment interest on the statutory damage awards, and attorney's fees. The court further ordered a permanent injunction such that the defendant was disallowed from further infringement of &quot;any portion of the copyrighted elements of any research reports&quot; generated by those two plaintiffs.&lt;ref&gt;''Barclay's'' (District Court), ''supra'', at 328-331.&lt;/ref&gt;

==Opinion of the Second Circuit Court of Appeal==

The defendant appealed the rulings of the District Court on hot-news misappropriation to the Court of Appeals for the Second Circuit. On appeal, the Second Circuit court reversed the decision below and ruled in favor of the defendant, finding that the plaintiffs' hot-news misappropriation claim was &quot;preempted by federal copyright law.&quot;&lt;ref name=&quot;ReferenceA&quot;&gt;''Barclay's'' (2nd Cir.), ''supra'', at 902.&lt;/ref&gt;

===Federal Preemption===

The Court recalled that in order to determine whether a state-law claim is preempted by the federal Copyright Act, {{usc|17|301}} sets forth a two part test:
* The General Scope Requirement: if the impugned state-law claim &quot;seeks to vindicate 'legal or equitable rights that are equivalent' to one of the bundle of exclusive rights already protected by copyright law under {{usc|17|106}}&quot;;
and
* The Subject Matter Requirement: &quot;if the work in question is of the type of works protected by the Copyright Act under {{usc|17|102}} and {{usc|17|103}}.&quot;

The Second Circuit found that the plaintiffs recommendations satisfied the &quot;subject matter&quot; requirement as well as the &quot;general scope&quot; requirement of the Copyright Act.&lt;ref name=&quot;ReferenceA&quot;/&gt; They held that the hot-news misappropriation exemption from pre-emption carved out in the ''NBA'' case was explicitly intended to be narrow in scope.&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 897-98.&lt;/ref&gt; In addition, they characterized the 5-part test that was set out in NBA and applied by the District Court as obiter dictum.&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 901.&lt;/ref&gt;

Judge Raggi, in a separate concurring opinion, disagreed with the majority that the NBA test was dictum, and would have held that although pre-emption occurred; the 5-part test was valid, but had been misapplied in the Court below.&lt;ref&gt;''Barclay's'' (2nd Cir.), ''supra'', at 907-8.&lt;/ref&gt;

==Organizations Concerned about the Second Circuit's Ruling==
The case was followed widely and several prominent organizations filed ''[[amicus curiae]]'' briefs as intervenors.

The [[Electronic Frontier Foundation]], [[Citizen Media Law Project]], and [[Public Citizen, Inc.]] filed an ''amicus curiae'' brief urging the Second Circuit court to take into consideration the First Amendment as it related to the district court judgement's potential restraints on Americans' abilities to gather and comment on the news of the day.

[[Google]] and [[Twitter]] filed an ''amicus ''brief calling for the repudiation of the hot-news misappropriation tort. The Second Circuit held that this was beyond the bounds of the court's power and not relevant to the case.

The [[Newspaper Association of America]], [[The New York Times Company]], [[Philadelphia Media Holdings]], [[Stephens Media]], ''[[Time (magazine)|Time]]'', and the ''[[Washington Post]]'' filed an ''amicus'' brief.

[[The Securities Industry and Financial Markets Association]], [[Dow Jones &amp; Company]], and [[The Investorside Research Association]] each filed ''amicus'' briefs.

==Second Circuit's Humorous Footnotes==

===Irony===
In recounting the story of the case, the Second Circuit Court wrote that they &quot;find little to take issue with in the district court's careful findings of facts, to which we must in any event defer. We therefore borrow freely from them.&quot; These sentences are followed by a footnote which reads, &quot;The irony of doing so in the context of a copyright-infringement and 'hot news'-misappropriation case is not lost on us.&quot;&quot; (Footnote 3).

===Biblical References===
The Second Circuit court comments on the district court's characterization of the defendant's behavior as having a &quot;biblical&quot; tone,&quot; where the district court asserts that the defendant had &quot;reaped where it had not sown.&quot; This claim is then footnoted by the Second Circuit to include an analysis of this phrase as it appears in the [[Bible]]: &quot;In the Bible, that turn of phrase seems to be more a threat than a promise. See, e.g., [[Epistle to the Galatians|Galatians]] 6:7: &quot;God is not mocked, for whatever a man sows, that he will also reap.&quot; But cf. [[Leviticus]] 23:22, setting forth circumstances under which persons are forbidden to reap where they have sown.&quot;

==References==

{{Reflist|refs=
&lt;ref name=&quot;2ndCircuitOpinion&quot;&gt;{{Cite court
|litigants=Barclay's Capital Inc. v. Theflyonthewall.com, Inc.
|vol=650
|reporter=F.3d
|opinion=876
|court=2nd Cir.
|date=June 20, 2011
|url = http://scholar.google.co.il/scholar_case?case=3905720684011410549&amp;hl=en&amp;as_sdt=2006&amp;as_vis=1
}}&lt;/ref&gt;

&lt;ref name=&quot;DistrictOpinion&quot;&gt;{{Cite court
|litigants=Barclay's Capital Inc. v. Theflyonthewall.com, Inc.
|vol=700
|reporter=F.Supp.2d
|opinion=310
|court=Dist. Court, S.D.N.Y.
|date=March 18, 2010
|url = http://scholar.google.co.il/scholar_case?case=2571947736946721031&amp;hl=en&amp;as_sdt=2006&amp;as_vis=1
}}&lt;/ref&gt;

&lt;ref name=&quot;INS&quot;&gt;{{Cite court
|litigants=International News Service v. Associated Press
|vol=248
|reporter=U.S.
|opinion=215
|court=
|date=December 23, 1918
|url = http://caselaw.lp.findlaw.com/cgi-bin/getcase.pl?court=us&amp;vol=248&amp;invol=215
}}&lt;/ref&gt;

&lt;ref name=&quot;FII&quot;&gt;{{Cite court
|litigants=Fin. Info., Inc. v. Moody's Investors Serv., Inc.
|vol=808
|reporter=F.2d
|opinion=204
|pinpoint=209
|court=2nd Cir.
|date=1986
|url=http://scholar.google.co.il/scholar_case?case=13069026130551291498&amp;hl=en&amp;as_sdt=2006&amp;as_vis=1
}}&lt;/ref&gt;

&lt;ref name=&quot;NBA&quot;&gt;{{Cite court
|litigants=National Basketball Ass'n v. Motorola, Inc.
|vol=105
|reporter=F.3d
|opinion=841
|court=2nd Cir.
|date=1997
|url=http://scholar.google.co.il/scholar_case?case=10739824072419550811&amp;hl=en&amp;as_sdt=2006&amp;as_vis=1
}}&lt;/ref&gt;

&lt;ref name=skadden&gt;{{Cite web
| last1 = Dreyer
| first1 = Anthony J.
| last2 = Levi
| first2 = Stuart D.
| title = Second Circuit Redefines Elements of 'Hot News' Misappropriation Claims
| work = Skadden, Arps, Slate, Meagher &amp; Flom LLP
| accessdate = 2014-12-15
| date = 2012-06-29
| url = http://www.skadden.com/insights/second-circuit-redefines-elements-hot-news-misappropriation-claims
}}&lt;/ref&gt;
}}

[[Category:Big data]]
[[Category:2011 in United States case law]]
[[Category:United States Court of Appeals for the Second Circuit cases]]
[[Category:Barclays litigation]]
[[Category:Merrill Lynch]]
[[Category:Morgan Stanley]]</text>
      <sha1>9ukdgf5q27kpffeqz7e2lxgv5efl3j3</sha1>
    </revision>
  </page>
  <page>
    <title>Data lineage</title>
    <ns>0</ns>
    <id>44783487</id>
    <revision>
      <id>673672016</id>
      <parentid>673671672</parentid>
      <timestamp>2015-07-29T18:37:08Z</timestamp>
      <contributor>
        <username>Shabihsyed</username>
        <id>25885910</id>
      </contributor>
      <comment>/* Data flow Reconstruction */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33432">{{peacock|date=May 2015}}

{{Orphan|date=December 2014}}

&quot;'''Data lineage''' is defined as a data life cycle that includes the data's origins and where it moves over time.&quot; &lt;ref&gt;http://www.techopedia.com/definition/28040/data-lineage&lt;/ref&gt; It describes what happens to data as it goes through diverse processes. It helps provide visibility into the analytics pipeline and simplifies tracing errors back to their sources. It also enables replaying specific portions or inputs of the dataflow for step-wise debugging or regenerating lost output. In fact, database systems have used such information, called data provenance, to address similar validation and debugging challenges already.&lt;ref name=&quot;DeSoumyarupa&quot;&gt;De, Soumyarupa. (2012). Newt : an architecture for lineage based replay and debugging in DISC systems. UC San Diego: b7355202. Retrieved from: https://escholarship.org/uc/item/3170p7zn&lt;/ref&gt;

'''Data provenance''' documents the inputs, entities, systems, and processes that influence data of interest, in effect providing a historical record of the data and its origins. The generated evidence supports essential forensic activities such as data-dependency analysis, error/compromise detection and recovery, and auditing and compliance analysis. &quot;'''Lineage''' is a simple type of '''why provenance'''.&quot;&lt;ref name=&quot;DeSoumyarupa&quot;/&gt;

==Case for Data Lineage==
The world of [[big data]] is changing dramatically right before our eyes. Statistics say that Ninety percent (90%) of the world’s data has been created in the last two years alone.&lt;ref&gt;http://newstex.com/2014/07/12/thedataexplosionin2014minutebyminuteinfographic/&lt;/ref&gt; This explosion of data has resulted in the ever-growing number of systems and automation at all levels in all sizes of organizations.

Today, distributed systems like Google [[Map Reduce]],&lt;ref&gt;Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on
large clusters. Commun. ACM, 51(1):107–113, January 2008.&lt;/ref&gt; Microsoft Dryad,&lt;ref&gt;Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly.
Dryad: distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference onComputer
Systems 2007, EuroSys ’07, pages 59–72, New York, NY, USA, 2007. ACM.&lt;/ref&gt; Apache Hadoop &lt;ref&gt;Apache Hadoop. http://hadoop.apache.org.&lt;/ref&gt;(an open-source project) and Google Pregel&lt;ref&gt;Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. Pregel: a system for largescale graph processing. In Proceedings of the 2010 international conference on Managementof data, SIGMOD ’10, pages 135–146, New York, NY, USA, 2010. ACM.&lt;/ref&gt; provide such platforms for businesses and users. However, even with these systems, [[big data]] analytics can take several hours, days or weeks to run, simply due to the data volumes involved. For example, a ratings prediction algorithm for the Netflix Prize challenge took nearly 20 hours to execute on 50 cores, and a large-scale image processing task to estimate geographic information took 3 days to complete using 400 cores.&lt;ref&gt;Shimin Chen and Steven W. Schlosser. Map-reduce meets wider varieties of
applications. Technical report, Intel Research, 2008.&lt;/ref&gt; &quot;The Large Synoptic Survey Telescope is expected to generate terabytes of data every night and eventually store more than 50 petabytes, while in the bioinformatics sector, the largest genome 12 sequencing houses in the world now store petabytes of data apiece.&quot;&lt;ref&gt;The data deluge in genomics. https://www-304.ibm.com/connections/blogs/ibmhealthcare/entry/data overload in genomics3?lang=de, 2010.&lt;/ref&gt;
Due to the humongous size of the [[big data]], there could be features in the data that are not considered in the machine learning algorithm, possibly even outliers. It is very difficult for a data scientist to trace an unknown or an unanticipated result.

===Big Data Debugging===
[[Big data]] analytics is the process of examining large data sets to uncover hidden patterns, unknown correlations, market trends, customer preferences and other useful business information. They apply machine learning algorithms etc. to the data which transform the data. Due to the humongous size of the data, there could be unknown features in the data, possibly even outliers. It is pretty difficult for a data scientist to actually debug an unexpected result.

The massive scale and unstructured nature of data, the complexity of these analytics pipelines, and long runtimes pose significant manageability and debugging challenges. Even a single error in these analytics can be extremely difficult to identify and remove. While one may debug them by re-running the entire analytics through a debugger for step-wise debugging, this can be expensive due to the amount of time and resources needed. Auditing and data validation are other major problems due to the growing ease of access to relevant data sources for use in experiments, sharing of data between scientific communities and use of third-party data in business enterprises.&lt;ref&gt;Yogesh L. Simmhan, Beth Plale, and Dennis Gannon. A survey of data prove-
nance in e-science. SIGMOD Rec., 34(3):31–36, September 2005.&lt;/ref&gt;&lt;ref name=&quot;IanFosterJensVockler&quot;&gt;Ian Foster, Jens Vockler, Michael Wilde, and Yong Zhao. Chimera: A Virtual Data System for Representing, Querying, and Automating Data Derivation. In 14th International Conference on Scientific and Statistical Database Management, July 2002.&lt;/ref&gt;&lt;ref name=&quot;Benjamim&amp;Luiz&quot;&gt;Benjamin H. Sigelman, Luiz Andr Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald Beaver, Saul Jaspan, and Chandan Shanbhag. Dapper, a large-scale distributed systems tracing infrastructure. Technical report, Google Inc, 2010.&lt;/ref&gt;&lt;ref name=&quot;PeterBuneman&quot;&gt;Peter Buneman, Sanjeev Khanna, and Wang Chiew Tan. Data provenance: Some basic issues. In Proceedings of the 20th Conference on Foundations of SoftwareTechnology and Theoretical Computer Science, FST TCS 2000, pages 87–93, London, UK, UK, 2000. Springer-Verlag&lt;/ref&gt; These problems will only become larger and more acute as these systems and data continue to grow. As such, more cost-efficient ways of analyzing DISC system analytics are crucial to their continued use.

===Challenges in [[Big Data]] Debugging===

====Massive Scale====
The past two decades have seen a nuclear explosion in the collection and storage of digital information. In 2012, 2.8 [[zettabytes]]—that’s 1 [[sextillion]] bytes, or the equivalent of 24 quintillion tweets—were created or replicated, according to the research firm IDC. There are hundreds or thousands of petabyte-scale databases today, and we’d compare their size to what existed two decades ago, only every time the basis of comparison would be zero. Here’s a look at some of the world’s largest and most interesting data sets. Working with this scale of data has become very challenging.&lt;ref&gt;The Wired. http://www.wired.com/2013/04/bigdata/&lt;/ref&gt;

====Unstructured Data====
The phrase [[unstructured data]] usually refers to information that doesn't reside in a traditional row-column database. As you might expect, it's the opposite of structured data the data stored in fields in a database. Unstructured data files often include text and multimedia content. Examples include e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents. Note that while these sorts of files may have an internal structure, they are still considered &quot;unstructured&quot; because the data they contain doesn't fit neatly in a database.
Experts estimate that 80 to 90 percent of the data in any organization is unstructured. And the amount of unstructured data in enterprises is growing significantly often many times faster than structured databases are growing. &quot;[[Big data]] can include both structured and unstructured data, but IDC estimates that 90 percent of [[big data]] is unstructured data.&quot;&lt;ref&gt;Webopedia http://www.webopedia.com/TERM/U/unstructured_data.html&lt;/ref&gt;

====Long Runtime====
In today’s hyper competitive business environment, companies not only have to find and analyze the relevant data they need, they must find it quickly. The challenge is going through the sheer volumes of data and accessing the level of detail needed, all at a high speed. The challenge only grows as the degree of granularity increases. One possible solution is hardware. Some vendors are using increased memory and powerful parallel processing to crunch large volumes of data extremely quickly. Another method is putting data in-memory but using a grid computing approach, where many machines are used to solve a problem. Both approaches allow organizations to explore huge data volumes. Even this level of sophisticated hardware and software, few of the image processing tasks in large scale take a few days to few weeks.&lt;ref&gt;SAS. http://www.sas.com/resources/asset/five-big-data-challenges-article.pdf&lt;/ref&gt; Debugging of the data processing is extremely hard due to long run times.

====Complex Platform====
[[Big Data]] platforms have a very complicated structure. Data is distributed among several machines. Typically the jobs are mapped into several machines and results are later combined by reduce operations. Debugging of a [[big data]] pipeline becomes very challenging because of the very nature of the system. It will not be an easy task for the data scientist to figure out which machine's data has the outliers and unknown features causing a particular algorithm to give unexpected results.

===Proposed Solution===
Data provenance or data lineage can be used to make the debugging of [[big data]] pipeline easier. This necessitates the collection of data about data transformations. The below section will explain data provenance in more detail.

==Data Provenance==
Data Provenance provides a historical record of the data and its origins. The provenance of data which is generated by complex transformations such as workflows is of considerable value to scientists. From it, one can ascertain the quality of the data based on its ancestral data and derivations, track back sources of errors, allow automated re-enactment of derivations to update a data, and provide attribution of data sources. Provenance is also essential to the business domain where it can be used to drill down to the source of data in a data warehouse, track the creation of intellectual property, and provide an audit trail for regulatory purposes.

The use of data provenance is proposed in distributed systems to trace records through a dataflow, replay the dataflow on a subset of its original inputs and debug data flows. To do so, one needs to keep track of the set of inputs to each operator, which were used to derive each of its outputs. Although there are several forms of provenance, such as copy-provenance and how-provenance,&lt;ref name=&quot;PeterBuneman&quot; /&gt;&lt;ref&gt;Robert Ikeda and Jennifer Widom. Data lineage: A survey. Technical report, Stanford University, 2009.&lt;/ref&gt; the information we need is a simple form of '''why-provenance, or lineage''', as defined by Cui et al.&lt;ref name=&quot;YCui&quot;&gt;Y. Cui and J. Widom. Lineage tracing for general data warehouse transformations. VLDB Journal, 12(1), 2003.&lt;/ref&gt;

==Lineage Capture==
Intuitively, for an operator T producing output o, lineage consists of triplets of form {I, T, o}, where I is the set of inputs to T used to derive o. Capturing lineage for each operator T in a dataflow enables users to ask questions such as “Which outputs were produced by an input i on operator T ?” and “Which inputs produced output o in operator T ?”&lt;ref name=&quot;DeSoumyarupa&quot;/&gt; A query that finds the inputs deriving an output is called a backward tracing query, while one that finds the outputs produced by an input is called a forward tracing query.&lt;ref name=&quot;RobertIkedaHyunjung&quot;&gt;Robert Ikeda, Hyunjung Park, and Jennifer Widom. Provenance for generalized map and reduce workflows. In Proc. of CIDR, January 2011.&lt;/ref&gt; Backward tracing is useful for debugging, while forward tracing is useful for tracking error propagation.&lt;ref name=&quot;RobertIkedaHyunjung&quot; /&gt; Tracing queries also form the basis for replaying an original dataflow.&lt;ref name=&quot;IanFosterJensVockler&quot; /&gt;&lt;ref name=&quot;YCui&quot; /&gt;&lt;ref name=&quot;RobertIkedaHyunjung&quot; /&gt; However, to efficiently use lineage in a DISC system, we need to be able to capture lineage at multiple levels (or granularities) of operators and data, capture accurate lineage for DISC processing constructs and be able to trace through multiple dataflow stages efficiently.

DISC system consists of several levels of operators and data, and different use cases of lineage can dictate the level at which lineage needs to be captured.  Lineage can be captured at the level of the job, using files and giving lineage tuples of form {IF i, M RJob, OF i }, lineage can also be captured at the level of each task, using records and giving, for example, lineage tuples of form {(k rr, v rr ), map, (k m, v m )}. The first form of lineage is called coarse-grain lineage, while the second form is called fine-grain lineage. Integrating lineage across different granularities enables users to ask questions such as “Which file read by a MapReduce job produced this particular output record?” and can be useful in debugging across different operator and data granularities within a dataflow.&lt;ref name=&quot;DeSoumyarupa&quot; /&gt;
[[File:Map Reduce Job -1.png|thumb|center|500px|Map Reduce Job showing containment relashionshpis]]

To capture end-to-end lineage in a DISC system, we use the Ibis model,&lt;ref&gt;C. Olston and A. Das Sarma. Ibis: A provenance manager for multi-layer
systems. In Proc. of CIDR, January 2011.&lt;/ref&gt; which introduces the notion of containment hierarchies for operators and data. Specifically, Ibis proposes that an operator can be contained within another and such a relationship between two operators is called '''operator containment'''. &quot;Operator containment implies that the contained (or child) operator performs a part of the logical operation of the containing (or parent) operator.&quot;&lt;ref name=&quot;DeSoumyarupa&quot; /&gt; For example, a MapReduce task is contained in a job. Similar containment relationships exist for data as well, called data containment. Data containment implies that the contained data is a subset of the containing data (superset).
[[File:Containment Hierarchy.png|thumb|center|500px|Containment Hierarchy]]

==Active vs Lazy Lineage==
Lazy lineage collection typically captures only coarse-grain lineage at run time. These systems incur low capture overheads due to the small amount of lineage they capture. However, to answer fine-grain tracing queries, they must replay the data flow on all (or a large part) of its input and collect fine-grain lineage during the replay. This approach is suitable for forensic systems, where a user wants to debug an observed bad output.

Active collection systems capture entire lineage of the data flow at run time. The kind of lineage they capture may be coarse-grain or fine-grain, but they do
not require any further computations on the data flow after its execution. Active fine-grain lineage collection systems incur higher capture overheads than lazy collection systems. However, they enable sophisticated replay and debugging.&lt;ref name=&quot;DeSoumyarupa&quot; /&gt;

==Actors==
An actor is an entity that transforms data; it may be a Dryad vertex, individual map and reduce operators, a MapReduce job, or an entire dataflow pipeline. Actors act as black-boxes and the inputs and outputs of an actor are tapped to capture lineage in the form of associations, where an association is a triplet {i, T, o} that relates an input i with an output o for an actor T . The instrumentation thus captures lineage in a dataflow one actor at a time, piecing it into a set of associations for each actor. The system developer needs to capture the data an actor reads (from other actors) and the data an actor writes (to other actors). For example, a developer can treat the Hadoop Job Tracker as an actor by recording the set of files read and written by each job.
&lt;ref name=&quot;mainPaper&quot;&gt;Dionysios Logothetis, Soumyarupa De, and Kenneth Yocum. 2013. Scalable lineage capture for debugging DISC analytics. In Proceedings of the 4th annual Symposium on Cloud Computing (SOCC '13). ACM, New York, NY, USA, , Article 17 , 15 pages.&lt;/ref&gt;

==Associations==
Association is a combination of the inputs, outputs and the operation itself. The operation is represented in terms of a black box also known as the actor. The associations describe the transformations that are applied on the data. The associations are stored in the association tables. Each unique actor is represented by its own association table. An association itself looks like {i, T, o} where i is the set of inputs to the actor T and o is set of outputs given produced by the actor. Associations are the basic units of Data Lineage. Individual associations are later clubbed together to construct the entire history of transformations that were applied to the data.&lt;ref name=&quot;DeSoumyarupa&quot;/&gt;

==Architecture==
[[Big data]] systems scale horizontally i.e. increase capacity by adding new hardware or software entities into the distributed system. The distributed system acts as a single entity in the logical level even though it comprises multiple hardware and software entities. The system should continue to maintain this property after horizontal scaling. An important advantage of horizontal scalability is that it can provide the ability to increase capacity on the fly. The biggest plus point is that horizontal scaling can be done using commodity hardware.

The horizontal scaling feature of [[Big Data]] systems should be taken into account while creating the architecture of lineage store. This is essential because the lineage store itself should also be able to scale in parallel with the [[Big data]] system. The number of associations and amount of storage required to store lineage will increase with the increase in size and capacity of the system. The architecture of [[Big data]] systems makes the use of a single lineage store not appropriate and impossible to scale. The immediate solution to this problem is to distribute the lineage store itself.&lt;ref name=&quot;DeSoumyarupa&quot;/&gt;

The best case scenario is to use a local lineage store for every machine in the distributed system network. This allows the lineage store also to scale horizontally. In this design, the lineage of data transformations applied to the data on a particular machine is stored on the local lineage store of that specific machine. The lineage store typically stores association tables. Each actor is represented by its own association table. The rows are the associations themselves and columns represent inputs and outputs. This design solves 2 problems. It allows horizontal scaling of the lineage store. If a single centralized lineage store was used, then this information had to be carried over the network, which would cause additional network latency. The network latency is also avoided by the use of a distributed lineage store.&lt;ref name=&quot;mainPaper&quot;/&gt;

[[File:Selection 065.png|thumb|center|500px|Architecture of Lineage Systems]]

==Data flow Reconstruction==
The information stored in terms of associations needs to be combined by some means to get the data flow of a particular job. In a distributed system a job is broken down into multiple tasks. One or more instances run a particular task. The results produced on these individual machines are later combined together to finish the job. Tasks running on different machines perform multiple transformations on the data in the machine. All the transformations applied to the data on a machines is stored in the local lineage store of that machines. This information needs to be combined together to get the lineage of the entire job. The lineage of the entire job should help the data scientist understand the data flow of the job and he/she can use the data flow to debug the [[big data]] pipeline. The data flow is reconstructed in 3 stages.

===Association tables===
The first stage of the data flow reconstruction is the computation of the association tables. The association tables exists for each actor in each local lineage store. The entire association table for an actor can be computed by combining these individual association tables. This is generally done using a series of equality joins based on the actors themselves. In few scenarios the tables might also be joined using inputs as the key. Indexes can also be used to improve the efficiency of a join.The joined tables need to be stored on a single instance or a machine to further continue processing. There are multiple schemes that are used to pick a machine where a join would be computed. The easiest one being the one with minimum CPU load. Space constraints should also be kept in mind while picking the instance where join would happen.

===Association Graph===
The second step in data flow reconstruction is computing an association graph from the lineage information. The graph represents the steps in the data flow. The actors act as vertices and the associations act as edges. Each actor T is linked to its upstream and downstream actors in the data flow. An upstream actor of T is one that produced the input of T, while a downstream actor is one that consumes the output of T . Containment relationships are always considered while creating the links. The graph consists of three types of links or edges.

====Explicitly specified links====
The simplest link is an explicitly specified link between two actors. These links are explicitly specified in the code of a machine learning algorithm. When an actor is aware of its exact upstream or downstream actor, it can communicate this information to lineage API. This information is later used to link these actors during the tracing query. For example, in the [[MapReduce]] architecture, each map instance knows the exact record reader instance whose output it consumes.&lt;ref name=&quot;DeSoumyarupa&quot;/&gt;

====Logically inferred links====
Developers can attach data flow [[archetypes]] to each logical actor. A data flow archetype explains how the children types of an actor type arrange themselves in a data flow. With the help of this information, one can infer a link between each actor of a source type and a destination type. For example, in the [[MapReduce]] architecture, the map actor type is the source for reduce, and vice versa. The system infers this from the data flow archetypes and duly links map instances with reduce instances. However, there may be several [[MapReduce]] jobs in the data flow, and linking all map instances with all reduce instances can create false links. To prevent this, such links are restricted to actor instances contained within a common actor instance of a containing (or parent) actor type. Thus, map and reduce instances are only linked to each other if they belong to the same job.&lt;ref name=&quot;DeSoumyarupa&quot;/&gt;

====Implicit links through data set sharing====
In distributed systems, sometimes there are implicit links, which are not specified during execution. For example, an implicit link exists between an actor that wrote to a file and another actor that read from it. Such links connect actors which use a common data set for execution. The dataset is the output of the first actor and is the input of the actor following it.&lt;ref name=&quot;DeSoumyarupa&quot;/&gt;

===Topological Sorting===
The final step in the data flow reconstruction is the [[Topological sorting]] of the association graph. The directed graph created in the previous step is topologically sorted to obtain the order in which the actors have modified the data. This inherit order of the actors defines the data flow of the big data pipeline or task.

==Tracing &amp; Replay==
This is the most crucial step in [[Big Data]] debugging. The  captured lineage is combined and processed to obtain the data flow of the pipeline. The data flow helps the data  scientist or a developer to look deeply into the actors and their transformations. This step allows the data scientist to figure out the part of the algorithm that is generating the unexpected output. A [[big data]] pipeline can go wrong in 2 broad ways. The first is a presence of a suspicious actor in the data-flow. The second being the existence of outliers in the data.

The first case can be debugged by tracing the data-flow. By using lineage and data-flow information together a data scientist can figure out how the inputs are converted into outputs. During the process actors that behave unexpectedly can be caught. Either these actors can be removed from the data flow or they can be augmented by new actors to change the data-flow. The improved data-flow can be replayed to test the validity of it. Debugging faulty actors include recursively performing coarse-grain replay on actors in the data-flow,&lt;ref&gt;Wenchao Zhou, Qiong Fei, Arjun Narayan, Andreas Haeberlen, Boon Thau Loo, and Micah Sherr. Secure network provenance. In Proceedings of 23rd ACM Symposium on Operating System Principles (SOSP), December 2011.&lt;/ref&gt; which can be expensive in resources for long dataflows. Another approach is to manually inspect lineage logs to find anomalies,&lt;ref name=&quot;Benjamim&amp;Luiz&quot; /&gt;&lt;ref&gt;Rodrigo Fonseca, George Porter, Randy H. Katz, Scott Shenker, and Ion Stoica. X-trace: A pervasive network tracing framework. In In Proceedings of NSDI’07, 2007.&lt;/ref&gt; which can be tedious and time-consuming across several stages of a data-flow. Furthermore, these approaches work only when the data scientist can discover bad outputs. To debug analytics without known bad outputs, the data scientist need to analyze the data-flow for suspicious behavior in general. However, often, a user may not know the expected normal behavior and cannot specify predicates. This section describes a debugging methodology for retrospectively analyzing lineage to identify faulty actors in a multi-stage data-flow. We believe that sudden changes in an actor’s behavior, such as its average selectivity, processing rate or output size, is characteristic of an anomaly. Lineage can reflect such changes in actor behavior over time and across different actor instances. Thus, mining lineage to identify such changes can be useful in debugging faulty actors in a data-flow.
[[File:Tracing Anomalous Actors.png|thumb|center|400px|Tracing Anomalous Actors]]

The second problem i.e. the existence of outliers can also be identified by running the data-flow step wise and looking at the transformed outputs. The data scientist finds a subset of outputs that are not in accordance to the rest of outputs. The inputs which are causing these bad outputs are the outliers in the data. This problem can be solved by removing the set of outliers from the data and replaying the entire data-flow. It can also be solved by modifying the machine learning algorithm by adding, removing or moving actors in the data-flow. The changes in the data-flow are successful if the replayed data-flow does not produce bad outputs.
[[File:Tracing Outliers in the data.png|thumb|center|400px|Tracing Outliers in the data]]

==Challenges==
Even though use data lineage is a novel way of debugging of [[big data]] pipelines, the process is not simple. The challenges are scalability of lineage store, fault tolerance of the lineage store, accurate capture of lineage for black box operators and many others. These challenges must be considered carefully and trade offs between them need to be evaluated to make a realistic design for data lineage capture.

===Scalability===
DISC systems are primarily batch processing systems designed for high throughput. They execute several jobs per analytics, with several tasks per job. The overall number of operators executing at any time in a cluster can range from hundreds to thousands depending on the cluster size. Lineage capture for
these systems must be able scale to both large volumes of data and numerous operators to avoid being a bottleneck for the DISC analytics.

===Fault tolerance===
Lineage capture systems must also be fault tolerant to avoid rerunning data flows to capture lineage. At the same time, they must also accommodate failures in the DISC system. To do so, they must be able to identify a failed DISC task and avoid storing duplicate copies of lineage between the partial lineage generated by the failed task and duplicate lineage produced by the restarted task. A lineage system should also be able to gracefully handle multiple instances of local lineage systems going down. This can achieved by storing replicas of lineage associations in multiple machines. The replica can act like a backup in the event of the real copy being lost.

===Black-box operators===
Lineage systems for DISC dataflows must be able to capture accurate lineage across black-box operators to enable fine-grain debugging. Current approaches to this include Prober, which seeks to find the minimal set of inputs that can produce a specified output for a black-box operator by replaying the data-flow several times to deduce the minimal set,&lt;ref&gt;Anish Das Sarma, Alpa Jain, and Philip Bohannon. PROBER: Ad-Hoc Debugging of Extraction and Integration Pipelines. Technical report, Yahoo, April 2010.&lt;/ref&gt; and dynamic slicing, as used by Zhang et al.&lt;ref&gt;Mingwu Zhang, Xiangyu Zhang, Xiang Zhang, and Sunil Prabhakar. Tracing lineage beyond relational operators. In Proc. Conference on Very Large Data Bases (VLDB), September 2007.&lt;/ref&gt; to capture lineage for [[NoSQL]] operators through binary rewriting to compute dynamic slices. Although producing highly accurate lineage, such techniques can incur significant time overheads for capture or tracing, and it may be preferable to instead trade some accuracy for better performance. Thus, there is a need for a lineage collection system for DISC dataflows that can capture lineage from arbitrary operators with reasonable accuracy, and without significant overheads in capture or tracing.

===Efficient tracing===
Tracing is essential for debugging, during which, a user can issue multiple tracing queries. Thus, it is important that tracing has fast turnaround times. Ikeda et al.&lt;ref name=&quot;RobertIkedaHyunjung&quot; /&gt; can perform efficient backward tracing queries for MapReduce dataflows, but are not generic to different DISC systems and do not perform efficient forward queries. Lipstick,&lt;ref&gt;Yael Amsterdamer, Susan B. Davidson, Daniel Deutch, Tova Milo, and Julia Stoyanovich. Putting lipstick on a pig: Enabling database-style workflow provenance. In Proc. of VLDB, August 2011.&lt;/ref&gt; a lineage system for Pig,&lt;ref&gt;Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. Pig latin: A not-so-foreign language for data processing. In Proc. of ACM SIGMOD, Vancouver, Canada, June 2008.&lt;/ref&gt; while able to perform both backward and forward tracing, is specific to Pig and SQL operators and can only perform coarse-grain tracing for black-box operators. Thus, there is a need for a lineage system that enables efficient forward and backward tracing for generic DISC systems and dataflows with black-box operators.

===Sophisticated replay===
Replaying only specific inputs or portions of a data-flow is crucial for efficient debugging and simulating what-if scenarios. Ikeda et al. present a methodology for lineage-based refresh, which selectively replays updated inputs to recompute affected outputs.&lt;ref&gt;Robert Ikeda, Semih Salihoglu, and Jennifer Widom. Provenance-based refresh in data-oriented workflows. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM ’11, pages 1659–1668, New York, NY, USA, 2011. ACM.&lt;/ref&gt; This is useful during debugging for re-computing outputs when a bad input has been fixed. However, sometimes a user may want to remove the bad input and replay the lineage of outputs previously affected by the error to produce error-free outputs. We call this exclusive replay. Another use of replay in debugging involves replaying bad inputs for step-wise debugging (called selective replay). Current approaches to using lineage in DISC systems do not address these. Thus, there is a need for a lineage system that can perform both exclusive and selective replays to address different debugging needs.

===Anomaly detection===
One of the primary debugging concerns in DISC systems is identifying faulty operators. In long dataflows with several hundreds of operators or tasks, manual inspection can be tedious and prohibitive. Even if lineage is used to narrow the subset of operators to examine, the lineage of a single output can still span several operators. There is a need for an inexpensive automated debugging system, which can substantially narrow the set of potentially faulty operators, with reasonable accuracy, to minimize the amount of manual examination required.&lt;ref name=&quot;DeSoumyarupa&quot;/&gt;

==See also==
&lt;!-- please do not list specific implementations here --&gt;
* [[Provenance]]
* [[Big Data]]
* [[Topological Sorting]]
* [[Debugging]]
* [[NoSQL]]
* [[Scalability]]
* [[Directed acyclic graph]]

==References==
{{Reflist|33em}}

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Big data| ]]</text>
      <sha1>5dqtpzay94arrq2khvblzq0mb0b0s51</sha1>
    </revision>
  </page>
  <page>
    <title>CBIG Consulting</title>
    <ns>0</ns>
    <id>45061797</id>
    <revision>
      <id>645673930</id>
      <parentid>642649809</parentid>
      <timestamp>2015-02-04T23:32:26Z</timestamp>
      <contributor>
        <username>Wiscoeditor</username>
        <id>23492115</id>
      </contributor>
      <minor/>
      <comment>/* External links */ removed broken category link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7924">{{Infobox company|  name   =  CBIG Consulting
|  logo = [[Image:CBIG Consulting.jpg|250px|CBIG Consulting]]
|  type   = [[Limited liability company|LLC]]
|  foundation     = 2002
|  location       = [[Rosemont, Illinois]], [[United States|U.S.]]&lt;br&gt; Offices in Auckland (NZ), Austin, Boston, Denver, Lombard, London (UK), Raleigh, San Francisco, Seattle, Singapore (SG), Sydney (AU) 
|  key_people     = John Onder, Principal&lt;br/&gt;Don Arendarczyk, Principal&lt;br/&gt;John Harmann, Principal&lt;br/&gt;Todd Nash, Principal
|  industry       = [[Professional services]]
|  services       = [[Consultant|Consulting]]
|  num_employees  = 100-250
|  homepage       = {{url|http://www.cbigconsulting.com/}}
}}
'''CBIG Consulting''' is an international consulting group that specializes in [[business intelligence]] (BI), [[Big_data|Big Data]] analytics, [[data warehouse]], and [[Cloud_computing|Cloud]]-based analytics solutions. CBIG is consistently ranked as a top BI and Big Data analytics solution provider.&lt;ref&gt;{{cite web|title=CIO Story’s 20 Most Powerful Big Data Solution Providers, 2014, “CBIG Consulting: Assisting Big Data Endeavors With A Two-Pronged Service Approach”|url=http://ciostory.com/magazine/September-2014/BigData20/?ro=html5,flash#page=26|website=www.ciostory.com|publisher=CIO Story|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=CIO Review’s 20 Most Promising Healthcare Tech Solutions Providers, 2014, “CBIG Consulting:  Analytics that Transform Healthcare Delivery”|url=http://healthcare.cioreview.com/vendor/2014/cbig_consulting|website=www.cioreview.com|publisher=CIO Review|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Top Big Data Analytics Companies, USA|url=http://www.recovendor.com/top-big-data-analytics-companies-list/|website=www.recovendor.com|publisher=Recovendor|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=CIO Review's Top 100 Most Promising Big Data Companies,&quot;CBIG Consulting: A Decade of Expertise in Data Warehouse and Business Intelligence Consulting&quot;|url=http://bigdata.cioreview.com/vendor/2014/CBIG_Consulting|website=www.cioreview.com|publisher=CIO Review|accessdate=15 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Top Big Data and BI Consulting Companies|url=https://clutch.co/research/big-data-consultants|website=www.clutch.co|publisher=Clutch (formerly SourcingLine)|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=6.	Top Hadoop Consulting Companies|url=https://clutch.co/research/hadoop-consultants|website=www.clutch.co|publisher=Clutch (formerly SourcingLine)|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Top Big Data Marketing Analytics Consultants|url=https://clutch.co/research/marketing-analytics-consultants|website=www.clutch.com|publisher=Clutch (formerly SourcingLine)|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Top Big Data Operations and Process Improvement Analytics Consultants|url=https://clutch.co/directory/big-data-operations-consultants|website=www.clutch.co|publisher=Clutch (formerly SourcingLine)|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=10.	Top Big Data Compliance, Fraud, and Risk Management Consultants,|url=https://clutch.co/directory/big-data-financial-consultants|website=www.clutch.co|publisher=Clutch (formerly SouringLine)|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=CIO Review’s 20 Most Promising Data Analytics Consultants, “CBIG Consulting:  Intelligently Powering the Vertical Analytics”|url=http://www.cioreview.com/magazine/CBIG-Consulting-Intelligently-Powering-The-Vertical-Analytics-KMTP687094221.html|website=www.cioreview.com|publisher=CIO Review|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=CIO Review’s 20 Most Promising Big Data Companies, “CBIG Consulting: Quality Solutions for Large Data Assets&quot;|url=http://www.cioreview.com/magazine/CBIG-Consulting-Providing-Quality-Solutions-for-Large-Data-Assets-PQPA441624927.html|website=www.cioreview.com|publisher=CIO Review|accessdate=14 January 2015}}&lt;/ref&gt; 
CBIG Consulting’s customers come from many industries including healthcare, pharmaceutical, consumer packaged goods, retail, internet, financial, utilities, telecommunications, education, manufacturing and logistics. The company has employees across 12 offices in North America, Australia, New Zealand, Singapore and the United Kingdom.

== History ==
CBIG Consulting was initially founded as Chicago Business Intelligence Group, Inc., in Rosemont, IL, a technology hub northwest of Chicago, IL in 2002. In 2007, CBIG Recruiting &amp; Staffing was created. In 2012, Chicago Business Intelligence Group formed a separate consulting division, renaming it CBIG Consulting, LLC.&lt;ref&gt;{{cite web|title=Chicago Business Intelligence Group Announces Name Change to CBIG Consulting|url=http://www.prweb.com/releases/2012/10/prweb9990480.htm|website=www.prweb.com|publisher=PR Web|accessdate=14 January 2015}}&lt;/ref&gt; 

* 2002 - Chicago Business Intelligence Group, Inc. formed.
* 2007 - CBIG Recruiting and Staffing division formed.
* 2008 - Boston and San Francisco offices opened.
* 2009 - CBIG Cloud™ Business Intelligence platform created.
* 2010 - CBIG Framework™ Big Data methodology created.
* 2012 - Consulting division rebranded as CBIG Consulting, LLC.
* 2012 - Denver office opened.&lt;ref&gt;{{cite web|title=Todd Saunders Joins CBIG Consulting as Principal in Charge of Denver, CO Office|url=http://www.prweb.com/releases/business/intelligence/prweb10102310.htm|website=www.prweb.com|publisher=PR Web|accessdate=14 January 2015}}&lt;/ref&gt;
* 2013 - CBIG ADC™ (American Development Center) formed.&lt;ref&gt;{{cite web|title=CBIG Consulting Adds Business Intelligence Jobs, Second Office in Chicago Area|url=http://www.prweb.com/releases/2013/big-data-consultants/prweb10444046.htm|website=www.prweb.com|publisher=PR Web|accessdate=14 January 2015}}&lt;/ref&gt;
* 2013 - Sydney and Austin offices opened.&lt;ref&gt;{{cite web|title=CBIG Consulting Affirms International Expansion with New Office in Asia-Pacific Region|url=http://www.prweb.com/releases/2013/6/prweb10825138.htm|website=www.prweb.com|publisher=PR Web|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=CBIG Consulting Continues Expansion with Office in Austin, TX|url=http://www.prweb.com/releases/2013/bi-consulting/prweb10907695.htm|website=www.prweb.com|publisher=PR Web|accessdate=14 January 2015}}&lt;/ref&gt;
* 2014 - CBIG LeanBI™ and CBIG LeanBD™ methodologies introduced.
* 2014 - Raleigh, Singapore, London, and Auckland offices opened.&lt;ref&gt;{{cite web|title=CBIG Consulting Continues to Hit Growth Targets, Opens Southeast Regional Office|url=http://www.prweb.com/releases/raleigh-nc/bi-consultants/prweb11586512.htm|website=www.prweb.com|publisher=PR Web|accessdate=15 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=CBIG Consulting Continues Asia-Pacific Expansion with the Opening of a New Office in Singapore|url=http://www.prweb.com/releases/singapore/data-analytics-consulting/prweb12300803.htm|website=www.prweb.com|publisher=PR Web|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=CBIG Consulting Brings Big Data Solutions to UK, Opens London Office|url=http://www.prweb.com/releases/london/bi-consultants/prweb12337628.htm|website=www.prweb.com|publisher=PR Web|accessdate=14 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=CBIG Consulting Continues Asia-Pacific Expansion with New Office in Auckland, New Zealand|url=http://www.prweb.com/releases/big-data-consulting/auckland-new-zealand/prweb12410653.htm|website=www.prweb.com|publisher=PR Web|accessdate=14 January 2015}}&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
* {{official website|http://www.cbigconsulting.com/}}

[[Category:Management consulting firms of the United States]]
[[Category:Information technology consulting firms of the United States]]
[[Category:Business intelligence companies]]
[[Category:Analytics]]
[[Category:Big data]]
[[Category:Companies established in 2002]]</text>
      <sha1>l8ikmi3x8lo5jw1ju1tfove0pxyenbu</sha1>
    </revision>
  </page>
  <page>
    <title>Zoomdata</title>
    <ns>0</ns>
    <id>44663824</id>
    <revision>
      <id>656049994</id>
      <parentid>647786082</parentid>
      <timestamp>2015-04-12T00:55:19Z</timestamp>
      <contributor>
        <username>Midas02</username>
        <id>12205148</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[Reston]] (link changed to [[Reston, Virginia]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6727">{{Infobox company
|name             = Zoomdata
| logo     =

|industry         = [[Data analytics|Big Data Analytics]]
|founder          = Justin Langseth ([[CEO]])
|lHQ_city    = [[Reston, Virginia]],
|homepage         = {{URL|http://www.zoomdata.com|zoomdata.com}}
}}

'''Zoomdata.''' is a [[Reston, Virginia|Reston]], [[Virginia]]-based [[data visualization]] and [[analytics]] company founded in 2012. Its aim is &quot;to process large volumes of data faster and present users with their results more quickly.&quot;&lt;ref name=&quot;WaPo&quot;/&gt;

==Technology==

Zoomdata markets a [[data visualization]] and analytics tool that allows customers to explore and analyze the vast quantities of data in their datastores. The product is different from other tools in the industry due to a patent the company holds around “Data Sharpening&quot;.&lt;ref&gt;{{cite web|url=http://techcrunch.com/2014/10/06/zoomdata-scores-17m-to-disrupt-the-business-intelligence-market-again/|title=Zoomdata Scores $17M to Help Update the Business Intelligence Market|publisher=TechCrunch|accessdate=6 October 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.lightreading.com/analytics/analytics-systems/zoomdata-raises-$17m-to-beautify-big-data/d/d-id/711258/|title=Zoomdata Raises $17m to Beautify Big Data=LightReading- Reedy, Sarah|accessdate=10 October 2014}}&lt;/ref&gt; The approach involves returning the results of a query that is run instantly, while the image ‘sharpens’ and becomes clearer as more data is processed. The product includes a connector studio that directly connects to database, search, streaming, flat file and in-memory data sources. Matt Asay of readwrite.com compared it watching a streaming movie, where you see some results immediately, soon followed by the whole.&lt;ref&gt;{{cite web|last1=Asay|first1=Matt|title=Finally—Business Intelligence Comes To Big Data|url=https://readwrite.com/2014/11/11/business-intelligence-big-data-zoomdata-justin-langseth|website=readwrite.com|accessdate=3 February 2015}}&lt;/ref&gt;

Zoomdata also gives users access to the company’s [[D3.js]]-based Visualization Studio to edit D3 code and create custom visualizations from within the tool. With the Visualization Studio, users can “live-code” and view changes they are making as they type.

==Investors==

In November 2012 Zoomdata announced a $1.1 million seed round with funding from a series of angel investors.&lt;ref&gt;{{cite web|url=http://www.prnewswire.com/news-releases/big-data-visualization-startup-zoomdata-launches-with-11m-of-seed-funding-179107441.html|title=Big Data Visualization Startup Zoomdata Launches with $1.1M of Seed Funding|publisher=PR Newswire|accessdate=13 November 2012}}&lt;/ref&gt; In July 2013 Zoomdata raised a $4.1 million Series A round led by [[Columbus Nova Technology Partners]] (CNTP) with participation from [[New Enterprise Associates]] (NEA), CIT GAP Funds, Razor’s Edge Ventures and B7.&lt;ref&gt;{{cite web|url=https://gigaom.com/2012/11/13/heres-how-it-looks-when-big-data-goes-mobile-first/|title=Here’s how it looks when Big Data goes mobile first   |publisher=Gigaom- Harris, Derrick|accessdate=13 November 2012}}&lt;/ref&gt;  The company announced a $17 million Series B led by [[Accel Partners]] in October 2014 with participation from existing investors NEA, CNTP, Razor’s Edge Ventures, and B7.&lt;ref name=&quot;WaPo&quot;&gt;{{cite news|last1=Overly|first1=Steven|title=Zoomdata collects $17M from slate of investors that includes Accel Partners|accessdate=3 February 2015|work=The Washington Post|date=6 October 2014 |url=http://www.washingtonpost.com/business/capitalbusiness/zoomdata-collects-17m-from-slate-of-investors-that-includes-accel-partners/2014/10/03/52cda050-4a41-11e4-891d-713f052086a0_story.html}}&lt;/ref&gt;

==Management==

CEO Justin Langseth previously founded four other startups: Strategy.com, Claraview,&lt;ref&gt;{{cite web|url=http://dssresources.com/papers/features/langseth/langseth02082004.html|title=Real-Time Data Warehousing: Challenges and Solutions |publisher=DDS Resources|accessdate=2 February 2004}}&lt;/ref&gt; [[Clarabridge]] and Augaroo &lt;ref&gt;{{cite web|url=http://www.oreilly.com/pub/au/5843|title=Justin Langseth |publisher=O’Reilly|accessdate=10 December 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.nvtc.org/tec/JustinLangseth.php|title=An Interview with Justin Langseth |publisher=Northern Virginia Technology Council|accessdate=8 May 2006}}&lt;/ref&gt; and currently holds 14 technology [[Patents]].&lt;ref&gt;{{cite web|url=http://www.patentbuddy.com/Inventor/Langseth-Justin/569432|title=Patents by Inventor Justin Langseth|publisher=Patent Buddy|accessdate =13 December 2013}}&lt;/ref&gt; CFO Bob Aldrich was the CFO of Parature (acquired by [[Microsoft]] in January 2014).&lt;ref&gt;{{cite web|url=http://www.parature.com/parature-names-robert-aldrich-chief-financial-officer/|title=Parature Names Robert Aldrich Chief Financial Officer|publisher=Parature | accessdate =7 March 2008}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://techcrunch.com/2014/01/06/microsoft-acquires-parature-for-100m-in-deal-to-bolster-knowledge-base-for-dynamics-crm-platform/|title=Microsft Acquires for $100m to Boster Knowledge Base for Dynamic CRM Platform|publisher=Tech Crunch =Williams, Alex|accessdate  =6 January 2014}}&lt;/ref&gt;  VP of Marketing and Business Development Russ Cosentino was on the founding teams at [[Clarabridge]] and [[Brainbench]],&lt;ref&gt;{{cite web|url=https://zoomdata.zendesk.com/hc/en-us/articles/203643086-Feb-26-2013-Zoomdata-Launches-Beta-Bringing-Real-Time-Data-Visualization-to-the-iPad/|title=Zoomdata Launches Beta, Brings Real-Time Data Visualization to iPad|publisher=marketwire|accessdate =26 February 2013}}&lt;/ref&gt; VP of Sales Tom Ulrich was head of Sales at [[Actian]] and Gridpoint,&lt;ref&gt;{{cite web|url=http://investing.businessweek.com/research/stocks/private/person.asp?personId=47453438&amp;privcapId=24601844&amp;previousCapId=24601844&amp;previousTitle=Actian%20Corporation/|title=Tom Ulrich|publisher=Business Week|accessdate =3 December 2014}}&lt;/ref&gt; VP of Product Management Farzad Aref led consulting services team at [[Clarabridge]], [[IBM]] and [[Deloitte]],&lt;ref&gt;{{cite web|url= http://www.venturedeal.com/Executives/Farzad-Aref-Zoomdata-VP%20DDASH%20Product%20Management-Profile|title=Farzad Aref|publisher=Venture Deal|accessdate =3 December 2014}}&lt;/ref&gt; VP of Engineering Jorge Alarcon previously led technology efforts at [[Clarabridge]], Global Computer Enterprises, [[CACI]] and [[AMS]] and VP of Customer Solutions Ruhollah Farchtchi led Big Data efforts at [[Unisys]] and analytics at [[Booz Allen]].

==References==
{{Reflist}}

[[Category:2012 establishments in Virginia]]
[[Category:Big data]]
[[Category:Business intelligence companies]]
[[Category:Companies based in Reston, Virginia]]
[[Category:Companies established in 2012]]</text>
      <sha1>a1nxq86lhg4oqlrwliokwwkiwj7ygl0</sha1>
    </revision>
  </page>
  <page>
    <title>Session (web analytics)</title>
    <ns>0</ns>
    <id>39412349</id>
    <revision>
      <id>667144347</id>
      <parentid>647722335</parentid>
      <timestamp>2015-06-16T03:56:33Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor/>
      <comment>[[Yoelle Maarek|Maarek, Yoelle]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14941">'''Sessions''', or '''visits''', is a unit of measurement in [[web analytics]], capturing either a user's actions within a particular time period, or a user's actions in completing a particular task. As well as being directly useful as a metric within web analytics, sessions are also used in operational analytics and to provide personalised features, such as [[Recommender system|user-specific recommendations]] for other pages or items to view. These uses are dependent on session reconstruction - taking a series of user events and splitting the series into a set of sessions - which tends to use one of two classes of methodologies: ''time-oriented approaches'', which use user inactivity as a signal to end a session and begin a new one, and ''navigation-based approaches'', which divide requests into sessions based on an unbroken chain of [[hyperlink]]s between the requested pages.

== Definition ==
The definition of &quot;session&quot; varies, particularly when applied to [[Web search engine|search engines]].{{sfn|Gayo-Avello|2009|p=1824}} Generally, a session is understood to consist of &quot;a sequence of requests made by a single end-user during a visit to a particular site&quot;,{{sfn|Arlitt|2000|p=2}} In the specific context of search engines, &quot;sessions&quot; and &quot;query sessions&quot; have multiple, contradictory and interchangeable definitions;{{sfn|Gayo-Avello|2009|p=1824}} some researchers consider a session or query session to be all queries made by a user in [[Session_reconstruction#Time-oriented_approaches|a particular time period]],{{sfn|Donato|2010|p=324}} while others argue that sessions can be divided thematically, and a &quot;session&quot; is a series of queries with a consistent underlying user need, and that sessions terminate when that need does, even if the user continues searching for other purposes.{{sfn|Gayo-Avello|2009|p=1825}}{{sfn|Lam|2007|p=147}}

== Uses ==
Sessions can be used directly in web analytics, with sessions-per-user serving as a metric of website usage.{{sfn|Weischdel|2006|p=464}}{{sfn|Catledge|1995|p=5}} Other metrics used within research and applied web analytics include session length,{{sfn|Jansen|2006|p=10}} and user actions per session;{{sfn|Jansen|2000|p=12}} session length, particularly, is seen as a more accurate alternative to measuring [[page view]]s.{{sfn|Khoo|2008|p=377}} With all of these metrics, and with sessions as a concept, the goal is to improve the website's usability, due to the substantial impact that usability has on website usage and operator profits.{{sfn|Heer|2002|p=243}} Sessions are also used to provide personalised features such as [[Recommender system|user-specific recommendations]] and search term suggestions.{{sfn|Huang|2003|p=638}}

Reconstructed sessions have also been used to measure total user input, including to measure the number of [[man-hour|labour hour]]s taken to construct [[Wikipedia]].{{sfn|Geiger|2014|p=1}} Sessions are also used for operational analytics, including developing data anonymisation methodologies, identifying anomalies in networking,{{sfn|Meiss|2009|p=177}} and [[Benchmark (computing)|synthetic workload generation]] for testing servers with artificial traffic.{{sfn|Arlitt|2000|p=8}} Some writers have argued that sessions are not appropriate as a workload characterisation metric within the context of [[e-commerce]] platforms, due to substantial variations in how different classes of user interact with that type of site. Instead, a [[state transition network]] is suggested.{{sfn|Menascé|1999|p=119}}

== Session reconstruction ==
[[File:Time vs. Navigation orientation (Session reconstruction).svg|thumb|right|an illustration of the different criteria used by different session reconstruction approaches.]]
Essential to the use of sessions in web analytics is being able to identify them. This is known as &quot;session reconstruction&quot;. Approaches to session reconstruction can be divided into two main categories: time-oriented, and navigation-oriented.{{sfn|Spiliopoulou|2003|p=176}}

=== Time-oriented approaches ===
Time-oriented approaches to session reconstruction look for a period of inactivity, or &quot;inactivity threshold&quot;: a span of time between requests by a user. Once this period of inactivity is reached, the user is assumed to have left the site or stopped using the browser entirely, and the session is ended: further requests from the same user are considered a second session. A common value for the inactivity threshold is 30 minutes,{{sfn|Eickhoff|2014|p=3}}{{sfn|Ortega|2010|p=332}} a well-established value sometimes described as the industry standard.{{sfn|Eickhoff|2014|p=3}} The utility of this value has been questioned: some researchers have argued that it produces artefacts around naturally long sessions,{{sfn|Mehrzadi|2012|p=3}} and have experimented with other thresholds, including 10 and 60 minutes.{{sfn|He|2002|p=733}} Despite this, Jones &amp; Klinkner argue in a paper at the 2008 Conference on Information and Knowledge Management that, at least in relation to search data, &quot;no time threshold is effective at identifying [sessions]&quot;.{{sfn|Jones|2008|p=2}}

One alternative that has been proposed is using user-specific thresholds rather than a single, global threshold for the entire dataset.{{sfn|Murray|2006|p=3}}{{sfn|Mehrzadi|2012|p=1}} This has the problem of assuming that the thresholds follow a [[bimodal distribution]], and is not suitable for datasets that cover a long period of time.{{sfn|Mehrzadi|2012|p=3}}

=== Navigation-oriented approaches ===
Navigation-oriented approaches exploit the structure of websites - specifically, the presence of [[hyperlink]]s and the tendency of users to navigate between pages on the same website by clicking on them, rather than typing the full URL into their browser.{{sfn|Spiliopoulou|2003|p=176}} One way of identifying sessions by looking at this data is to build a map of the website: if the user's first page can be identified, the &quot;session&quot; of actions lasts until they land on a page which cannot be accessed from any of the previously-accessed pages. This takes into account backtracking, where a user will retrace their steps before opening a new page.{{sfn|Cooley|1999|p=19}} A simpler approach, which does not take backtracking into account, is to simply require that the [[HTTP referer]] of each request be a page that is already in the session. If it is not, a new session is created.{{sfn|Cooley|1999|p=23}} This class of heuristics &quot;exhibits very poor performance&quot; on websites that contain [[Framing (World Wide Web)|framesets]].{{sfn|Berendt|2003|p=179}}

== References ==
{{reflist|3}}

== Bibliography ==
*{{cite journal|last=Arlitt|first=Martin|date=2000|title=Characterizing Web User Sessions|journal=SIGMETRICS Performance Evaluation Review|volume=28|url=http://www.hpl.hp.com/techreports/2000/HPL-2000-43.pdf|ref=harv}}
*{{cite book|last=Berendt|first=Bettina|coauthors=Mobasher, Bamshad; Nakagawa, Miki; Spiliopoulou, Myra.|date=2012|title=WEBKDD 2002 - Mining Web Data for Discovering Usage Patterns and Profiles|publisher=Springer|date=2003|series=WEBKDD|chapter=The Impact of Site Structure and User Environment on Session Reconstruction in Web Usage Analysis|url=http://warhol.wiwi.hu-berlin.de/~berendt/Papers/webkdd02_bookversion.pdf|isbn=978-3-540-39663-5|ref=harv|doi=10.1007/978-3-540-39663-5_10}}
*{{cite journal|last=Catledge|first=L.|coauthors=Pitkow, J.|date=1995|title=Characterizing browsing strategies in the world-wide web|journal=Proceedings of the Third International World-Wide Web Conference on Technology, tools and applications|volume=27|url=https://smartech.gatech.edu/xmlui/bitstream/handle/1853/3558/95-13.pdf|ref=harv|doi=10.1016/0169-7552(95)00043-7}}
*{{cite journal|last=Cooley|first=Robert|coauthors=Mobasher, Bamshad; Srivastava, Jaideep|date=1999|title=Data Preparation for Mining World Wide Web Browsing Patterns|journal=Knowledge and Information Systems|publisher=Springer|volume=1|issue=1|issn=0219-3116|url=http://facweb.cs.depaul.edu/mobasher/classes/ect584/papers/cms-kais.pdf|ref=harv|doi=10.1007/BF03325089}}
*{{cite journal|last=Donato|first=Debora|coauthors=Bonchi, Francesco; Chi, Tom; [[Yoelle Maarek|Maarek, Yoelle]].|date=2010|title=o you want to take notes?: identifying research missions in Yahoo! search pad|journal=Proceedings of the 19th International Conference on World Wide Web|publisher=ACM|url=http://www.francescobonchi.com/p321.pdf|ref=harv}}
*{{cite journal|last=Eickhoff|first=Carsten|coauthors=Teevan, Jaime., White, Ryen., Dumais, Susan.|date=2014|title=Lessons from the Journey: A Query Log Analysis of Within-Session Learning|journal=Proceedings of the Seventh International Conference on Web Search and Web Data Mining|publisher=ACM|url=http://research.microsoft.com/en-us/um/people/teevan/publications/papers/wsdm14.pdf|ref=harv|doi=10.1145/2556195.2556217}}
*{{cite journal|last=Gayo-Avello|first=Daniel|date=2009|title=A survey on session detection methods in query logs and a proposal for future evaluation|journal=Information Sciences|issue=179|issn=0020-0255|ref=harv|url=http://www.msit2005.mut.ac.th/msit_media/1_2552/itec0950/materials/20090715164101yh.pdf}}
*{{cite journal|last=Geiger|first=R.S.|coauthors=Halfaker, A.|date=2014|title=Using Edit Sessions to Measure Participation in Wikipedia|journal=Proceedings of the 2013 ACM Conference on Computer Supported Cooperative Work|publisher=ACM|url=http://www.stuartgeiger.com/cscw-sessions.pdf|doi=10.1145/2441776.2441873|ref=harv}}
*{{cite journal|last=He|first=Daqing|coauthors=Goker, Ayse; Harper, David J.|date=2002|title=Combining evidence for automatic Web session identification|journal=Information Processing and Management|publisher=Elsevier|volume=38|issn=0306-4573|ref=harv|doi=10.1016/S0306-4573(01)00060-7}}
*{{cite journal|last=Heer|first=Jeffrey|coauthors=Chi, Ed H.|date=2002|title=Separating the swarm: categorization methods for user sessions on the web|journal=Proceedings of the SIGCHI Conference on Human factors in Computing Systems|publisher=ACM|volume=4|issue=1|ref=harv}}
*{{cite journal|last=Huang|first=Chien‐Kang|coauthors=Chien, Lee‐Feng; Oyang, Yen‐Jen.|date=2003|title=Relevant term suggestion in interactive web search based on contextual information in query session logs|journal=Journal of the American Society for Information Science and Technology|publisher=American Society for Information Science and Technology|volume=54|issue=7|ref=harv}}
*{{cite journal|last=Jansen|first=Bernard J.|coauthors=Spink, Amanda. Saracevic, Tefko|date=2000|title=Real life, real users, and real needs: a study and analysis of user queries on the web|journal=Information Processing and Management|volume=36|issn=0306-4573|url=http://faculty.ist.psu.edu/jjansen/academic/pubs/ipm98/ipm98.pdf|ref=harv|doi=10.1016/S0306-4573(99)00056-4}}
*{{cite journal|last=Jansen|first=Bernard J.|coauthors=Spink, Amanda|date=2006|title=How are we searching the world wide web? A comparison of nine search engine transaction logs|journal=Information Processing and Management|volume=42|issue=1|issn=0306-4573|url=http://eprints.qut.edu.au/4945/1/4945_1.pdf|ref=harv|doi=10.1016/j.ipm.2004.10.007}}
*{{cite journal|last=Jones|first=Rosie|coauthors=Klinkner, Kristina Lisa|date=2008|title=Beyond the Session Timeout: Automatic Hierarchical Segmentation of Search Topics in Query Logs|journal=CIKM 08|publisher=ACM|url=http://www.cs.cmu.edu/~rosie/papers/jonesKlinknerCIKM2008.pdf|ref=harv|doi=10.1145/1458082.1458176}}
*{{cite journal|last=Khoo|first=Michael|coauthors=Pagano, Joe; Washington, Anne L.; Recker, Mimi; Palmer, Bart; Donahue, Robert A. |date=2008|title=Using Web Metrics to Analyze Digital Libraries|journal=Proceedings of the 8th ACM/IEEE-CS joint conference on Digital libraries|publisher=ACM|url=http://www.ist.drexel.edu/faculty/mkhoo/docs/09_jcdl.pdf|ref=harv}}
*{{cite journal|last=Lam|first=Heidi|coauthors=Russell, Daniel; Tang, Diane; Munzner, Tamara.|date=2007|title=Session viewer: Visual exploratory analysis of web session logs|journal=IEEE Symposium on Visual Analytics Science and Technology|publisher=IEEE|ref=harv}}
*{{cite book|last=Mehrzadi|first=David|coauthors=Feitelson, Dror G.|title=Proceedings of the 5th Annual International Systems and Storage Conference|publisher=ACM|date=2012|series=SYSTOR '12|chapter=On Extracting Session Data from Activity Logs|url=http://www.cs.huji.ac.il/~feit/papers/Ses12SYSTOR.pdf|isbn=978-1-4503-1448-0|ref=harv|doi=10.1145/2367589.2367592}}
*{{cite journal|last=Meiss|first=Mark|coauthors=Duncan, John; Gonçalves, Bruno; Ramasco, José J.; Menczer, Filippo|date=2009|title=What’s in a Session: Tracking Individual Behavior on the Web|journal=Proceedings of the 20th ACM conference on Hypertext and hypermedia|publisher=ACM|url=http://ifisc.uib-csic.es/~jramasco/text/hypertext09.pdf|ref=harv}}
*{{cite journal|last=Menascé|first=Daniel A.|coauthors=Almeida, V.; Fonseca, R.; Mendes, M.|date=1999|title=A Methodology for Workload Characterization of E-commerce Sites|journal=Proceedings of ACM Conference on Electronic Commerce|publisher=ACM|url=http://cs.brown.edu/~rfonseca/pubs/menasce99e-com-char.pdf|ref=harv}}
*{{cite journal|last=Murray|first=G. Craig|coauthors=Lin, Jimmy; Chowdhury, Abdur.|date=2006|title=Identification of User Sessions with Hierarchical Agglomerative Clustering|journal=Proceedings of the American Society for Information Science and Technology|publisher=American Society for Information Science and Technology|volume=43|issue=1|url=http://www.umiacs.umd.edu/~jimmylin/publications/Murray_etal_ASIST2006.pdf|ref=harv|doi=10.1002/meet.14504301312}}
*{{cite journal|last=Ortega|first=J.L.|coauthors=Aguillo, I.|date=2010|title=Differences Between Web Sessions According to the Origin of their Visits|journal=Journal of Informetrics|publisher=Elsevier|volume=4|issue=3|issn=1751-1577|url=http://isidroaguillo.webometrics.info/sites/default/files/publicaciones/Ortega2010-Differences_between_web_sessions_according_to_the_origin_of_their_visits.pdf|ref=harv|doi=10.1016/j.joi.2010.02.001}}
*{{cite journal|last=Spiliopoulou|first=Myra|coauthors=Mobasher, Bamshad; Berendt, Bettina; Nakagawa, Miki;|date=2003|title=A framework for the evaluation of session reconstruction heuristics in web-usage analysis|journal=Informs Journal on Computing|issn=1526-5528|url=http://warhol.wiwi.hu-berlin.de/~berendt/Papers/spiliopoulou_etal_2003.pdf|ref=harv|doi=10.1287/ijoc.15.2.171.14445}}
*{{cite journal|last=Weischdel|first=Birgit|coauthors=Huizingh, Eelko K. R. E.|date=2006|title=Website optimization with web metrics: a case study|journal=Proceedings of the 8th International Conference on Electronic Commerce|url=http://aaa.volospin.com/BT606B/Website_Optimization_p463-weischedel.pdf|ref=harv|doi=10.1145/1151454.1151525}}

[[Category:Marketing terminology]]
[[Category:Business intelligence]]
[[Category:Data mining]]
[[Category:Big data]]
[[Category:Web analytics]]
[[Category:Internet marketing]]</text>
      <sha1>fhlrsltvdqd1wvud86psfconk53ee0w</sha1>
    </revision>
  </page>
  <page>
    <title>Professional Services Champions League</title>
    <ns>0</ns>
    <id>45461394</id>
    <revision>
      <id>665663658</id>
      <parentid>665663542</parentid>
      <timestamp>2015-06-05T20:04:04Z</timestamp>
      <contributor>
        <username>Rubbish computer</username>
        <id>23085006</id>
      </contributor>
      <comment>/* Financial Modeling World Championships */Reverted own edit as misread</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4737">{{Multiple issues|{{more footnotes|date=February 2015}}{{peacock|date=February 2015}}{{advert|date=February 2015}}}}

'''Professional Services Champions League''' (PSCL) is an organization responsible for world championship leagues for the Professional Services and Consulting industry.  The World Championship events include the Financial Modeling World Championships, Big Data Analytics World Championships, Acturial World Championships and Customer Loyalty Gamification World Championships.

== History ==
PSCL was founded in 2012 by Johann Odou and John R Persico in Melbourne, Australia. PSCL attracts over 10,000 professionals and students from 110 countries annually. Each world championship involves two online qualification rounds, followed by an Top 16 Finals in a host city. Major partners include [[IBM]], [[Microsoft]],&lt;ref&gt;{{cite web|title= Microsoft Sponsors of Excel World Championship|author=Microsoft|url=http://blogs.office.com/2014/10/23/microsoft-sponsors-modeloff-2014/}}&lt;/ref&gt; [[KPMG]], [[Thomson Reuters]] and [[Intralinks]].&lt;ref&gt;{{cite web|title= Intralinks Major Sponsor of ModelOff|author=Intralinks|url=http://blogs.intralinks.com/dealcloser/2014/08/pscl-intralinks-dealspace-partner-2014-modeloff}}&lt;/ref&gt;

== Overview ==

=== Financial Modeling World Championships ===
The competition is among the world's largest international [[financial modeling]] competitions for 4,000 professionals and students in [[Financial Analysis]], [[Investment Banking]], [[Corporate Finance]], [[Accounting]], [[Actuarial]] and [[Analytics]] professionals. The annual Finals are held in New York.&lt;ref&gt;{{cite web|title= New Zealand whiz kid crowned financial modelling world champion at ModelOff - Australian Financial Review|author=Australian Financial Review|url=http://www.afr.com/f/free/markets/capital/cfo/cfo_kiwi_whiz_kid_crowned_modelling_PZwvC5kFItKaApz7sEcAgP}}&lt;/ref&gt;

=== Big Data Analytics World Championships ===
The international business [[hackathon]] attracts 2,500 professionals and students in [[Business Intelligence]], [[Business Analytics]], [[Software Engineering]] and [[Big Data]] disciplines. The annual Finals are held in Austin, Texas. KPMG, IBM, [[CISCO]], [[Hortonworks]] are major global partners.&lt;ref&gt;{{cite web|title= KPMG Sponsors TEXATA Big Data Analytics World Championship - KPMG|author=KPMG|url=http://www.kpmg.com/global/en/issuesandinsights/articlespublications/press-releases/pages/kpmg-becomes-major-sponsor-of-texata-2014.aspx}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title= Graduate student makes Finals of TEXATA Big Data Worldwide competition - University of Missouri|author=University of Missouri|url=http://engineering.missouri.edu/2014/12/grad-student-selected-as-finalist-for-worldwide-texata-competition/}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title= Vietnam comes second at Big Data Analytics World Title - Vietnam Express News|author=Vietnam Express News|url=http://vnexpress.net/tin-tuc/the-gioi/nguoi-viet-5-chau/co-gai-viet-gianh-giai-nhi-cong-nghe-thong-tin-quoc-te-3113627.html}}&lt;/ref&gt;

=== Customer Loyalty and Gamification World Championships ===
The international [[case study]] competition has over 2,000 professionals and students participating, typically from [[Marketing]], [[Innovation]] and [[Management Consulting]]. Major Sponsors include: [[Acxiom]], [[World Vision]], [[Maritz, LLC|Maritz]], [[LoyaltyOne]]. The 2015 Finals will be held in Toronto, Canada.&lt;ref&gt;{{cite web|title= Local CTO scoops Loyalty Gamification World Championship World Title - CIO Magazine |author=CIO Magazine|url=http://www.itweb.co.za/index.php?option=com_content&amp;view=article&amp;id=137018}}&lt;/ref&gt;

=== Actuarial Modeling World Championships ===
The international competition is focused on Actuarial Sciences, Insurance and Risk Analysis.

==See also==
* [[Financial Modeling]]
* [[Big Data]]
* [[Gamification]]
* [[Actuarial Science]]
* [[United States Academic Decathlon]]
* [[Student competition]]
* [[Student design competition]]
* [[TopCoder.com|TopCoder]]

==References ==
&lt;references /&gt;

==External links==
*[http://www.pscl.co/ Professional Services World Championship League]
*[http://www.modeloff.com/ Financial Modeling World Championship]
*[http://www.texata.com/ Big Data Analytics World Championship]
*[http://www.acturian.com/ Actuarial Sciences World Championship]
*[http://www.theloyaltygames.com/ Customer Loyalty and Gamification World Championship]

[[Category:Competitions]]
[[Category:Forecasting competitions]]
[[Category:Big data]]
[[Category:Finance]]
[[Category:Mathematics]]
[[Category:Business plan competitions]]
[[Category:Intellectual competitions]]
[[Category:Student quiz competitions]]
[[Category:Quiz games]]
[[Category:Challenge awards]]
[[Category:United States Academic Decathlon]]</text>
      <sha1>81s8yyl2kll6z2bxyitgr91ok94zy1v</sha1>
    </revision>
  </page>
  <page>
    <title>NoSQLz</title>
    <ns>0</ns>
    <id>45567577</id>
    <revision>
      <id>650537595</id>
      <parentid>649730813</parentid>
      <timestamp>2015-03-09T01:10:28Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10850)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1819">{{Infobox software
| name                   = NoSQLz
| logo                   =
| screenshot             =
| caption                =
| author                 =
| developer              = NoSQLz developers
| released               = 2014
| status                 = Active
| latest release version = v2.1.0
| latest release date    = {{release date|2015|03|02}}
| latest preview version = 
| latest preview date    = 
| frequently updated     = yes
| programming language   = [[IBM High Level Assembler]]
| operating system       = [[z/OS]]
| language               = English
| genre                  = [[Key-value pair|Key-value]] store
| license                = [[Free software]] / [[Proprietary software]]
| website                = http://www.nosqlz.com
}}

'''NoSQLz''' is a consistent key-value [[Big data]] store ([[NoSQL]] database) for [[z/OS]] IBM systems.&lt;ref&gt;[http://www.nosqlz.com NoSQLz project homepage]&lt;/ref&gt; It was developed by systems programmer Thierry Falissard in 2013. The purpose was to provide a low-cost alternative to all proprietary [[mainframe]] [[DBMS]] (version 1 is [[free software]]).

==Distinctive Features==
NoSQLz only provides basic [[CRUD]] functions. It is designed to be very straightforward and easy to implement.

[[ACID]] properties are provided, so as to have &quot;real transactions&quot;, through [[optimistic concurrency control]], [[timestamp-based concurrency control]] and [[multiversion concurrency control]] (MVCC).

==Interfaces==
Unlike version 1, version 2 of NoSQLz is chargeable and supports [[IBM Parallel Sysplex]]. The NoSQLz DBMS can be interfaced in [[Rexx]], [[Cobol]], [[IBM High Level Assembler]], etc.

==References==
{{Reflist}}

{{DEFAULTSORT:NoSQLz}}
[[Category:Free database management systems]]
[[Category:NoSQL]]
[[Category:Big data]]
[[Category:Databases]]</text>
      <sha1>aguc5iyeyladnngkk5tqiywl3xrd2kl</sha1>
    </revision>
  </page>
  <page>
    <title>Metafor Software</title>
    <ns>0</ns>
    <id>39608502</id>
    <revision>
      <id>668701195</id>
      <parentid>668700103</parentid>
      <timestamp>2015-06-26T01:55:37Z</timestamp>
      <contributor>
        <ip>204.107.141.240</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5595">{{Infobox company
  | logo=
  | name =Metafor Software
  | type=[[Private Company]]
  | industry= [[Anomaly detection]]
  | foundation=2010
  | location_city=[[Vancouver, BC]]
  | location_country=[[Canada]]
  | key_people = {{unbulleted list
    | John Fowlkes {{small|([[CEO]])}}
    | Toufic Boubez {{small|([[Chief technology officer|CTO]], [[Entrepreneurship|Co-founder]])}}
}}
  | website=http://metaforsoftware.com
}}
'''Metafor Software''' is an [[anomaly detection]] and [[predictive analytics]] company based in Vancouver, British Columbia. The company produces software for [[big data]] security [[analytics]] and [[IT Operations Analytics|IT Operations Analytics (ITOA)]]. Using [[machine learning]] techniques inspired by how the human brain works, Metafor detects unexpected changes and behavioral anomalies in [[Machine-generated data]], and sends alerts when systems start deviating from the observed norm.&lt;ref&gt;{{cite web|title=http://www.crunchbase.com/company/metafor-software|url=http://www.crunchbase.com/company/metafor-software/|publisher=CrunchBase}}&lt;/ref&gt; Metafor uses [[Unsupervised learning|unsupervised]], [[non-parametric]] behavioral analytics to identify activity indicative of security threats and impending performance problems.&lt;ref&gt;{{cite web|last= Kusnetzky|first= Dan|title= Metafor Software CTO talks about challenges in anomaly detection |url= http://www.zdnet.com/metafor-software-cto-talks-about-challenges-in-anomaly-detection-7000029936/|publisher=ZDNet}}&lt;/ref&gt; By quickly locating inconsistencies, Metafor can replace manual troubleshooting with automated diagnostics.&lt;ref&gt;{{cite web|last=Chan|first=Elliot|title= Metafor Software: Detecting Unexpected Changes in Your Computers |url= http://www.techvibes.com/blog/metafor-software-toufic-boubez-2013-05-30|publisher=TECHVIBES}}&lt;/ref&gt; They are aiming to build a product that can not just alert about anomalies, but also help normalize them with a single mouse click.
&lt;ref&gt;{{cite web|last=Smolaks|first=Max|title= Metafor Software: Using Machine Learning To Predict Server Issues |url= http://www.techweekeurope.co.uk/interview/metafor-software-uses-machine-learning-to-predict-server-issues-115532/|publisher=TechWeek europe}}&lt;/ref&gt;

==History==
Metafor Software was founded in 2010 by Toufic Boubez and Jenny Yang. Toufic previously co-founded Layer 7 Technologies, which was acquired by CA Technologies in 2013.&lt;ref&gt;{{cite web|last=Shaw|first=Gilian|title= Layer 7 sale to CA Technologies creates new millionaires in Vancouverís tech sector |url= http://blogs.vancouversun.com/2013/05/03/layer-7-sale-to-ca-technologies-creates-new-millionaires-in-vancouvers-tech-sector/|publisher=Vancouver Sun}}&lt;/ref&gt; In 2010 Metafor won the Generator Challenge - a competition created to recognize Vancouver's most promising early-stage technology companies and were awarded free workspace at the Discovery Parks campus.&lt;ref&gt;{{cite web|last=Knowlton|first=Thomas|title= Vancouver awards eight innovative startups free tech spaceóis the next HootSuite or EA among them?|url= http://www.techvibes.com/blog/vancouver-awards-eight-innovative-startups-free-tech-spaceis-the-next-hootsuite-or-ea-among-them|publisher=TECHVIBES}}&lt;/ref&gt; Metafor Software was named to the 2013 ICT Emerging Rockets list as part of the Ready to Rocket recognition program.&lt;ref&gt;{{cite web| title= Metafor Software Named to the 2013 ICT Emerging Rockets List |url= http://www.prweb.com/releases/anomaly/detection/prweb10583566.htm/|publisher=PRWeb}}&lt;/ref&gt; In April 2013 Metafor started providing its [[software as a service]] (SaaS) based anomaly detection engine as a free Beta.&lt;ref&gt;{{cite web|title= Metafor Software Solves Server Configuration Drift ñ Announces Environment Anomaly Detection Engine |url= http://vanedgecapital.com/metafor-software-solves-server-configuration-drift-announces-environment-anomaly-detection-engine/|publisher=Vanedge Capital}}&lt;/ref&gt; In 2014, Metafor was named Cool Vendor in Application Performance Monitoring (APM) and IT Operations Analytics (ITOA)î by analyst firm [[Gartner]].&lt;ref&gt;{{cite web|last= Kowall|first= Jonah|title= Cool Vendors in Application Performance Monitoring (APM) and IT Operations Analytics (ITOA) |url= http://blogs.gartner.com/jonah-kowall/2014/04/30/cool-vendors-in-application-performance-monitoring-apm-and-it-operations-analytics-itoa/|publisher=Gartner}}&lt;/ref&gt; In June 2015, Metafor Software was acquired by [[Splunk]].&lt;ref&gt;{{cite web|last= Raywood|first= Dan|title= Splunk expands anomaly detection and machine learning with Metafor acquisition |url= https://451research.com/report-short?entityId=85848|publisher=451 Research}}&lt;/ref&gt;

==References==
{{reflist}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;

==External links==
* [http://metaforsoftware.com  Metafor Software Website]

[[Category:Computer security software companies]]
[[Category:Computer security companies]]
[[Category:Data security]]
[[Category:Database security]]
[[Category:Computer performance]]
[[Category:Analytics]]
[[Category:Big data]]
[[Category:Computer data]]
[[Category:Companies established in 2010]]
[[Category:Computer companies of Canada]]
[[Category:Software companies of Canada]]
[[Category:Performance management]]
[[Category:Information technology management]]
[[Category:Applied machine learning]]
[[Category:Network management]]
[[Category:System administration]]
[[Category:Companies based in Vancouver]]</text>
      <sha1>g8hhspi5oonal642k4p8j0qup3zcs4h</sha1>
    </revision>
  </page>
  <page>
    <title>Qizx</title>
    <ns>0</ns>
    <id>45416001</id>
    <revision>
      <id>669038718</id>
      <parentid>669016392</parentid>
      <timestamp>2015-06-28T13:19:40Z</timestamp>
      <contributor>
        <username>Walter Görlitz</username>
        <id>121745</id>
      </contributor>
      <comment>/* External links */ removed parent cat/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2367">{{Infobox software
| name                   = Qizx XML DBMS
| genre                  = [[Native XML database]]
| programming language   = [[Java (programming language)|Java]]
| operating system       = [[Cross-platform]]
| license                = [[Commercial software|Commercial]]
}}
'''Qizx''' is a proprietary [[XML database]] that provides [[XML database|native]] storage for [[XML]] data.

'''Qizx''' was initially developed by Xavier Franc of Axyana&lt;ref name=xavier&gt;{{cite web|last1=Franc|first1=Xavier|title=Xavier Franc Linked in profile|url=http://fr.linkedin.com/pub/xavier-franc/21/779/38|website=Linked in|accessdate=16 February 2015}}&lt;/ref&gt; and was purchased by [[Qualcomm]] in 2013.&lt;ref&gt;{{cite web|title=Qizx - native XML database engine supporting XQuery / XQuery Update / XQuery Full-Text / XPath2|url=http://www.axyana.com/|accessdate=16 February 2015}}&lt;/ref&gt;  '''Qizx''' was re-released by [[Qualcomm]] in late 2014 on [[Amazon Web Services]].&lt;ref name=qizxaws&gt;{{cite web|url=https://aws.amazon.com/marketplace/pp/B00RM04ZFM|title=Qualcomm Qizx on AWS Marketplace|accessdate=16 February 2015}}&lt;/ref&gt;

== Packaging and Bindings ==

The '''Qizx''' database can be configured to run embedded into an application, in a hosted client-server environment, or as a software service hosted on [[Amazon Web Services]].  The client-server version supports database [[Computer_cluster|clustering]] for both [[Load_balancing_(computing)|load balancing]] and [[Data_redundancy|data redundancy]].  '''Qizx''' is bundled with a multi-platform GUI client and can also be accessed through a [[REST]]ful API that includes embedded online documentation.

'''Qizx''' includes bindings in [[Java_(programming_language)|Java]], [[Python (programming language)|Python]], [[C]] and [[C#]] as well as native [[XPath]] and [[XQuery]] support.  '''Qizx''' also provides a number of extensions to the [[XQuery]] language for updating documents, accessing document metadata and various other tasks.  

== References ==
{{reflist}}

== External links ==
* [https://www.qualcomm.com/qizx Qualcomm Qizx official website]
* [https://aws.amazon.com/marketplace/pp/B00RM04ZFM Qualcomm Qizx on AWS Marketplace]

[[Category:Big data]]
[[Category:Database-related software for Linux]]
[[Category:NoSQL]]
[[Category:Qualcomm software]]
[[Category:Semantic Web]]
[[Category:XML databases]]</text>
      <sha1>f2sgx272bbgyzdf66w94bswg29y4mdi</sha1>
    </revision>
  </page>
  <page>
    <title>HPCC Systems</title>
    <ns>0</ns>
    <id>44648749</id>
    <revision>
      <id>656676407</id>
      <parentid>645975300</parentid>
      <timestamp>2015-04-15T23:13:57Z</timestamp>
      <contributor>
        <username>Iaritmioawp</username>
        <id>22040210</id>
      </contributor>
      <comment>Added three categories.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2489">'''HPCC Systems''' is part of LexisNexis Risk Solutions and was formed to promote and sell the HPCC big data platform. In June 2011, it announced the offering of the HPCC Systems platform under an open source dual license model.&lt;ref&gt;{{cite web |title=LexisNexis open-sources its Hadoop killer |url=https://gigaom.com/2011/06/15/lexisnexis-open-sources-its-hadoop-killer/ |date=15 June 2011 |accessdate=8 November 2014 |website=GigaOM}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=LexisNexis Will Open-Source Its Hadoop Alternative for Handling Big Data |url=http://readwrite.com/2011/06/15/lexisnexis-open-sources-its-hadoop-alternative |date=15 June 2011 |accessdate=20 November 2014 |website=ReadWrite}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=HPCC A New/Old Kid In Town To Take On Hadoop |url=http://www.networkworld.com/community/blog/hpcc-newold-kid-town-take-hadoop |date=16 June 2011 |accessdate=2 December 2014 |website=NetworkWorld}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=LexisNexis Joins Linux Foundation |url=http://readwrite.com/2011/06/15/lexisnexis-open-sources-its-hadoop-alternative |date=17 June 2011 |accessdate=29 November 2014 |website= The Linux Foundation}}&lt;/ref&gt;

HPCC Systems offers both a Community Edition and an Enterprise Edition. The Community Edition is free to download, includes the source code and is released under the GNU Affero GPL v.3 license. The Enterprise Edition is available under a paid commercial license and includes training, support, indemnification and additional modules.
In November 2011, HPCC Systems announced the availability of its Thor Data Refinery Cluster on the Amazon Web Services platform.&lt;ref&gt;{{cite web |title= HPCC Announces Availability of ETL Cluster On Amazon Web Services |url=http://cloud-computing-today.com/2011/12/03/hpcc-announces-availability-of-etl-cluster-on-amazon-web-services/ |date=17 December 2012 |accessdate=30 November 2014 |website= Cloud Computing Today}}&lt;/ref&gt;
In January 2012, HPCC Systems made available a number of distributed Machine Learning algorithms on its HPCC platform.]&lt;ref&gt;{{cite web |title=HPCC Systems Intros Machine Learning Beta |url=http://www.datanami.com/2012/01/31/hpcc_systems_intros_machine_learning_beta/ |date=31 January 2012 |accessdate=29 November 2014 |website= Datanami}}&lt;/ref&gt;
HPCC Systems is an alternative to Hadoop and legacy mainframe computing technology.

==See also==
*[[HPCC]]

==References==
{{Reflist}}

[[Category:Big data]]
[[Category:Data warehousing products]]
[[Category:Distributed file systems]]</text>
      <sha1>ehxb3t9f02932rqnx42wrzl2ojy2rm9</sha1>
    </revision>
  </page>
  <page>
    <title>LogiNext</title>
    <ns>0</ns>
    <id>46442556</id>
    <revision>
      <id>673520570</id>
      <parentid>668921776</parentid>
      <timestamp>2015-07-28T19:46:08Z</timestamp>
      <contributor>
        <ip>12.180.133.18</ip>
      </contributor>
      <comment>Filled in 3 bare reference(s) with [[:en:WP:REFILL|reFill]], spelling and capitalization edits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6647">{{Multiple issues|
{{advert|date=April 2015}}
{{orphan|date=April 2015}}
}}

&lt;!-- EDIT BELOW THIS LINE --&gt;
{{Infobox company
| name             = LogiNext Solutions
| logo             = 
| type             = [[Private Limited|Private]] 
| traded_as        = &lt;!-- not a public company --&gt;
| foundation       = [[Mountain View, California|Mountain View]], [[California|CA]], [[USA]]  ({{Start date|2013|10}}), [[Mumbai]], [[India]]  ({{Start date|2014|01}})
| location_city    = [[Mumbai]]
| location_country = [[India]]
| area_served      = Worldwide
| key_people       = Dhruvil Sanghvi &lt;small&gt;(Co-Founder, [[Chief executive officer|CEO]])&lt;/small&gt;,Manisha Raisinghani &lt;small&gt;(Co-Founder, [[Chief technology officer|CTO]])&lt;/small&gt;
| industry = Logistics and Advanced Data Analytics
| revenue =  &lt;!-- not disclosed --&gt;
| num_employees = 11-30 (Mar 2015)
| homepage = [http://www.loginextsolutions.com www.loginextsolutions.com]
| footnotes =
| intl = yes
}}
'''LogiNext Solutions''' is privately held [[big data analytics]] start-up backed by Indian Angel Network. LogiNext uses smart technology to provide real-time visibility and optimization solutions to logistics companies. This helps in avoiding delays, incorporating transparency, real-time tracking to get unique insights about their moving assets, distribution networks, inventory and supply chain.&lt;ref&gt;https://storify.com/StartupSpeaks/taking-logistics-to-the-next-level StartupSpeaks&lt;/ref&gt;

==History==
LogiNext was started by Dhruvil Sanghvi and Manisha Raisinghani in the US in late 2013.&lt;ref&gt;http://www.nextbigwhat.com/loginext-real-time-tracking-logistics-297/ NextBigWhat&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.firstfewcustomers.com/understanding-the-buying-behavior-of-large-enterprises-and-re-working-your-strategy/|title=Understand Buyer Behavior of enterprises to develop product - First Few Customers|work=First Few Customers|accessdate=28 July 2015}}&lt;/ref&gt; The co-founders are graduates from [[Carnegie Mellon University]]. Before starting LogiNext, Dhruvil has worked at [[Deloitte Consulting]],  [[A.T. Kearney]] and  [[Ernst &amp; Young]] and Manisha with [[IBM]] and [[Mastek]]. The company is serving around 15 paying clients and is doing pilots for 25 companies, including [[Flipkart]] and [[Snapdeal]].&lt;ref&gt;http://www.vccircle.com/news/technology/2015/04/07/saas-based-logistics-focused-big-data-startup-loginext-raises-funding-ian  VC Circle&lt;/ref&gt; LogiNext received a funding of USD 500,000 from Indian Angel Network in April 2015.&lt;ref&gt;http://economictimes.indiatimes.com/news/emerging-businesses/startups/logistics-analytics-startup-loginext-raises-funds-from-ian/articleshow/46839846.cms Economic Times&lt;/ref&gt;&lt;ref&gt;http://www.livemint.com/Companies/Rzg9pSj2wVQExFh5Fy8UiN/LogiNext-raised-funds-from-Indian-Angel-Network.html Live Mint&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.medianama.com/2015/04/223-funding-roundup-swiggy-instalively-and-loginext/|title=Funding roundup: Swiggy, InstaLively and LogiNext|author=Sneha Johari|work=MediaNama|accessdate=28 July 2015}}&lt;/ref&gt;

==Partnership with Reliance Industries ltd and Microsoft==

[[Mukesh Ambani]] controlled [[Reliance Industries]] (RIL) partnered with Microsoft Ventures to set up a GenNext Innovation Hub that serves as an [[Business incubator|incubator]] for start-ups in India.&lt;ref&gt;http://www.gennexthub.com/ GennextHub&lt;/ref&gt;&lt;ref&gt;{{cite av media|url=https://www.youtube.com/watch?v=sxpqwdxcrsY|title=Dhruvil Sanghvi @dhruvilsanghvi|date=18 February 2015|work=YouTube|accessdate=28 July 2015}}&lt;/ref&gt; LogiNext was one of the 11 start ups that were part of its first ever incubation program.&lt;ref&gt;http://www.livemint.com/Industry/XQllVDr2QK6qpmtvt9NqeJ/RIL-Microsoft-Ventures-nurture-startups.html Mint&lt;/ref&gt;&lt;ref&gt;http://www.gennexthub.com/startup_loginext.html&lt;/ref&gt; Microsoft Ventures aids in organizing mentor meetings, conducting workshops, seminars, and customer, partner and investor meetings. RIL supports by providing infrastructure and also invites the heads of its departments to hold workshops.&lt;ref&gt;http://www.indiaprwire.com/pressrelease/information-technology/20150219358096.htm&lt;/ref&gt;

==Products==
1. '''Line Haul Transport Analytics (Inter-City Hub-To-Hub)'''&lt;br /&gt;
'''Track-A-Pack''' uses matchbox-sized trackers which are placed in shipment bags, manifests or vehicles and helps to locate them as they change hands across surface, rail and air transport.&lt;ref&gt;http://yourstory.com/2014/04/loginext/ YourStory&lt;/ref&gt;&lt;ref&gt;http://partners.telefonica.com/our-partners/loginext-e6a66bb3-6727-e411-a930-d89d6764562c Telefonica&lt;/ref&gt;&lt;br /&gt;
2. '''Last Mile Delivery Optimization (Inter-City Hub-To-door)'''&lt;br /&gt;
'''Track-Last-Mile''' The last mile solution helps to reduce this cost with smart mobile apps and cloud based planning and optimization engine. Around 30% of the logistics cost is spent on last mile deliveries.&lt;ref&gt;https://www.youtube.com/watch?v=XboT1Lnp1V0 CNBC YoungTurks&lt;/ref&gt;&lt;br /&gt;
3. '''Pick-Up &amp; Delivery Automation(Intra-City Door-To-Door)'''&lt;br /&gt;
'''Point-to-Point(P2P)''' is based on smart mobile apps which helps to allocate tasks to nearest available delivery boys/trucks taking into consideration the priority, distance, preferred pick-up/delivery time window, capacity, location and type of vehicle. This product enables logistics based companies to provide any &quot;Uber for X&quot; kind of service. .&lt;ref&gt;https://www.loginextsolutions.com/is-it-the-right-time-for-hyper-local/&lt;/ref&gt;&lt;br /&gt;

==Recognition==
LogiNext has been recognized at various platforms for its entrepreneurial achievement
# Part of 10 startups in the first batch of GenNext Innovation Hub.&lt;ref&gt;http://articles.economictimes.indiatimes.com/2014-09-06/news/53627742_1_microsoft-ventures-accelerator-programmes-microsoft-india Economic Times&lt;/ref&gt;
# TechSparks 2014 – Top 30 Emerging Technology Product Companies of India.&lt;ref&gt;http://technode.com/2014/10/21/yourstory-techsparks-2014-unveils-list-30-indian-startups-watch/ YourStory&lt;/ref&gt;
# Part of top 15 startups by NextBigWhat in &quot;Hot &amp; Upcoming Startups&quot;.&lt;ref&gt;http://www.nextbigwhat.com/biggies-awards/hot-upcoming-startup-biggies-awards/ NextBigWhat&lt;/ref&gt;
# Top 30 startups IBM GEP Startup Camp, India’s first B2B startup competition held by a corporate company.&lt;ref&gt;	http://www.nextbigwhat.com/ibm-gep-india-smartcamp-list-shortlisted-startup-companies-1530-297/ NextBigWhat&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* [http://www.loginextsolutions.com  LogiNext Corporate Website]

[[Category:Big data]]
[[Category:Analytics]]
[[Category:Logistics]]
[[Category:Solutions]]
[[Category:Companies based in Mumbai]]
[[Category:Privately held companies of India]]</text>
      <sha1>2zyib6ikou9k3p843q6rtcxqvevt8w5</sha1>
    </revision>
  </page>
  <page>
    <title>SQream DB</title>
    <ns>0</ns>
    <id>46890563</id>
    <revision>
      <id>670020675</id>
      <parentid>669961857</parentid>
      <timestamp>2015-07-05T07:45:19Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (11296)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5511">{{Multiple issues|{{advert|date=June 2015}}{{Orphan|date=June 2015}}{{notability|date=June 2015}}}}
{{Infobox software
| name                   = SQream DB
| developer              = SQream Technologies Ltd.
| released               = {{start date|2014}}
| programming language   = [[CUDA]], [[C++]], [[Haskell (programming language)|Haskell]]
| operating system       = [[Linux]]
| genre                  = [[Relational database management system|RDBMS]]
| status                 = Active
| license                = [[Proprietary software|Proprietary]]
| website                = [http://www.sqream.com SQream Technologies]
}}

'''SQream DB''' is an [[SQL]] [[RDBMS]] solution from SQream Technologies Ltd. that specifically utilizes [[Nvidia]] [[GPU]]s to run [[SQL]] queries.&lt;ref&gt;{{cite web|title=This insanely fast big data startup uses only one server – and just got $7.4M in funding|url=http://www.geektime.com/2015/06/09/this-insanely-fast-big-data-startup-uses-only-one-server-and-just-got-7-4m-in-funding/|website=Geektime|accessdate=22 June 2015}}&lt;/ref&gt;
SQream DB is designed for massive analytic workloads in the petabyte [[Online analytical processing|analytics]] market.

==History==
SQream DB is the first product from SQream Technologies Ltd, founded in 2010 by entrepreneur Ami Gal and GPU technology expert Kostya Varakin.

SQream DB was first released in 2014&lt;ref&gt;{{cite web|url=http://www.geektime.com/2014/03/27/sqream-tech-unveils-new-big-data-platform/|title=SQream Tech unveils new big data platform|work=Geektime}}&lt;/ref&gt; after a successful partnership with Orange Silicon Valley.&lt;ref name=&quot;enerprisetech&quot;&gt;{{cite web|title=Telco Calls On GPU-Native SQream SQL Database|url=http://www.enterprisetech.com/2014/03/28/telco-calls-gpu-native-sqream-sql-database/|website=Enterprise Tech|accessdate=5 October 2014}}&lt;/ref&gt;

==Performance==
The company claims it is much faster and can do so with a 40x price reduction compared to other big-data solutions.&lt;ref name=&quot;nvidia-blog&quot;&gt;{{cite web|title=IBM, Orange Use GPUs for Next Generation Enterprise Big Data Analytics at GTC|url=http://blogs.nvidia.com/blog/2014/03/21/ibm-orange-gpus-big-data/|website=Nvidia Blog|accessdate=5 October 2014}}&lt;/ref&gt;
The company also claimed [[Orange S.A.|Orange]] saved $6,000,000 by using SQream's database solution, when presenting at the 2014 GPU Technologies Conference.&lt;ref name=pdf1&gt;{{cite web|title=Getting big data done on a GPU-Based database|url=http://on-demand.gputechconf.com/gtc/2014/presentations/S4644-big-data-gpu-based-database.pdf|website=GPU Technology Conference|accessdate=5 October 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.prweb.com/releases/2014/03/prweb11705346.htm|title=SQream Technologies and Orange Silicon Valley Demo Groundbreaking Big Data Platform at GTC|date=26 March 2014|work=PRWeb}}&lt;/ref&gt;
SQream DB is aimed at the budget multi-terrabyte analytics market, due to its smaller footprint, more modest hardware requirements and extensive use of compression.&lt;ref&gt;{{cite web|url=http://www.datanami.com/2015/04/22/a-shoebox-size-data-warehouse-powered-by-gpus/|title=A Shoebox-Size Data Warehouse Powered by GPUs|work=Datanami}}&lt;/ref&gt;

SQream DB is also the basis for a solution named GenomeStack,&lt;ref&gt;{{cite web|title=SQream Raises $7.4M in Funding Round|url=https://www.genomeweb.com/business-policy-funding/sqream-raises-74m-funding-round|website=genomeweb|accessdate=22 June 2015}}&lt;/ref&gt; which makes extensive use of SQream DB's capabilities for querying many DNA sequences simultaneously.&lt;ref&gt;{{cite web|url=http://www.bio-itworld.com/2015/4/27/april-news-bio-it-world-conference-around-industry.html|title=April News From the Bio-IT World Conference and Around the Industry|work=bio-itworld.com}}&lt;/ref&gt;&lt;ref&gt;http://www.globes.co.il/news/article.aspx?fbdid=1001037069&lt;/ref&gt;

==Patents==
The company has applied for numerous patents, encompassing [[Parallel computing|parallel execution]] queries on [[multi-core processor]]s,&lt;ref name=&quot;patent1&quot;&gt;{{cite web|title=Patent WO 2012025915 A1 - A system and method for the parallel execution of database queries over cpus and multi core processors|url=http://www.google.com/patents/WO2012025915A1|website=Google Patents|accessdate=5 October 2014}}&lt;/ref&gt;&lt;ref name=&quot;patent2&quot;&gt;{{cite web|title=Patent WO 2012025915 A8 - A system and method for the parallel execution of database queries over cpus and multi core processors|url=https://www.google.com/patents/WO2012025915A8|website=Google Patents|accessdate=5 October 2014}}&lt;/ref&gt; speeding up parallel execution on [[Vector processor|vector enabled architecture]]&lt;ref name=&quot;patent3&quot;&gt;{{cite web|title=Patent WO 2014020605 A1 - A method for pre-processing and processing query operation on multiple data chunk on vector enabled architecture|url=https://www.google.com/patents/WO2014020605A1|website=Google Patents|accessdate=5 October 2014}}&lt;/ref&gt; and others.

==References==
{{reflist}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;

== External links ==
*{{official website|http://sqream.com/solutions/products/sqream-db/}}

[[Category:SQL]]
[[Category:Relational database management systems]]
[[Category:RDBMS software for Linux]]
[[Category:Products introduced in 2014]]
[[Category:Column-oriented DBMS software for Linux]]
[[Category:Big data]]
[[Category:Israeli inventions]]
[[Category:Haskell software]]</text>
      <sha1>0buxil9jd4bn9tvwuru53hoyxurbzx6</sha1>
    </revision>
  </page>
  <page>
    <title>Web intelligence</title>
    <ns>0</ns>
    <id>22113059</id>
    <revision>
      <id>666700465</id>
      <parentid>666697614</parentid>
      <timestamp>2015-06-13T01:09:34Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>Changed top-description and added another book</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3483">'''Web intelligence''' is the area of scientific research and development that explores the roles and makes use of [[artificial intelligence]] and [[information technology]] for new products, services and frameworks that are empowered by the [[World Wide Web]].&lt;ref&gt;{{cite book| first=Ning | last=Zhong | first2=Jiming | last2=Liu Yao| year=2003| last3=Yao| first3=Yiyu| title=Web Intelligence | isbn=978-3540443841|url=https://books.google.de/books?id=6LmrCAAAQBAJ&amp;pg=PA0|page=1}}&lt;/ref&gt;

The term was born&lt;!--coined?--&gt; in a paper written by Ning Zhong, Jiming Liu Yao and Y.Y. Ohsuga in the Computer Software and Applications Conference in 2000.&lt;ref&gt;{{Citation| doi=10.1109/CMPSAC.2000.884768| first=Ning | last=Zhong | author-link=Ning Zhong| first2=Jiming | last2=Liu Yao| author2-link=Jiming Liu Yao| year=2000| last3=Yao| first3=Y.Y.| last4=Ohsuga| first4=S.| title=Web Intelligence | publisher=Computer Software and Applications Conference, 2000. COMPSAC 2000. The 24th Annual International| pages=469|url = http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=884768| chapter=Web Intelligence (WI)| isbn=0-7695-0792-1}}&lt;/ref&gt;

== Research ==
The research about the web intelligence covers&lt;!--spans over--&gt; many fields – including [[data mining]] (in particular [[web mining]]), [[information retrieval]], the [[semantic web]], web [[Data warehouse|data warehousing]] – typically with a focus on [[web personalization]] and [[adaptive website]]s.&lt;ref&gt;{{Citation| first=Juan | last=Velasquez | author-link=Juan D. Velasquez Silva| first2=Palade | last2=Vacile | author2-link=Vasile Palade| year=2008| title=Adaptive Web Site: A Knowledge Extraction from Web Data Approach|edition=1st| publisher=IOS Press|url=|isbn=978-1-58603-831-1}}&lt;/ref&gt;

== References ==
{{reflist}}

== External links ==
* [http://wi-consortium.org/ Web Intelligence Consortium], an international, non-profit organization dedicated to advancing world-wide scientific research and industrial development in the field of Web Intelligence
* [http://wi.dii.uchile.cl/?lang=en Web intelligence Research Group at University of Chile]
* [https://www.coursera.org/course/bigdata Web Intelligence and Big Data Course] on [[Coursera]]

==Further reading==
* {{cite book| first=Ning | last=Zhong | first2=Jiming | last2=Liu Yao| year=2003| last3=Yao| first3=Yiyu| title=Web Intelligence | isbn=978-3540443841|url=https://books.google.de/books?id=6LmrCAAAQBAJ&amp;pg=PA0}}
* {{cite book|last1=Shroff|first1=Gautam|title=The Intelligent Web: Search, smart algorithms, and big data|date=January 2014|isbn=978-0199646715}}
* {{cite book| first1=Juan | last1=Velasquez | first2=Palade | last2=Vacile | year=2008| title=Adaptive Web Site: A Knowledge Extraction from Web Data Approach|edition=1st| publisher=IOS Press|url=|isbn=978-1-58603-831-1}}
* {{cite book|last1=Chbeir|first1=Richard|last2=Badr|first2=Youakim|last3=Abraham|first3=Ajith|last4=Hassanien|first4=Aboul-Ella|title=Emergent Web Intelligence: Advanced Information Retrieval (Advanced Information and Knowledge Processing)|date=April 2010|isbn=978-1849960731|url=http://deca.cuc.edu.cn/Community/cfs-filesystemfile.ashx/__key/CommunityServer.Components.PostAttachments/00.00.00.11.45/Emergent-Web-Intelligence-Advanced-Information-Retrieval.pdf}}

{{Comp-stub}}

[[Category:Artificial intelligence]]
[[Category:Crowdsourcing]]
[[Category:Big data]]
[[Category:Data mining]]
[[Category:Collective intelligence]]
[[Category:Social information processing]]</text>
      <sha1>8asjwm43btsf1s075fh3vu6rfkv2ud1</sha1>
    </revision>
  </page>
  <page>
    <title>Predix (software)</title>
    <ns>0</ns>
    <id>47108437</id>
    <revision>
      <id>669981317</id>
      <parentid>669868701</parentid>
      <timestamp>2015-07-04T23:56:09Z</timestamp>
      <contributor>
        <username>SwisterTwister</username>
        <id>7505908</id>
      </contributor>
      <minor/>
      <comment>SwisterTwister moved page [[Predix(software)]] to [[Predix (software)]]: Space</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2267">'''Predix''' is [[General Electric]]s's software platform for the [[Industrial Internet]].&lt;ref&gt;{{cite web|author=GE |url=https://www.gesoftware.com/predix |title=Predix Powers Industrial-strength Apps |publisher=gesoftware.com |accessdate=2015-06-29}}&lt;/ref&gt;

==Industrial application==
'''Predix''' as a cloud-based PaaS ([[platform as a service]]) is claimed to enable industrial-scale analytics for asset performance management (APM) and operations optimization by providing a standard way to connect machines, data, and people. GE expects Predix software to do for factories and plants what [[Apple Inc.|Apple]]'s iOS did for cell phones.&lt;ref&gt;{{cite web|first=James|last=Passeri|url=http://www.thestreet.com/story/13174112/1/ge-expects-predix-software-to-do-for-factories-what-apples-ios-did-for-cell-phones.html |title=GE Expects Predix Software to Do for Factories What Apple's iOS Did for Cell Phones |publisher=thestreet.com |date=2015-03-06 |accessdate=2015-06-29}}&lt;/ref&gt;
The software was introduced to the market and made available to all companies in 2015.&lt;ref&gt;{{cite web|first=James|last=Passeri|url=http://www.businesswire.com/news/home/20141009005691/en/GE-Open-Predix-Industrial-Internet-Platform-Users#.VZEagCuUdfA |title=GE to Open Up Predix Industrial Internet Platform to All Users|publisher=businesswire.com |accessdate=2015-06-29}}&lt;/ref&gt; Built on [[Cloud Foundry]] [[open source]] technology, Predix provides a [[microservices]] based delivery model with a distributed architecture (cloud, and on-machine).&lt;ref&gt;{{cite web|url=http://www.i-programmer.info/news/0/8686.html|title=Predix - A Platform for the Industrial Internet Of Things|first=Sue|last=Gee|date=24 June 2015}}&lt;/ref&gt;

==See also==
* [[Cloud-Based Design and Manufacturing|Cloud-based design and manufacturing]]
* [[Big data]]
* [[SCADA]]
* [[Industrial Ethernet]]
* [[Internet of Things]]
* [[Machine to machine]]
* [[Industrial control system]]
* [[Industry 4.0]]
* [[Intelligent Maintenance Systems]]
* [[Cyber-physical system]]
* [[Machine-generated data]]

==References==
{{Reflist}}

{{DEFAULTSORT:Industrial Internet}}
[[Category:Industrial automation]]
[[Category:Industrial computing]]
[[Category:Internet of Things]]
[[Category:Technology forecasting]]
[[Category:Big data]]</text>
      <sha1>jcxza9tyyxcof0rawsq0ypjy3s8za39</sha1>
    </revision>
  </page>
  <page>
    <title>Oracle NoSQL Database</title>
    <ns>0</ns>
    <id>37111193</id>
    <revision>
      <id>673606544</id>
      <parentid>671298525</parentid>
      <timestamp>2015-07-29T09:00:21Z</timestamp>
      <contributor>
        <username>Anandchandak15</username>
        <id>25608221</id>
      </contributor>
      <comment>/* Benchmarking */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17239">{{single source|date=June 2015}}
{{NPOV|date=June 2015}}
{{Infobox software
| name                   = Oracle NoSQL DB
| logo                   = [[File:Oracle NOSQL Database.jpg|Oracle NOSQL Database.jpg]]
| developer              = [[Oracle Corporation]]
| released               = {{Start date|Sept 2011}}
| status                 = Active
| latest release version = 12.1.3.3.4
| latest release date    = {{release date|2015|05|06|df=yes}}
| latest preview version = 
| latest preview date    = 
| frequently updated     = yes
| programming language   = [[Java]]
| operating system       = [[Cross-platform]]
| language               = English
| genre                  = [[key-value database]]
| license                = AGPL and Proprietary
| website =                [http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/index.html Oracle NoSQL Database] 
}}

'''Oracle NoSQL Database''' is a [[NoSQL]]-type [[Distributed database|distributed]] [[key-value database]] &lt;ref&gt;[http://www.oracle.com/technetwork/products/nosqldb/overview/index.html Oracle NoSQL Database]&lt;/ref&gt;&lt;ref&gt;[https://blogs.oracle.com/charlesLamb/ Oracle NoSQL Database Performance Tests]&lt;/ref&gt;&lt;ref name=iw&gt;[http://www.infoworld.com/d/data-explosion/first-look-oracle-nosql-database-179107 &quot;First look: Oracle NoSQL Database&quot;], by Peter Wayner, [[Infoworld]]&lt;/ref&gt;&lt;ref&gt;[http://www.forbes.com/sites/oracle/2013/08/22/do-you-know-nosql/ Do you know NoSQL]&lt;/ref&gt; by [[Oracle Corporation]].It provides transactional semantics for data manipulation, horizontal scalability, and simple administration and monitoring.

Oracle NoSQL Database provides a very simple data model to the application developer. Each row is identified by a unique key, and also has a value, of arbitrary length, which is interpreted by the application. The application can manipulate (insert, delete, update, read) a single row in a transaction. The application can also perform an iterative, non-transactional scan of all the rows in the database. 

== Licensing ==
The Oracle NoSQL Database is distributed in two editions, ''Oracle NoSQL Database Server Community Edition'' under an [[AGPL license]] and ''Oracle NoSQL Enterprise Edition'' under the Oracle Commercial License.

The Oracle NoSQL Database is licensed using a [[freemium]] model: Open source versions of Oracle NoSQL Community Edition  available, but end users can pay for additional features and support from Oracle Stores.&lt;ref&gt;[https://shop.oracle.com/pls/ostore/f?p=dstore:product:3674543818887::NO:RP%2C6:P6_LPI%2CP6_PPI:124789930244771286351865%2C &quot; Oracle Stores&quot;]&lt;/ref&gt; If you are integrating with other Oracle Products such as [[Oracle Enterprise Manager]], [[Oracle Coherence]] then Oracle NoSQL Enterprise Edition should be purchased.

The Oracle NoSQL Database drivers &lt;ref&gt;[http://www.oracle.com/technetwork/database/database-technologies/nosqldb/downloads/index.html Oracle NoSQL Database drivers]&lt;/ref&gt; are licensed pursuant to the [[Apache 2.0 License]] and used with both the community and enterprise editions.&lt;ref&gt;http://docs.oracle.com/cd/NOSQL/html/index.html&lt;/ref&gt;

== Main features ==

;Architecture &lt;ref&gt;[http://www.oracle.com/technetwork/database/nosqldb/learnmore/nosql-database-498041.pdf &quot;Oracle NoSQL Database White Paper&quot;]&lt;/ref&gt;
Oracle NoSQL Database is built upon the [[Berkeley DB|Oracle Berkeley DB]] Java Edition [[High-availability cluster|high-availability]] storage engine. In addition to that it adds a layer of services for use in distributed environments to provide a distributed, highly available key/value storage,  suited for large-volume, latency-sensitive applications. 

In this respect, an ''[[Infoworld]]'' review mentions that Oracle bought the company which developed the Berkeley DB,  [[Sleepycat Software]].&lt;ref name=iw/&gt;

;Sharding and replication
Oracle NoSQL Database is a client-server,[[Shard (database architecture)|sharded]],[[Shared nothing architecture|shared-nothing]] system. The data in each shard are replicated on each of the nodes which comprise the shard. It provides a simple key-value paradigm to the application developer. The major key for a record is hashed to identify the shard that the record belongs to.Oracle NoSQL Database is designed to support changing the number of shards dynamically in response to availability of additional hardware. If the number of shards changes, key-value pairs are redistributed across the new set of shards dynamically, without requiring a system shutdown and restart. A shard is made up of a single electable master node which can serve read and write requests,and several replicas (usually two or more) which can serve read requests. Replicas are kept up to date using streaming replication. Each change on the master node is committed locally to disk and also propagated to the replicas.

;High availability and fault-tolerance &lt;ref&gt;[https://docs.oracle.com/cd/E57769_01/html/GettingStartedGuide/introduction.html Oracle NoSQL High Availability]&lt;/ref&gt; 
Oracle NoSQL Database provides single-master,multi-replica [[Replication (computing)#Database replication|database replication]]. Transactional data is delivered to all replica nodes with flexible durability policies per transaction. In the event the master replica node fails, a consenus based [[Paxos (computer science)|PAXOS]] -based automated fail-over election process minimizes downtime. As soon as the failed node is repaired, it rejoins the shard, is brought up to date and then becomes available for processing read requests.Thus, the Oracle NoSQL Database server can tolerate failures of nodes within a shard and also multiple failures of nodes in distinct shards.

Proper placement of masters and replicas on server hardware (racks and interconnect switches) by Oracle NoSQL Database is intended to increase the availability on commodity servers.

;Transparent load balancing
Oracle NoSQL Database Driver &lt;ref&gt;[http://www.oracle.com/technetwork/products/nosqldb/overview/default-497223.html Intelligent Drivers]&lt;/ref&gt; partitions the data in real time and evenly distributes it across the storage nodes. It is network topology and latency-aware, routing read and write operations to the most appropriate storage node in order to optimize load distribution and performance.

;ACID compliant transaction
Oracle NoSQL Database provides [[ACID]] complaint transactions for full Create, Read, Update and Delete [[Create, read, update and delete|(CRUD)]] operations, with adjustable durability and consistency transactional guarantees.You can also execute a sequence of operations as a single [[Atomicity (database systems)|atomic]] unit as long as all the records that are going to be operated upon share the same Major Key Path.&lt;ref&gt;[http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/nosql-transactions-497227.html &quot;Oracle NoSQL ACID Compliance Transactions&quot;]&lt;/ref&gt;

;JSON data format
Oracle NoSQL Database has support for the [[Apache Avro|Avro]] &lt;ref&gt;[http://docs.oracle.com/cd/E26161_02/html/GettingStartedGuide/avrobindings.html Oracle NoSQL Avro Bindings]&lt;/ref&gt;  data serialization, which provides a compact, schema-based binary data format. Avro allows you to define a schema
(using [[JSON]]) for the data contained in a record's value and it also supports schema evolution. Configurable Smart Topology System administrators indicate how much capacity is available on a given storage node,allowing more capable storage nodes to host multiple replication nodes. Once the system knows about the capacity for the storage nodes in a configuration, it automatically allocates replication nodes intelligently. This is intended for better load balancing for the system, better use of system resources and minimizing system impact in the event of storage node failure. Smart Topology also supports Data Centers, ensuring that a full set of replicas is initially allocated to each data center.

;Elastic configuration &lt;ref&gt;[http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/elastic-expansion-2046283.html Oracle Elastic NoSQL Elastic Expansion]&lt;/ref&gt;
&quot;Elasticity&quot; refers to dynamic online expansion of the deployed cluster.  One can add more storage nodes to increase the capacity, performance, reliability, or all of the above
Oracle NoSQL Database includes a topology planning feature, with which an administrator can now modify the configuration of a NoSQL database, while the database is still online.
This allows the administrator to:

*Increase Data Distribution: by increasing number of shards in the cluster, which increases write throughput.
*Increase Replication Factor: by assigning additional replication nodes to each shard, which increases read throughput and system availability.
*Rebalance Data Store: by modifying the capacity of a storage node(s), the system can be rebalanced, re-allocating replication nodes to the available storage nodes,&lt;ref&gt;[http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/nosql-storage-497226.htm Storage Nodes]&lt;/ref&gt; as  appropriate. 
The topology rebalance command allows the administator to move replication nodes and/or partitions from over utilized nodes onto underutilized storage nodes or vice versa

;Administration and system monitoring &lt;ref&gt;[http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/nosql-admin-497228.html Oracle NoSQL Easy Administration]&lt;/ref&gt;
Oracle NoSQL Database provides an administration service, which can be accessed either from a web console or a command-line interface (CLI). This service supports core functionality such as the ability to configure, start, stop and monitor a storage node, without requiring manual effort with configuration files, shell scripts, or explicit database operations. In addition it also allows [[Java Management Extensions| Java Management Extensions (JMX)]] or [[Simple Network Management Protocol]] (SNMP) agents to be available for monitoring. This allows management clients to poll information about the status, performance metrics and operational parameters of the storage node and its managed services.

;CAP
Oracle NoSQL Database is configurable to be either C/P or A/P in [[CAP theorem|CAP]].&lt;ref&gt;[http://dbmsmusings.blogspot.in/2011/10/overview-of-oracle-nosql-database.html &quot;Overview of Oracle NoSQL Database]&lt;/ref&gt; In particular, if writes are configured to be performed synchronously to all replicas, it is C/P in CAP i.e a partition or node failure causes the system to be unavailable for writes. If replication is performed asynchronously, and reads are configured to be served from any replica, it is A/P in CAP i.e the system is always available, but there is no guarantee of consistency.

;Table data model
Release 3.0 introduces tabular data structure, which simplifies application data modeling by leveraging existing schema design core concepts. Table model is layered on top of the distributed key-value structure, inheriting all its advantages and simplifying application design even further by enabling seamless integration with familiar SQL-based applications

;Secondary index &lt;ref&gt;[http://www.infoq.com/news/2014/04/oracle-nosql-database-3.0  Oracle NoSQL Supports Secondary Indexes]&lt;/ref&gt;
Primary key only based indexing limits number of low latency access paths. Sometime application needs a few non-primary-key based paths to support the whole solution for the
real-time system. Being able to define secondary index on any value field improves performance for queries.

;APIs &lt;ref&gt;[http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/nosql-api-497225.html Oracle NoSQL API for Application Development]&lt;/ref&gt;
Oracle NoSQL Database includes support for Java, C, Python, REST APIs. These simple APIs allow the application developer to perform CRUD operations on Oracle NoSQL Database. These libraries also include Avro support, so that developers can serialize key-value records and de-serialize keyvalue records interchangeably between C and Java applications. 

With Oracle NoSQL Database 12.1.3.2.5 support for Oracle REST Data Services has been added. This allows customers to build a REST-based application that can access data in either Oracle Database or Oracle NoSQL Database.

;Large Object support &lt;ref&gt;[http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/lob-2046284.html LOB]&lt;/ref&gt;
Stream based APIs are provided in the product to read and write Large Objects (LOBs) such as audio and video files, without having to materialize the value in its entirety in memory. This is intended to decrease the  latency of operations across mixed workloads of objects of varying sizes.

;Apache Hadoop integration &lt;ref&gt;[http://www.oracle.com/technetwork/articles/bigdata/nosql-hadoop-1654158.html Oracle NoSQl Apache Hadoop Integration]&lt;/ref&gt;
''KVAvroInputFormat and KVInputFormat'' &lt;ref&gt;[https://docs.oracle.com/cd/E26161_02/html/javadoc/index.html?oracle/kv/hadoop/KVInputFormat.html Oracle NoSQL Hadoop Integration Classes]&lt;/ref&gt; classes are available to read data from Oracle NoSQL Database natively into Hadoop Map/Reduce jobs. One use for this class is to read
NoSQL Database records into Oracle Loader for Hadoop.

;Oracle Database integration via external tables (EE only)
Support for external table allows fetching Oracle NoSQL data from Oracle database using SQL statements such as Select, Select Count(*) etc. Once NoSQL data is exposed through
external tables, one can access the data via standard JDBC drivers and/or visualize it through enterprise Business Intelligence tools.

;Integration with other Oracle products (EE only)
Oracle Event Processing (OEP) provides read access to Oracle NoSQL Database via the NoSQL Database cartridge. Once the cartridge is configured, CQL queries can be used to
query the data.Oracle Semantic Graph has developed a Jena Adapter for Oracle NoSQL Database &lt;ref&gt;[http://www.oracle.com/technetwork/database-options/spatialandgraph/downloads/index-156999.html  &quot;Oracle Semantic Graph has developed a Jena Adapter for Oracle NoSQL Database&quot;]&lt;/ref&gt;  to store large volumes of [[Resource Description Framework|RDF]] data (as triplets/quadruplets). This adapter enables fast access to graph data stored in Oracle NoSQL Database via [[SPARQL]] queries. An integration with  [[Oracle Coherence]] has been provided that allows Oracle NoSQL Database to be used as a cache for [[Oracle Coherence]] applications, also allowing applications to directly access cached data from Oracle NoSQL Database.

;Online rolling upgrade &lt;ref&gt;[http://www.oracle.com/technetwork/database/database-technologies/nosqldb/overview/rollingupgrade-2046275.html Oracle NoSQL Rolling Upgrade]&lt;/ref&gt;
Oracle NoSQL Database provides facilities to perform a rolling upgrade, allowing a system administrator to upgrade all of the nodes in the NoSQL Database cluster while the database continues to remain online and available to clients.

;Multi zone deployment
Oracle NoSQL Database supports the definition of multiple zones from within the topology deployment planner. It leverages the definition of these zones internally to intelligently allocate replication of processes and data, in order to improve reliability during hardware,network &amp; power related failure scenarios.There are two types of zones: Primary zones contain nodes that can be served as masters or replicas and are typically connected by fast interconnects. Secondary zones contain nodes which can only be served as replicas. Secondary zones can be used to provide low latency read access to data at a distant location, or to offload read-only workloads, like analytics, report generation, and data exchange for improved workload management.

;Enterprise security (EE only)
OS-independent, cluster-wide password-based user authentication and Oracle Wallet integration, enables greater protection&lt;ref&gt;[http://www.dbta.com/Editorial/News-Flashes/Oracle-NoSQL-Database-30-Ups-Security-and-Performance-96084.aspx &quot;Oracle NoSQL Database 3.0 Ups security and performance]&lt;/ref&gt; from unauthorized access to sensitive data.
Additionally, session-level Secure Sockets Layer (SSL) encryption and network port restrictions are intended to  improve the protection from network intrusion.

== Performance==

The Oracle NoSQL DB team has worked with several key Oracle partners, including Intel and [[Cisco Systems|Cisco]],&lt;ref&gt;[http://www.cisco.com/c/dam/en/us/solutions/collateral/data-center-virtualization/data-center-virtualization/LE-32102-OracleNoSQLDB.pdf &quot;Oracle NoSQL Benchmarking with Cisco  Unified Computing and Servers (UCS)]&lt;/ref&gt; performed benchmark testing using [[YCSB|Yahoo! Cloud Serving Benchmarks (YCSB)]] on various hardware configurations, and published its results through blogs or white papers. For example, in 2012 Oracle reported that Oracle NoSQL Database exceeded 1 million mixed YCSB Ops/Sec. &lt;ref&gt;[https://blogs.oracle.com/charlesLamb/entry/oracle_nosql_database_exceeds_1 &quot;Oracle NoSQL Database Exceeds 1 Million Mixed YCSB Ops/Sec&quot;]&lt;/ref&gt;

==References==
{{Reflist|2}}


[[Category:Oracle software]]
[[Category:NoSQL]]
[[Category:Database-related software for Linux]]
[[Category:Big data]]
[[Category:Key-value databases]]</text>
      <sha1>eyfs03cz2on8tgb8iy3k7wdjbzlxakf</sha1>
    </revision>
  </page>
  <page>
    <title>Draft:Prescriptive Lineage</title>
    <ns>118</ns>
    <id>47205155</id>
    <revision>
      <id>670856607</id>
      <parentid>670856430</parentid>
      <timestamp>2015-07-10T17:31:49Z</timestamp>
      <contributor>
        <username>Chafas</username>
        <id>5324520</id>
      </contributor>
      <minor/>
      <comment>Chafas moved page [[Wikipedia:Prescriptive Lineage]] to [[Draft:Prescriptive Lineage]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="463">{{Advert|article|date=July 2015}}

==History==

== See also ==
*  [[Governance]]


==Resources==
'''Articles'''&lt;br /&gt;
* [http://www.dbmag.intelligententerprise.com/showArticle.jhtml?articleID=202400140 The Easy Way to Quick Data Access]


'''Apache Atlas'''
* http://www.ibm.com/developerworks/downloads/im/datastudiodev/?S_TACT=105AGX01&amp;S_CMP=LP


{{DEFAULTSORT:}}
[[Category:Hadoop]]
[[Category: Big data]]
[[Category: Data Management]]
[[Category:Persistence]]</text>
      <sha1>pw7crv0g6upg2wm8wj3xk8yvqtenxtp</sha1>
    </revision>
  </page>
</mediawiki>
